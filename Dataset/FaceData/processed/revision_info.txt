arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25
--------------------
tensorflow version: 2.19.0
--------------------
git hash: b'860368ceb20db792b6b943d52ff5dfb99fcbb23b'
--------------------
b'diff --git a/.gitattributes b/.gitattributes\ndeleted file mode 100644\nindex 64e2803..0000000\n--- a/.gitattributes\n+++ /dev/null\n@@ -1,3 +0,0 @@\n-*.zip filter=lfs diff=lfs merge=lfs -text\n-*.ckpt* filter=lfs diff=lfs merge=lfs -text\n-*.pb filter=lfs diff=lfs merge=lfs -text\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3) copy.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3) copy.png\ndeleted file mode 100644\nindex 91be536..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3) copy.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3).png\ndeleted file mode 100644\nindex 91be536..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro.png\ndeleted file mode 100644\nindex 0540b28..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro (2).png\ndeleted file mode 100644\nindex e9ef9f0..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro.png\ndeleted file mode 100644\nindex 04ff255..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (2).png\ndeleted file mode 100644\nindex 0c37c23..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (3).png\ndeleted file mode 100644\nindex 8e7842e..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (3).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro.png\ndeleted file mode 100644\nindex 91b18ff..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (2).png\ndeleted file mode 100644\nindex 881520b..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (3).png\ndeleted file mode 100644\nindex 03ab7d3..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (3).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro.png\ndeleted file mode 100644\nindex fa1f42a..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro (2).png\ndeleted file mode 100644\nindex d4d9e24..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro.png\ndeleted file mode 100644\nindex 7467151..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_36_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_36_Pro.png\ndeleted file mode 100644\nindex 4336cd9..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_36_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro (2).png\ndeleted file mode 100644\nindex c3131df..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro.png\ndeleted file mode 100644\nindex e010c70..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (2).png\ndeleted file mode 100644\nindex 2a5ed54..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (3).png\ndeleted file mode 100644\nindex 0d89ab7..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (3).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro.png\ndeleted file mode 100644\nindex 669d82a..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (2).png\ndeleted file mode 100644\nindex 38b750a..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (3).png\ndeleted file mode 100644\nindex c35fb76..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (3).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro.png\ndeleted file mode 100644\nindex b1eb45c..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (2).png\ndeleted file mode 100644\nindex b331c99..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (3).png\ndeleted file mode 100644\nindex 90c13a1..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (3).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro.png\ndeleted file mode 100644\nindex 49b56f8..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro (2).png\ndeleted file mode 100644\nindex 10d6da8..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro.png\ndeleted file mode 100644\nindex 731c604..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (2).png\ndeleted file mode 100644\nindex ce6f384..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (3).png\ndeleted file mode 100644\nindex 93075b3..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (3).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro.png\ndeleted file mode 100644\nindex 2df0742..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (2).png\ndeleted file mode 100644\nindex c609125..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (3).png\ndeleted file mode 100644\nindex 4fb46a0..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (3).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro.png\ndeleted file mode 100644\nindex ccacd60..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro (2).png\ndeleted file mode 100644\nindex 79dc7da..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro.png\ndeleted file mode 100644\nindex 77e1025..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (2).png\ndeleted file mode 100644\nindex 81a3125..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (3).png\ndeleted file mode 100644\nindex 70fab4b..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (3).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro.png\ndeleted file mode 100644\nindex 628e2ce..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro (2).png\ndeleted file mode 100644\nindex df9dc1f..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro.png\ndeleted file mode 100644\nindex ec12b5d..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_22_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_22_Pro.png\ndeleted file mode 100644\nindex 93da5e6..0000000\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_22_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_54_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_54_Pro.png\ndeleted file mode 100644\nindex 5197b4f..0000000\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_54_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_55_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_55_Pro.png\ndeleted file mode 100644\nindex da2d35a..0000000\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_55_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro (2).png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro (2).png\ndeleted file mode 100644\nindex b458418..0000000\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro.png\ndeleted file mode 100644\nindex c9ded27..0000000\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro (2).png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro (2).png\ndeleted file mode 100644\nindex 12980b8..0000000\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro.png\ndeleted file mode 100644\nindex 36eeeb0..0000000\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro (2).png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro (2).png\ndeleted file mode 100644\nindex 3ca9822..0000000\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro.png\ndeleted file mode 100644\nindex 7a34d00..0000000\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_59_Pro (2).png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_59_Pro (2).png\ndeleted file mode 100644\nindex 5bfc7ed..0000000\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_59_Pro (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png b/Dataset/FaceData/processed/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png\ndeleted file mode 100644\nindex 8257ab5..0000000\nBinary files a/Dataset/FaceData/processed/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png b/Dataset/FaceData/processed/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png\ndeleted file mode 100644\nindex 45870ff..0000000\nBinary files a/Dataset/FaceData/processed/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png b/Dataset/FaceData/processed/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png\ndeleted file mode 100644\nindex f4593ba..0000000\nBinary files a/Dataset/FaceData/processed/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/bounding_boxes_75121.txt b/Dataset/FaceData/processed/bounding_boxes_75121.txt\ndeleted file mode 100644\nindex c490bf8..0000000\n--- a/Dataset/FaceData/processed/bounding_boxes_75121.txt\n+++ /dev/null\n@@ -1,9 +0,0 @@\n-Dataset/FaceData/processed\\Quang\\WIN_20250411_21_48_55_Pro.png 573 373 798 664\n-Dataset/FaceData/processed\\Quang\\WIN_20250411_21_48_56_Pro (2).png 572 373 797 658\n-Dataset/FaceData/processed\\Quang\\WIN_20250411_21_48_57_Pro (2).png 571 370 804 659\n-Dataset/FaceData/processed\\Quang\\WIN_20250411_21_48_56_Pro.png 570 376 805 663\n-Dataset/FaceData/processed\\Quang\\WIN_20250411_21_48_58_Pro.png 574 376 802 659\n-Dataset/FaceData/processed\\Quang\\WIN_20250411_21_48_57_Pro.png 569 368 805 662\n-Dataset/FaceData/processed\\Quang\\WIN_20250411_21_48_59_Pro (2).png 568 370 806 663\n-Dataset/FaceData/processed\\Quang\\WIN_20250411_21_48_58_Pro (2).png 569 366 808 663\n-Dataset/FaceData/processed\\Quang\\WIN_20250411_21_48_54_Pro.png 574 380 800 659\ndiff --git a/Dataset/FaceData/processed/bounding_boxes_79144.txt b/Dataset/FaceData/processed/bounding_boxes_79144.txt\ndeleted file mode 100644\nindex ae1c6a8..0000000\n--- a/Dataset/FaceData/processed/bounding_boxes_79144.txt\n+++ /dev/null\n@@ -1,46 +0,0 @@\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_21_Pro.png 506 335 795 706\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_18_Pro (2).png 516 299 808 645\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_18_Pro (3).png 517 303 794 608\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_07_34_Pro.png 569 297 864 704\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_15_Pro.png 493 335 781 698\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_20_Pro.png 521 329 813 711\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_07_33_Pro.png 498 331 803 720\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_15_Pro (2).png 490 340 775 691\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_20_Pro (3).png 518 353 824 720\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_18_Pro.png 521 310 822 683\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_14_Pro (3).png 506 334 807 720\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_13_Pro.png 527 330 824 707\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_15_Pro (3).png 501 325 799 702\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_07_35_Pro.png 491 307 794 681\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_19_Pro (2).png 523 313 803 651\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_07_32_Pro.png 474 344 776 720\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_16_Pro.png 515 323 812 708\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_07_36_Pro.png 482 331 798 720\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_07_32_Pro (2).png 495 343 788 720\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_12_Pro (2).png 529 332 824 708\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_17_Pro (2).png 581 334 858 709\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_16_Pro (2).png 534 325 821 709\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_07_34_Pro (3).png 526 302 825 681\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_07_34_Pro (2).png 563 299 860 702\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_21_Pro (2).png 496 333 771 680\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_19_Pro.png 521 316 793 615\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_22_Pro.png 540 299 826 682\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_13_Pro (3).png 519 341 815 715\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_20_Pro (2).png 526 369 812 720\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_07_33_Pro (2).png 518 321 825 720\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_14_Pro.png 522 338 806 720\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_14_Pro (2).png 517 343 817 720\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_17_Pro.png 574 326 856 720\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_07_35_Pro (2).png 481 331 800 720\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_12_Pro.png 518 322 811 699\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_07_31_Pro.png 521 336 819 720\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_13_Pro (2).png 523 332 818 710\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_07_33_Pro (3).png 532 314 834 719\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_07_31_Pro (3).png 505 324 815 720\n-Dataset/FaceData/processed\\LVQuang\\WIN_20250331_09_26_17_Pro (3).png 561 315 848 708\n-Dataset/FaceData/processed\\TranDangHieu\\download (1).png 262 672 767 1357\n-Dataset/FaceData/processed\\TranDangHieu\\download (2).png 152 762 696 1478\n-Dataset/FaceData/processed\\TranDangHieu\\download.png 202 705 750 1444\n-Dataset/FaceData/processed\\Viruss\\Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png 50 121 926 1264\n-Dataset/FaceData/processed\\Viruss\\Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png 154 108 1417 1799\n-Dataset/FaceData/processed\\Viruss\\Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png 364 442 910 1148\ndiff --git a/Dataset/FaceData/processed/revision_info.txt b/Dataset/FaceData/processed/revision_info.txt\nindex f0f591b..4b6ee58 100644\n--- a/Dataset/FaceData/processed/revision_info.txt\n+++ b/Dataset/FaceData/processed/revision_info.txt\n@@ -2,6 +2,6 @@ arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/proc\n --------------------\n tensorflow version: 2.19.0\n --------------------\n-git hash: b\'69ff1e149c0d84a123d6516ddd82970e65392608\'\n+git hash: b\'860368ceb20db792b6b943d52ff5dfb99fcbb23b\'\n --------------------\n-b\'diff --git a/README.md b/README.md\\ndeleted file mode 100644\\nindex 16ec29c..0000000\\n--- a/README.md\\n+++ /dev/null\\n@@ -1,11 +0,0 @@\\n-# MiAI_FaceRecog_3\\n-Nh\\xe1\\xba\\xadn di\\xe1\\xbb\\x87n khu\\xc3\\xb4n m\\xe1\\xba\\xb7t kh\\xc3\\xa1 chu\\xe1\\xba\\xa9n x\\xc3\\xa1c b\\xe1\\xba\\xb1ng MTCNN v\\xc3\\xa0 Facenet!\\n-Ch\\xe1\\xba\\xa1y tr\\xc3\\xaan Tensorflow 2.x\\n-\\n-Article link: http://miai.vn/2019/09/11/face-recog-2-0-nhan-dien-khuon-mat-trong-video-bang-mtcnn-va-facenet/\\n-\\n-#M\\xc3\\xacAI \\n-Fanpage: http://facebook.com/miaiblog<br>\\n-Group trao \\xc4\\x91\\xe1\\xbb\\x95i, chia s\\xe1\\xba\\xbb: https://www.facebook.com/groups/miaigroup<br>\\n-Website: http://ainoodle.tech<br>\\n-Youtube: http://bit.ly/miaiyoutube<br>\\ndiff --git a/src/a b/src/a\\ndeleted file mode 100644\\nindex 8b13789..0000000\\n--- a/src/a\\n+++ /dev/null\\n@@ -1 +0,0 @@\\n-\\ndiff --git a/src/calculate_filtering_metrics.py b/src/calculate_filtering_metrics.py\\ndeleted file mode 100644\\nindex f60b9ae..0000000\\n--- a/src/calculate_filtering_metrics.py\\n+++ /dev/null\\n@@ -1,128 +0,0 @@\\n-"""Calculate filtering metrics for a dataset and store in a .hdf file.\\n-"""\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-import tensorflow as tf\\n-import numpy as np\\n-import argparse\\n-import facenet\\n-import os\\n-import sys\\n-import time\\n-import h5py\\n-import math\\n-from tensorflow.python.platform import gfile\\n-from six import iteritems\\n-\\n-def main(args):\\n-    dataset = facenet.get_dataset(args.dataset_dir)\\n-  \\n-    with tf.Graph().as_default():\\n-      \\n-        # Get a list of image paths and their labels\\n-        image_list, label_list = facenet.get_image_paths_and_labels(dataset)\\n-        nrof_images = len(image_list)\\n-        image_indices = range(nrof_images)\\n-\\n-        image_batch, label_batch = facenet.read_and_augment_data(image_list,\\n-            image_indices, args.image_size, args.batch_size, None, \\n-            False, False, False, nrof_preprocess_threads=4, shuffle=False)\\n-        \\n-        model_exp = os.path.expanduser(args.model_file)\\n-        with gfile.FastGFile(model_exp,\\\'rb\\\') as f:\\n-            graph_def = tf.GraphDef()\\n-            graph_def.ParseFromString(f.read())\\n-            input_map={\\\'input\\\':image_batch, \\\'phase_train\\\':False}\\n-            tf.import_graph_def(graph_def, input_map=input_map, name=\\\'net\\\')\\n-        \\n-        embeddings = tf.get_default_graph().get_tensor_by_name("net/embeddings:0")\\n-\\n-        with tf.Session() as sess:\\n-            tf.train.start_queue_runners(sess=sess)\\n-                \\n-            embedding_size = int(embeddings.get_shape()[1])\\n-            nrof_batches = int(math.ceil(nrof_images / args.batch_size))\\n-            nrof_classes = len(dataset)\\n-            label_array = np.array(label_list)\\n-            class_names = [cls.name for cls in dataset]\\n-            nrof_examples_per_class = [ len(cls.image_paths) for cls in dataset ]\\n-            class_variance = np.zeros((nrof_classes,))\\n-            class_center = np.zeros((nrof_classes,embedding_size))\\n-            distance_to_center = np.ones((len(label_list),))*np.NaN\\n-            emb_array = np.zeros((0,embedding_size))\\n-            idx_array = np.zeros((0,), dtype=np.int32)\\n-            lab_array = np.zeros((0,), dtype=np.int32)\\n-            index_arr = np.append(0, np.cumsum(nrof_examples_per_class))\\n-            for i in range(nrof_batches):\\n-                t = time.time()\\n-                emb, idx = sess.run([embeddings, label_batch])\\n-                emb_array = np.append(emb_array, emb, axis=0)\\n-                idx_array = np.append(idx_array, idx, axis=0)\\n-                lab_array = np.append(lab_array, label_array[idx], axis=0)\\n-                for cls in set(lab_array):\\n-                    cls_idx = np.where(lab_array==cls)[0]\\n-                    if cls_idx.shape[0]==nrof_examples_per_class[cls]:\\n-                        # We have calculated all the embeddings for this class\\n-                        i2 = np.argsort(idx_array[cls_idx])\\n-                        emb_class = emb_array[cls_idx,:]\\n-                        emb_sort = emb_class[i2,:]\\n-                        center = np.mean(emb_sort, axis=0)\\n-                        diffs = emb_sort - center\\n-                        dists_sqr = np.sum(np.square(diffs), axis=1)\\n-                        class_variance[cls] = np.mean(dists_sqr)\\n-                        class_center[cls,:] = center\\n-                        distance_to_center[index_arr[cls]:index_arr[cls+1]] = np.sqrt(dists_sqr)\\n-                        emb_array = np.delete(emb_array, cls_idx, axis=0)\\n-                        idx_array = np.delete(idx_array, cls_idx, axis=0)\\n-                        lab_array = np.delete(lab_array, cls_idx, axis=0)\\n-\\n-                        \\n-                print(\\\'Batch %d in %.3f seconds\\\' % (i, time.time()-t))\\n-                \\n-            print(\\\'Writing filtering data to %s\\\' % args.data_file_name)\\n-            mdict = {\\\'class_names\\\':class_names, \\\'image_list\\\':image_list, \\\'label_list\\\':label_list, \\\'distance_to_center\\\':distance_to_center }\\n-            with h5py.File(args.data_file_name, \\\'w\\\') as f:\\n-                for key, value in iteritems(mdict):\\n-                    f.create_dataset(key, data=value)\\n-                        \\n-def parse_arguments(argv):\\n-    parser = argparse.ArgumentParser()\\n-    \\n-    parser.add_argument(\\\'dataset_dir\\\', type=str,\\n-        help=\\\'Path to the directory containing aligned dataset.\\\')\\n-    parser.add_argument(\\\'model_file\\\', type=str,\\n-        help=\\\'File containing the frozen model in protobuf (.pb) format to use for feature extraction.\\\')\\n-    parser.add_argument(\\\'data_file_name\\\', type=str,\\n-        help=\\\'The name of the file to store filtering data in.\\\')\\n-    parser.add_argument(\\\'--image_size\\\', type=int,\\n-        help=\\\'Image size.\\\', default=160)\\n-    parser.add_argument(\\\'--batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch.\\\', default=90)\\n-    return parser.parse_args(argv)\\n-\\n-if __name__ == \\\'__main__\\\':\\n-    main(parse_arguments(sys.argv[1:]))\\ndiff --git a/src/decode_msceleb_dataset.py b/src/decode_msceleb_dataset.py\\ndeleted file mode 100644\\nindex 4556bfa..0000000\\n--- a/src/decode_msceleb_dataset.py\\n+++ /dev/null\\n@@ -1,87 +0,0 @@\\n-"""Decode the MsCelebV1 dataset in TSV (tab separated values) format downloaded from\\n-https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/\\n-"""\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-from scipy import misc\\n-import numpy as np\\n-import base64\\n-import sys\\n-import os\\n-import cv2\\n-import argparse\\n-import facenet\\n-\\n-\\n-# File format: text files, each line is an image record containing 6 columns, delimited by TAB.\\n-# Column1: Freebase MID\\n-# Column2: Query/Name\\n-# Column3: ImageSearchRank\\n-# Column4: ImageURL\\n-# Column5: PageURL\\n-# Column6: ImageData_Base64Encoded\\n-\\n-def main(args):\\n-    output_dir = os.path.expanduser(args.output_dir)\\n-  \\n-    if not os.path.exists(output_dir):\\n-        os.mkdir(output_dir)\\n-  \\n-    # Store some git revision info in a text file in the output directory\\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\\n-    facenet.store_revision_info(src_path, output_dir, \\\' \\\'.join(sys.argv))\\n-    \\n-    i = 0\\n-    for f in args.tsv_files:\\n-        for line in f:\\n-            fields = line.split(\\\'\\\\t\\\')\\n-            class_dir = fields[0]\\n-            img_name = fields[1] + \\\'-\\\' + fields[4] + \\\'.\\\' + args.output_format\\n-            img_string = fields[5]\\n-            img_dec_string = base64.b64decode(img_string)\\n-            img_data = np.fromstring(img_dec_string, dtype=np.uint8)\\n-            img = cv2.imdecode(img_data, cv2.IMREAD_COLOR) #pylint: disable=maybe-no-member\\n-            if args.size:\\n-                img = misc.imresize(img, (args.size, args.size), interp=\\\'bilinear\\\')\\n-            full_class_dir = os.path.join(output_dir, class_dir)\\n-            if not os.path.exists(full_class_dir):\\n-                os.mkdir(full_class_dir)\\n-            full_path = os.path.join(full_class_dir, img_name.replace(\\\'/\\\',\\\'_\\\'))\\n-            cv2.imwrite(full_path, img) #pylint: disable=maybe-no-member\\n-            print(\\\'%8d: %s\\\' % (i, full_path))\\n-            i += 1\\n-  \\n-if __name__ == \\\'__main__\\\':\\n-    parser = argparse.ArgumentParser()\\n-\\n-    parser.add_argument(\\\'output_dir\\\', type=str, help=\\\'Output base directory for the image dataset\\\')\\n-    parser.add_argument(\\\'tsv_files\\\', type=argparse.FileType(\\\'r\\\'), nargs=\\\'+\\\', help=\\\'Input TSV file name(s)\\\')\\n-    parser.add_argument(\\\'--size\\\', type=int, help=\\\'Images are resized to the given size\\\')\\n-    parser.add_argument(\\\'--output_format\\\', type=str, help=\\\'Format of the output images\\\', default=\\\'png\\\', choices=[\\\'png\\\', \\\'jpg\\\'])\\n-\\n-    main(parser.parse_args())\\n-\\ndiff --git a/src/download_and_extract.py b/src/download_and_extract.py\\ndeleted file mode 100644\\nindex a835ac2..0000000\\n--- a/src/download_and_extract.py\\n+++ /dev/null\\n@@ -1,51 +0,0 @@\\n-import requests\\n-import zipfile\\n-import os\\n-\\n-model_dict = {\\n-    \\\'lfw-subset\\\':      \\\'1B5BQUZuJO-paxdN8UclxeHAR1WnR_Tzi\\\', \\n-    \\\'20170131-234652\\\': \\\'0B5MzpY9kBtDVSGM0RmVET2EwVEk\\\',\\n-    \\\'20170216-091149\\\': \\\'0B5MzpY9kBtDVTGZjcWkzT3pldDA\\\',\\n-    \\\'20170512-110547\\\': \\\'0B5MzpY9kBtDVZ2RpVDYwWmxoSUk\\\',\\n-    \\\'20180402-114759\\\': \\\'1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-\\\'\\n-    }\\n-\\n-def download_and_extract_file(model_name, data_dir):\\n-    file_id = model_dict[model_name]\\n-    destination = os.path.join(data_dir, model_name + \\\'.zip\\\')\\n-    if not os.path.exists(destination):\\n-        print(\\\'Downloading file to %s\\\' % destination)\\n-        download_file_from_google_drive(file_id, destination)\\n-        with zipfile.ZipFile(destination, \\\'r\\\') as zip_ref:\\n-            print(\\\'Extracting file to %s\\\' % data_dir)\\n-            zip_ref.extractall(data_dir)\\n-\\n-def download_file_from_google_drive(file_id, destination):\\n-    \\n-        URL = "https://drive.google.com/uc?export=download"\\n-    \\n-        session = requests.Session()\\n-    \\n-        response = session.get(URL, params = { \\\'id\\\' : file_id }, stream = True)\\n-        token = get_confirm_token(response)\\n-    \\n-        if token:\\n-            params = { \\\'id\\\' : file_id, \\\'confirm\\\' : token }\\n-            response = session.get(URL, params = params, stream = True)\\n-    \\n-        save_response_content(response, destination)    \\n-\\n-def get_confirm_token(response):\\n-    for key, value in response.cookies.items():\\n-        if key.startswith(\\\'download_warning\\\'):\\n-            return value\\n-\\n-    return None\\n-\\n-def save_response_content(response, destination):\\n-    CHUNK_SIZE = 32768\\n-\\n-    with open(destination, "wb") as f:\\n-        for chunk in response.iter_content(CHUNK_SIZE):\\n-            if chunk: # filter out keep-alive new chunks\\n-                f.write(chunk)\\ndiff --git a/src/face_rec.py b/src/face_rec.py\\ndeleted file mode 100644\\nindex f92cccf..0000000\\n--- a/src/face_rec.py\\n+++ /dev/null\\n@@ -1,135 +0,0 @@\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-import tensorflow as tf\\n-import argparse\\n-import facenet\\n-import os\\n-import sys\\n-import math\\n-import pickle\\n-import align.detect_face\\n-import numpy as np\\n-import cv2\\n-import collections\\n-from sklearn.svm import SVC\\n-\\n-\\n-def main():\\n-    parser = argparse.ArgumentParser()\\n-    parser.add_argument(\\\'--path\\\', help=\\\'Path of the video you want to test on.\\\', default=0)\\n-    args = parser.parse_args()\\n-    \\n-    # Cai dat cac tham so can thiet\\n-    MINSIZE = 20\\n-    THRESHOLD = [0.6, 0.7, 0.7]\\n-    FACTOR = 0.709\\n-    IMAGE_SIZE = 182\\n-    INPUT_IMAGE_SIZE = 160\\n-    CLASSIFIER_PATH = \\\'Models/facemodel.pkl\\\'\\n-    VIDEO_PATH = args.path\\n-    FACENET_MODEL_PATH = \\\'Models/20180402-114759.pb\\\'\\n-\\n-    # Load model da train de nhan dien khuon mat - thuc chat la classifier\\n-    with open(CLASSIFIER_PATH, \\\'rb\\\') as file:\\n-        model, class_names = pickle.load(file)\\n-    print("Custom Classifier, Successfully loaded")\\n-\\n-    with tf.Graph().as_default():\\n-\\n-        # Cai dat GPU neu co\\n-        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\n-\\n-        with sess.as_default():\\n-\\n-            # Load model MTCNN phat hien khuon mat\\n-            print(\\\'Loading feature extraction model\\\')\\n-            facenet.load_model(FACENET_MODEL_PATH)\\n-\\n-            # Lay tensor input va output\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\n-            embedding_size = embeddings.get_shape()[1]\\n-\\n-            # Cai dat cac mang con\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\n-\\n-            people_detected = set()\\n-            person_detected = collections.Counter()\\n-\\n-            # Lay hinh anh tu file video\\n-            cap = cv2.VideoCapture(VIDEO_PATH)\\n-\\n-            while (cap.isOpened()):\\n-                # Doc tung frame\\n-                ret, frame = cap.read()\\n-\\n-                # Phat hien khuon mat, tra ve vi tri trong bounding_boxes\\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\n-\\n-                faces_found = bounding_boxes.shape[0]\\n-                try:\\n-                    # Neu co it nhat 1 khuon mat trong frame\\n-                    if faces_found > 0:\\n-                        det = bounding_boxes[:, 0:4]\\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\\n-                        for i in range(faces_found):\\n-                            bb[i][0] = det[i][0]\\n-                            bb[i][1] = det[i][1]\\n-                            bb[i][2] = det[i][2]\\n-                            bb[i][3] = det[i][3]\\n-\\n-                            # Cat phan khuon mat tim duoc\\n-                            cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\n-                            scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\n-                                                interpolation=cv2.INTER_CUBIC)\\n-                            scaled = facenet.prewhiten(scaled)\\n-                            scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\n-                            feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\n-                            emb_array = sess.run(embeddings, feed_dict=feed_dict)\\n-                            \\n-                            # Dua vao model de classifier\\n-                            predictions = model.predict_proba(emb_array)\\n-                            best_class_indices = np.argmax(predictions, axis=1)\\n-                            best_class_probabilities = predictions[\\n-                                np.arange(len(best_class_indices)), best_class_indices]\\n-                            \\n-                            # Lay ra ten va ty le % cua class co ty le cao nhat\\n-                            best_name = class_names[best_class_indices[0]]\\n-                            print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\n-\\n-                            # Ve khung mau xanh quanh khuon mat\\n-                            cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\n-                            text_x = bb[i][0]\\n-                            text_y = bb[i][3] + 20\\n-\\n-                            # Neu ty le nhan dang > 0.5 thi hien thi ten\\n-                            if best_class_probabilities > 0.5:\\n-                                name = class_names[best_class_indices[0]]\\n-                            else:\\n-                                # Con neu <=0.5 thi hien thi Unknow\\n-                                name = "Unknown"\\n-                                \\n-                            # Viet text len tren frame    \\n-                            cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\n-                                        1, (255, 255, 255), thickness=1, lineType=2)\\n-                            cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\\n-                                        cv2.FONT_HERSHEY_COMPLEX_SMALL,\\n-                                        1, (255, 255, 255), thickness=1, lineType=2)\\n-                            person_detected[best_name] += 1\\n-                except:\\n-                    pass\\n-\\n-                # Hien thi frame len man hinh\\n-                cv2.imshow(\\\'Face Recognition\\\', frame)\\n-                if cv2.waitKey(1) & 0xFF == ord(\\\'q\\\'):\\n-                    break\\n-\\n-            cap.release()\\n-            cv2.destroyAllWindows()\\n-\\n-\\n-main()\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\nindex 1a425a5..455f67a 100644\\n--- a/src/face_rec_cam.py\\n+++ b/src/face_rec_cam.py\\n@@ -52,9 +52,10 @@ def main():\\n             facenet.load_model(FACENET_MODEL_PATH)\\n \\n             # Get input and output tensors\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\n+            graph = tf.compat.v1.get_default_graph()\\n+            images_placeholder = graph.get_tensor_by_name("input:0")\\n+            embeddings = graph.get_tensor_by_name("embeddings:0")\\n+            phase_train_placeholder = graph.get_tensor_by_name("phase_train:0")\\n             embedding_size = embeddings.get_shape()[1]\\n \\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\ndiff --git a/src/freeze_graph.py b/src/freeze_graph.py\\ndeleted file mode 100644\\nindex 3584c18..0000000\\n--- a/src/freeze_graph.py\\n+++ /dev/null\\n@@ -1,103 +0,0 @@\\n-"""Imports a model metagraph and checkpoint file, converts the variables to constants\\n-and exports the model as a graphdef protobuf\\n-"""\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-from tensorflow.python.framework import graph_util\\n-import tensorflow as tf\\n-import argparse\\n-import os\\n-import sys\\n-import facenet\\n-from six.moves import xrange  # @UnresolvedImport\\n-\\n-def main(args):\\n-    with tf.Graph().as_default():\\n-        with tf.Session() as sess:\\n-            # Load the model metagraph and checkpoint\\n-            print(\\\'Model directory: %s\\\' % args.model_dir)\\n-            meta_file, ckpt_file = facenet.get_model_filenames(os.path.expanduser(args.model_dir))\\n-            \\n-            print(\\\'Metagraph file: %s\\\' % meta_file)\\n-            print(\\\'Checkpoint file: %s\\\' % ckpt_file)\\n-\\n-            model_dir_exp = os.path.expanduser(args.model_dir)\\n-            saver = tf.train.import_meta_graph(os.path.join(model_dir_exp, meta_file), clear_devices=True)\\n-            tf.get_default_session().run(tf.global_variables_initializer())\\n-            tf.get_default_session().run(tf.local_variables_initializer())\\n-            saver.restore(tf.get_default_session(), os.path.join(model_dir_exp, ckpt_file))\\n-            \\n-            # Retrieve the protobuf graph definition and fix the batch norm nodes\\n-            input_graph_def = sess.graph.as_graph_def()\\n-            \\n-            # Freeze the graph def\\n-            output_graph_def = freeze_graph_def(sess, input_graph_def, \\\'embeddings,label_batch\\\')\\n-\\n-        # Serialize and dump the output graph to the filesystem\\n-        with tf.gfile.GFile(args.output_file, \\\'wb\\\') as f:\\n-            f.write(output_graph_def.SerializeToString())\\n-        print("%d ops in the final graph: %s" % (len(output_graph_def.node), args.output_file))\\n-        \\n-def freeze_graph_def(sess, input_graph_def, output_node_names):\\n-    for node in input_graph_def.node:\\n-        if node.op == \\\'RefSwitch\\\':\\n-            node.op = \\\'Switch\\\'\\n-            for index in xrange(len(node.input)):\\n-                if \\\'moving_\\\' in node.input[index]:\\n-                    node.input[index] = node.input[index] + \\\'/read\\\'\\n-        elif node.op == \\\'AssignSub\\\':\\n-            node.op = \\\'Sub\\\'\\n-            if \\\'use_locking\\\' in node.attr: del node.attr[\\\'use_locking\\\']\\n-        elif node.op == \\\'AssignAdd\\\':\\n-            node.op = \\\'Add\\\'\\n-            if \\\'use_locking\\\' in node.attr: del node.attr[\\\'use_locking\\\']\\n-    \\n-    # Get the list of important nodes\\n-    whitelist_names = []\\n-    for node in input_graph_def.node:\\n-        if (node.name.startswith(\\\'InceptionResnet\\\') or node.name.startswith(\\\'embeddings\\\') or \\n-                node.name.startswith(\\\'image_batch\\\') or node.name.startswith(\\\'label_batch\\\') or\\n-                node.name.startswith(\\\'phase_train\\\') or node.name.startswith(\\\'Logits\\\')):\\n-            whitelist_names.append(node.name)\\n-\\n-    # Replace all the variables in the graph with constants of the same values\\n-    output_graph_def = graph_util.convert_variables_to_constants(\\n-        sess, input_graph_def, output_node_names.split(","),\\n-        variable_names_whitelist=whitelist_names)\\n-    return output_graph_def\\n-  \\n-def parse_arguments(argv):\\n-    parser = argparse.ArgumentParser()\\n-    \\n-    parser.add_argument(\\\'model_dir\\\', type=str, \\n-        help=\\\'Directory containing the metagraph (.meta) file and the checkpoint (ckpt) file containing model parameters\\\')\\n-    parser.add_argument(\\\'output_file\\\', type=str, \\n-        help=\\\'Filename for the exported graphdef protobuf (.pb)\\\')\\n-    return parser.parse_args(argv)\\n-\\n-if __name__ == \\\'__main__\\\':\\n-    main(parse_arguments(sys.argv[1:]))\\ndiff --git a/src/lfw.py b/src/lfw.py\\ndeleted file mode 100644\\nindex 9194433..0000000\\n--- a/src/lfw.py\\n+++ /dev/null\\n@@ -1,86 +0,0 @@\\n-"""Helper for evaluation on the Labeled Faces in the Wild dataset \\n-"""\\n-\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-import os\\n-import numpy as np\\n-import facenet\\n-\\n-def evaluate(embeddings, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\\n-    # Calculate evaluation metrics\\n-    thresholds = np.arange(0, 4, 0.01)\\n-    embeddings1 = embeddings[0::2]\\n-    embeddings2 = embeddings[1::2]\\n-    tpr, fpr, accuracy = facenet.calculate_roc(thresholds, embeddings1, embeddings2,\\n-        np.asarray(actual_issame), nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\\n-    thresholds = np.arange(0, 4, 0.001)\\n-    val, val_std, far = facenet.calculate_val(thresholds, embeddings1, embeddings2,\\n-        np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\\n-    return tpr, fpr, accuracy, val, val_std, far\\n-\\n-def get_paths(lfw_dir, pairs):\\n-    nrof_skipped_pairs = 0\\n-    path_list = []\\n-    issame_list = []\\n-    for pair in pairs:\\n-        if len(pair) == 3:\\n-            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \\\'_\\\' + \\\'%04d\\\' % int(pair[1])))\\n-            path1 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \\\'_\\\' + \\\'%04d\\\' % int(pair[2])))\\n-            issame = True\\n-        elif len(pair) == 4:\\n-            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \\\'_\\\' + \\\'%04d\\\' % int(pair[1])))\\n-            path1 = add_extension(os.path.join(lfw_dir, pair[2], pair[2] + \\\'_\\\' + \\\'%04d\\\' % int(pair[3])))\\n-            issame = False\\n-        if os.path.exists(path0) and os.path.exists(path1):    # Only add the pair if both paths exist\\n-            path_list += (path0,path1)\\n-            issame_list.append(issame)\\n-        else:\\n-            nrof_skipped_pairs += 1\\n-    if nrof_skipped_pairs>0:\\n-        print(\\\'Skipped %d image pairs\\\' % nrof_skipped_pairs)\\n-    \\n-    return path_list, issame_list\\n-  \\n-def add_extension(path):\\n-    if os.path.exists(path+\\\'.jpg\\\'):\\n-        return path+\\\'.jpg\\\'\\n-    elif os.path.exists(path+\\\'.png\\\'):\\n-        return path+\\\'.png\\\'\\n-    else:\\n-        raise RuntimeError(\\\'No file "%s" with extension png or jpg.\\\' % path)\\n-\\n-def read_pairs(pairs_filename):\\n-    pairs = []\\n-    with open(pairs_filename, \\\'r\\\') as f:\\n-        for line in f.readlines()[1:]:\\n-            pair = line.strip().split()\\n-            pairs.append(pair)\\n-    return np.array(pairs)\\n-\\n-\\n-\\ndiff --git a/src/train_softmax.py b/src/train_softmax.py\\ndeleted file mode 100644\\nindex 6b0b28b..0000000\\n--- a/src/train_softmax.py\\n+++ /dev/null\\n@@ -1,580 +0,0 @@\\n-"""Training a face recognizer with TensorFlow using softmax cross entropy loss\\n-"""\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-from datetime import datetime\\n-import os.path\\n-import time\\n-import sys\\n-import random\\n-import tensorflow as tf\\n-import numpy as np\\n-import importlib\\n-import argparse\\n-import facenet\\n-import lfw\\n-import h5py\\n-import math\\n-import tensorflow.contrib.slim as slim\\n-from tensorflow.python.ops import data_flow_ops\\n-from tensorflow.python.framework import ops\\n-from tensorflow.python.ops import array_ops\\n-\\n-def main(args):\\n-  \\n-    network = importlib.import_module(args.model_def)\\n-    image_size = (args.image_size, args.image_size)\\n-\\n-    subdir = datetime.strftime(datetime.now(), \\\'%Y%m%d-%H%M%S\\\')\\n-    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\\n-    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\\\'t exist\\n-        os.makedirs(log_dir)\\n-    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\\n-    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\\\'t exist\\n-        os.makedirs(model_dir)\\n-\\n-    stat_file_name = os.path.join(log_dir, \\\'stat.h5\\\')\\n-\\n-    # Write arguments to a text file\\n-    facenet.write_arguments_to_file(args, os.path.join(log_dir, \\\'arguments.txt\\\'))\\n-        \\n-    # Store some git revision info in a text file in the log directory\\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\\n-    facenet.store_revision_info(src_path, log_dir, \\\' \\\'.join(sys.argv))\\n-\\n-    np.random.seed(seed=args.seed)\\n-    random.seed(args.seed)\\n-    dataset = facenet.get_dataset(args.data_dir)\\n-    if args.filter_filename:\\n-        dataset = filter_dataset(dataset, os.path.expanduser(args.filter_filename), \\n-            args.filter_percentile, args.filter_min_nrof_images_per_class)\\n-        \\n-    if args.validation_set_split_ratio>0.0:\\n-        train_set, val_set = facenet.split_dataset(dataset, args.validation_set_split_ratio, args.min_nrof_val_images_per_class, \\\'SPLIT_IMAGES\\\')\\n-    else:\\n-        train_set, val_set = dataset, []\\n-        \\n-    nrof_classes = len(train_set)\\n-    \\n-    print(\\\'Model directory: %s\\\' % model_dir)\\n-    print(\\\'Log directory: %s\\\' % log_dir)\\n-    pretrained_model = None\\n-    if args.pretrained_model:\\n-        pretrained_model = os.path.expanduser(args.pretrained_model)\\n-        print(\\\'Pre-trained model: %s\\\' % pretrained_model)\\n-    \\n-    if args.lfw_dir:\\n-        print(\\\'LFW directory: %s\\\' % args.lfw_dir)\\n-        # Read the file containing the pairs used for testing\\n-        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\\n-        # Get the paths for the corresponding images\\n-        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\\n-    \\n-    with tf.Graph().as_default():\\n-        tf.set_random_seed(args.seed)\\n-        global_step = tf.Variable(0, trainable=False)\\n-        \\n-        # Get a list of image paths and their labels\\n-        image_list, label_list = facenet.get_image_paths_and_labels(train_set)\\n-        assert len(image_list)>0, \\\'The training set should not be empty\\\'\\n-        \\n-        val_image_list, val_label_list = facenet.get_image_paths_and_labels(val_set)\\n-\\n-        # Create a queue that produces indices into the image_list and label_list \\n-        labels = ops.convert_to_tensor(label_list, dtype=tf.int32)\\n-        range_size = array_ops.shape(labels)[0]\\n-        index_queue = tf.train.range_input_producer(range_size, num_epochs=None,\\n-                             shuffle=True, seed=None, capacity=32)\\n-        \\n-        index_dequeue_op = index_queue.dequeue_many(args.batch_size*args.epoch_size, \\\'index_dequeue\\\')\\n-        \\n-        learning_rate_placeholder = tf.placeholder(tf.float32, name=\\\'learning_rate\\\')\\n-        batch_size_placeholder = tf.placeholder(tf.int32, name=\\\'batch_size\\\')\\n-        phase_train_placeholder = tf.placeholder(tf.bool, name=\\\'phase_train\\\')\\n-        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\\\'image_paths\\\')\\n-        labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\\\'labels\\\')\\n-        control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\\\'control\\\')\\n-        \\n-        nrof_preprocess_threads = 4\\n-        input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\\n-                                    dtypes=[tf.string, tf.int32, tf.int32],\\n-                                    shapes=[(1,), (1,), (1,)],\\n-                                    shared_name=None, name=None)\\n-        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder, control_placeholder], name=\\\'enqueue_op\\\')\\n-        image_batch, label_batch = facenet.create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder)\\n-\\n-        image_batch = tf.identity(image_batch, \\\'image_batch\\\')\\n-        image_batch = tf.identity(image_batch, \\\'input\\\')\\n-        label_batch = tf.identity(label_batch, \\\'label_batch\\\')\\n-        \\n-        print(\\\'Number of classes in training set: %d\\\' % nrof_classes)\\n-        print(\\\'Number of examples in training set: %d\\\' % len(image_list))\\n-\\n-        print(\\\'Number of classes in validation set: %d\\\' % len(val_set))\\n-        print(\\\'Number of examples in validation set: %d\\\' % len(val_image_list))\\n-        \\n-        print(\\\'Building training graph\\\')\\n-        \\n-        # Build the inference graph\\n-        prelogits, _ = network.inference(image_batch, args.keep_probability, \\n-            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size, \\n-            weight_decay=args.weight_decay)\\n-        logits = slim.fully_connected(prelogits, len(train_set), activation_fn=None, \\n-                weights_initializer=slim.initializers.xavier_initializer(), \\n-                weights_regularizer=slim.l2_regularizer(args.weight_decay),\\n-                scope=\\\'Logits\\\', reuse=False)\\n-\\n-        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\\\'embeddings\\\')\\n-\\n-        # Norm for the prelogits\\n-        eps = 1e-4\\n-        prelogits_norm = tf.reduce_mean(tf.norm(tf.abs(prelogits)+eps, ord=args.prelogits_norm_p, axis=1))\\n-        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_norm * args.prelogits_norm_loss_factor)\\n-\\n-        # Add center loss\\n-        prelogits_center_loss, _ = facenet.center_loss(prelogits, label_batch, args.center_loss_alfa, nrof_classes)\\n-        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_center_loss * args.center_loss_factor)\\n-\\n-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\\n-            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\\n-        tf.summary.scalar(\\\'learning_rate\\\', learning_rate)\\n-\\n-        # Calculate the average cross entropy loss across the batch\\n-        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\\n-            labels=label_batch, logits=logits, name=\\\'cross_entropy_per_example\\\')\\n-        cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\\\'cross_entropy\\\')\\n-        tf.add_to_collection(\\\'losses\\\', cross_entropy_mean)\\n-        \\n-        correct_prediction = tf.cast(tf.equal(tf.argmax(logits, 1), tf.cast(label_batch, tf.int64)), tf.float32)\\n-        accuracy = tf.reduce_mean(correct_prediction)\\n-        \\n-        # Calculate the total losses\\n-        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\\n-        total_loss = tf.add_n([cross_entropy_mean] + regularization_losses, name=\\\'total_loss\\\')\\n-\\n-        # Build a Graph that trains the model with one batch of examples and updates the model parameters\\n-        train_op = facenet.train(total_loss, global_step, args.optimizer, \\n-            learning_rate, args.moving_average_decay, tf.global_variables(), args.log_histograms)\\n-        \\n-        # Create a saver\\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\\n-\\n-        # Build the summary operation based on the TF collection of Summaries.\\n-        summary_op = tf.summary.merge_all()\\n-\\n-        # Start running operations on the Graph.\\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\n-        sess.run(tf.global_variables_initializer())\\n-        sess.run(tf.local_variables_initializer())\\n-        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\\n-        coord = tf.train.Coordinator()\\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\\n-\\n-        with sess.as_default():\\n-\\n-            if pretrained_model:\\n-                print(\\\'Restoring pretrained model: %s\\\' % pretrained_model)\\n-                saver.restore(sess, pretrained_model)\\n-\\n-            # Training and validation loop\\n-            print(\\\'Running training\\\')\\n-            nrof_steps = args.max_nrof_epochs*args.epoch_size\\n-            nrof_val_samples = int(math.ceil(args.max_nrof_epochs / args.validate_every_n_epochs))   # Validate every validate_every_n_epochs as well as in the last epoch\\n-            stat = {\\n-                \\\'loss\\\': np.zeros((nrof_steps,), np.float32),\\n-                \\\'center_loss\\\': np.zeros((nrof_steps,), np.float32),\\n-                \\\'reg_loss\\\': np.zeros((nrof_steps,), np.float32),\\n-                \\\'xent_loss\\\': np.zeros((nrof_steps,), np.float32),\\n-                \\\'prelogits_norm\\\': np.zeros((nrof_steps,), np.float32),\\n-                \\\'accuracy\\\': np.zeros((nrof_steps,), np.float32),\\n-                \\\'val_loss\\\': np.zeros((nrof_val_samples,), np.float32),\\n-                \\\'val_xent_loss\\\': np.zeros((nrof_val_samples,), np.float32),\\n-                \\\'val_accuracy\\\': np.zeros((nrof_val_samples,), np.float32),\\n-                \\\'lfw_accuracy\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\n-                \\\'lfw_valrate\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\n-                \\\'learning_rate\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\n-                \\\'time_train\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\n-                \\\'time_validate\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\n-                \\\'time_evaluate\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\n-                \\\'prelogits_hist\\\': np.zeros((args.max_nrof_epochs, 1000), np.float32),\\n-              }\\n-            for epoch in range(1,args.max_nrof_epochs+1):\\n-                step = sess.run(global_step, feed_dict=None)\\n-                # Train for one epoch\\n-                t = time.time()\\n-                cont = train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder,\\n-                    learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, global_step, \\n-                    total_loss, train_op, summary_op, summary_writer, regularization_losses, args.learning_rate_schedule_file,\\n-                    stat, cross_entropy_mean, accuracy, learning_rate,\\n-                    prelogits, prelogits_center_loss, args.random_rotate, args.random_crop, args.random_flip, prelogits_norm, args.prelogits_hist_max, args.use_fixed_image_standardization)\\n-                stat[\\\'time_train\\\'][epoch-1] = time.time() - t\\n-                \\n-                if not cont:\\n-                    break\\n-                  \\n-                t = time.time()\\n-                if len(val_image_list)>0 and ((epoch-1) % args.validate_every_n_epochs == args.validate_every_n_epochs-1 or epoch==args.max_nrof_epochs):\\n-                    validate(args, sess, epoch, val_image_list, val_label_list, enqueue_op, image_paths_placeholder, labels_placeholder, control_placeholder,\\n-                        phase_train_placeholder, batch_size_placeholder, \\n-                        stat, total_loss, regularization_losses, cross_entropy_mean, accuracy, args.validate_every_n_epochs, args.use_fixed_image_standardization)\\n-                stat[\\\'time_validate\\\'][epoch-1] = time.time() - t\\n-\\n-                # Save variables and the metagraph if it doesn\\\'t exist already\\n-                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, epoch)\\n-\\n-                # Evaluate on LFW\\n-                t = time.time()\\n-                if args.lfw_dir:\\n-                    evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, \\n-                        embeddings, label_batch, lfw_paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, log_dir, step, summary_writer, stat, epoch, \\n-                        args.lfw_distance_metric, args.lfw_subtract_mean, args.lfw_use_flipped_images, args.use_fixed_image_standardization)\\n-                stat[\\\'time_evaluate\\\'][epoch-1] = time.time() - t\\n-\\n-                print(\\\'Saving statistics\\\')\\n-                with h5py.File(stat_file_name, \\\'w\\\') as f:\\n-                    for key, value in stat.iteritems():\\n-                        f.create_dataset(key, data=value)\\n-    \\n-    return model_dir\\n-  \\n-def find_threshold(var, percentile):\\n-    hist, bin_edges = np.histogram(var, 100)\\n-    cdf = np.float32(np.cumsum(hist)) / np.sum(hist)\\n-    bin_centers = (bin_edges[:-1]+bin_edges[1:])/2\\n-    #plt.plot(bin_centers, cdf)\\n-    threshold = np.interp(percentile*0.01, cdf, bin_centers)\\n-    return threshold\\n-  \\n-def filter_dataset(dataset, data_filename, percentile, min_nrof_images_per_class):\\n-    with h5py.File(data_filename,\\\'r\\\') as f:\\n-        distance_to_center = np.array(f.get(\\\'distance_to_center\\\'))\\n-        label_list = np.array(f.get(\\\'label_list\\\'))\\n-        image_list = np.array(f.get(\\\'image_list\\\'))\\n-        distance_to_center_threshold = find_threshold(distance_to_center, percentile)\\n-        indices = np.where(distance_to_center>=distance_to_center_threshold)[0]\\n-        filtered_dataset = dataset\\n-        removelist = []\\n-        for i in indices:\\n-            label = label_list[i]\\n-            image = image_list[i]\\n-            if image in filtered_dataset[label].image_paths:\\n-                filtered_dataset[label].image_paths.remove(image)\\n-            if len(filtered_dataset[label].image_paths)<min_nrof_images_per_class:\\n-                removelist.append(label)\\n-\\n-        ix = sorted(list(set(removelist)), reverse=True)\\n-        for i in ix:\\n-            del(filtered_dataset[i])\\n-\\n-    return filtered_dataset\\n-  \\n-def train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder, \\n-      learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, step, \\n-      loss, train_op, summary_op, summary_writer, reg_losses, learning_rate_schedule_file, \\n-      stat, cross_entropy_mean, accuracy, \\n-      learning_rate, prelogits, prelogits_center_loss, random_rotate, random_crop, random_flip, prelogits_norm, prelogits_hist_max, use_fixed_image_standardization):\\n-    batch_number = 0\\n-    \\n-    if args.learning_rate>0.0:\\n-        lr = args.learning_rate\\n-    else:\\n-        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\\n-        \\n-    if lr<=0:\\n-        return False \\n-\\n-    index_epoch = sess.run(index_dequeue_op)\\n-    label_epoch = np.array(label_list)[index_epoch]\\n-    image_epoch = np.array(image_list)[index_epoch]\\n-    \\n-    # Enqueue one epoch of image paths and labels\\n-    labels_array = np.expand_dims(np.array(label_epoch),1)\\n-    image_paths_array = np.expand_dims(np.array(image_epoch),1)\\n-    control_value = facenet.RANDOM_ROTATE * random_rotate + facenet.RANDOM_CROP * random_crop + facenet.RANDOM_FLIP * random_flip + facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization\\n-    control_array = np.ones_like(labels_array) * control_value\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\n-\\n-    # Training loop\\n-    train_time = 0\\n-    while batch_number < args.epoch_size:\\n-        start_time = time.time()\\n-        feed_dict = {learning_rate_placeholder: lr, phase_train_placeholder:True, batch_size_placeholder:args.batch_size}\\n-        tensor_list = [loss, train_op, step, reg_losses, prelogits, cross_entropy_mean, learning_rate, prelogits_norm, accuracy, prelogits_center_loss]\\n-        if batch_number % 100 == 0:\\n-            loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_, summary_str = sess.run(tensor_list + [summary_op], feed_dict=feed_dict)\\n-            summary_writer.add_summary(summary_str, global_step=step_)\\n-        else:\\n-            loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_ = sess.run(tensor_list, feed_dict=feed_dict)\\n-         \\n-        duration = time.time() - start_time\\n-        stat[\\\'loss\\\'][step_-1] = loss_\\n-        stat[\\\'center_loss\\\'][step_-1] = center_loss_\\n-        stat[\\\'reg_loss\\\'][step_-1] = np.sum(reg_losses_)\\n-        stat[\\\'xent_loss\\\'][step_-1] = cross_entropy_mean_\\n-        stat[\\\'prelogits_norm\\\'][step_-1] = prelogits_norm_\\n-        stat[\\\'learning_rate\\\'][epoch-1] = lr_\\n-        stat[\\\'accuracy\\\'][step_-1] = accuracy_\\n-        stat[\\\'prelogits_hist\\\'][epoch-1,:] += np.histogram(np.minimum(np.abs(prelogits_), prelogits_hist_max), bins=1000, range=(0.0, prelogits_hist_max))[0]\\n-        \\n-        duration = time.time() - start_time\\n-        print(\\\'Epoch: [%d][%d/%d]\\\\tTime %.3f\\\\tLoss %2.3f\\\\tXent %2.3f\\\\tRegLoss %2.3f\\\\tAccuracy %2.3f\\\\tLr %2.5f\\\\tCl %2.3f\\\' %\\n-              (epoch, batch_number+1, args.epoch_size, duration, loss_, cross_entropy_mean_, np.sum(reg_losses_), accuracy_, lr_, center_loss_))\\n-        batch_number += 1\\n-        train_time += duration\\n-    # Add validation loss and accuracy to summary\\n-    summary = tf.Summary()\\n-    #pylint: disable=maybe-no-member\\n-    summary.value.add(tag=\\\'time/total\\\', simple_value=train_time)\\n-    summary_writer.add_summary(summary, global_step=step_)\\n-    return True\\n-\\n-def validate(args, sess, epoch, image_list, label_list, enqueue_op, image_paths_placeholder, labels_placeholder, control_placeholder,\\n-             phase_train_placeholder, batch_size_placeholder, \\n-             stat, loss, regularization_losses, cross_entropy_mean, accuracy, validate_every_n_epochs, use_fixed_image_standardization):\\n-  \\n-    print(\\\'Running forward pass on validation set\\\')\\n-\\n-    nrof_batches = len(label_list) // args.lfw_batch_size\\n-    nrof_images = nrof_batches * args.lfw_batch_size\\n-    \\n-    # Enqueue one epoch of image paths and labels\\n-    labels_array = np.expand_dims(np.array(label_list[:nrof_images]),1)\\n-    image_paths_array = np.expand_dims(np.array(image_list[:nrof_images]),1)\\n-    control_array = np.ones_like(labels_array, np.int32)*facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\n-\\n-    loss_array = np.zeros((nrof_batches,), np.float32)\\n-    xent_array = np.zeros((nrof_batches,), np.float32)\\n-    accuracy_array = np.zeros((nrof_batches,), np.float32)\\n-\\n-    # Training loop\\n-    start_time = time.time()\\n-    for i in range(nrof_batches):\\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:args.lfw_batch_size}\\n-        loss_, cross_entropy_mean_, accuracy_ = sess.run([loss, cross_entropy_mean, accuracy], feed_dict=feed_dict)\\n-        loss_array[i], xent_array[i], accuracy_array[i] = (loss_, cross_entropy_mean_, accuracy_)\\n-        if i % 10 == 9:\\n-            print(\\\'.\\\', end=\\\'\\\')\\n-            sys.stdout.flush()\\n-    print(\\\'\\\')\\n-\\n-    duration = time.time() - start_time\\n-\\n-    val_index = (epoch-1)//validate_every_n_epochs\\n-    stat[\\\'val_loss\\\'][val_index] = np.mean(loss_array)\\n-    stat[\\\'val_xent_loss\\\'][val_index] = np.mean(xent_array)\\n-    stat[\\\'val_accuracy\\\'][val_index] = np.mean(accuracy_array)\\n-\\n-    print(\\\'Validation Epoch: %d\\\\tTime %.3f\\\\tLoss %2.3f\\\\tXent %2.3f\\\\tAccuracy %2.3f\\\' %\\n-          (epoch, duration, np.mean(loss_array), np.mean(xent_array), np.mean(accuracy_array)))\\n-\\n-\\n-def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, \\n-        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, log_dir, step, summary_writer, stat, epoch, distance_metric, subtract_mean, use_flipped_images, use_fixed_image_standardization):\\n-    start_time = time.time()\\n-    # Run forward pass to calculate embeddings\\n-    print(\\\'Runnning forward pass on LFW images\\\')\\n-    \\n-    # Enqueue one epoch of image paths and labels\\n-    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\\n-    nrof_flips = 2 if use_flipped_images else 1\\n-    nrof_images = nrof_embeddings * nrof_flips\\n-    labels_array = np.expand_dims(np.arange(0,nrof_images),1)\\n-    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\\n-    control_array = np.zeros_like(labels_array, np.int32)\\n-    if use_fixed_image_standardization:\\n-        control_array += np.ones_like(labels_array)*facenet.FIXED_STANDARDIZATION\\n-    if use_flipped_images:\\n-        # Flip every second image\\n-        control_array += (labels_array % 2)*facenet.FLIP\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\n-    \\n-    embedding_size = int(embeddings.get_shape()[1])\\n-    assert nrof_images % batch_size == 0, \\\'The number of LFW images must be an integer multiple of the LFW batch size\\\'\\n-    nrof_batches = nrof_images // batch_size\\n-    emb_array = np.zeros((nrof_images, embedding_size))\\n-    lab_array = np.zeros((nrof_images,))\\n-    for i in range(nrof_batches):\\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}\\n-        emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)\\n-        lab_array[lab] = lab\\n-        emb_array[lab, :] = emb\\n-        if i % 10 == 9:\\n-            print(\\\'.\\\', end=\\\'\\\')\\n-            sys.stdout.flush()\\n-    print(\\\'\\\')\\n-    embeddings = np.zeros((nrof_embeddings, embedding_size*nrof_flips))\\n-    if use_flipped_images:\\n-        # Concatenate embeddings for flipped and non flipped version of the images\\n-        embeddings[:,:embedding_size] = emb_array[0::2,:]\\n-        embeddings[:,embedding_size:] = emb_array[1::2,:]\\n-    else:\\n-        embeddings = emb_array\\n-\\n-    assert np.array_equal(lab_array, np.arange(nrof_images))==True, \\\'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline\\\'\\n-    _, _, accuracy, val, val_std, far = lfw.evaluate(embeddings, actual_issame, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\\n-    \\n-    print(\\\'Accuracy: %2.5f+-%2.5f\\\' % (np.mean(accuracy), np.std(accuracy)))\\n-    print(\\\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\\\' % (val, val_std, far))\\n-    lfw_time = time.time() - start_time\\n-    # Add validation loss and accuracy to summary\\n-    summary = tf.Summary()\\n-    #pylint: disable=maybe-no-member\\n-    summary.value.add(tag=\\\'lfw/accuracy\\\', simple_value=np.mean(accuracy))\\n-    summary.value.add(tag=\\\'lfw/val_rate\\\', simple_value=val)\\n-    summary.value.add(tag=\\\'time/lfw\\\', simple_value=lfw_time)\\n-    summary_writer.add_summary(summary, step)\\n-    with open(os.path.join(log_dir,\\\'lfw_result.txt\\\'),\\\'at\\\') as f:\\n-        f.write(\\\'%d\\\\t%.5f\\\\t%.5f\\\\n\\\' % (step, np.mean(accuracy), val))\\n-    stat[\\\'lfw_accuracy\\\'][epoch-1] = np.mean(accuracy)\\n-    stat[\\\'lfw_valrate\\\'][epoch-1] = val\\n-\\n-def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\\n-    # Save the model checkpoint\\n-    print(\\\'Saving variables\\\')\\n-    start_time = time.time()\\n-    checkpoint_path = os.path.join(model_dir, \\\'model-%s.ckpt\\\' % model_name)\\n-    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\\n-    save_time_variables = time.time() - start_time\\n-    print(\\\'Variables saved in %.2f seconds\\\' % save_time_variables)\\n-    metagraph_filename = os.path.join(model_dir, \\\'model-%s.meta\\\' % model_name)\\n-    save_time_metagraph = 0  \\n-    if not os.path.exists(metagraph_filename):\\n-        print(\\\'Saving metagraph\\\')\\n-        start_time = time.time()\\n-        saver.export_meta_graph(metagraph_filename)\\n-        save_time_metagraph = time.time() - start_time\\n-        print(\\\'Metagraph saved in %.2f seconds\\\' % save_time_metagraph)\\n-    summary = tf.Summary()\\n-    #pylint: disable=maybe-no-member\\n-    summary.value.add(tag=\\\'time/save_variables\\\', simple_value=save_time_variables)\\n-    summary.value.add(tag=\\\'time/save_metagraph\\\', simple_value=save_time_metagraph)\\n-    summary_writer.add_summary(summary, step)\\n-  \\n-\\n-def parse_arguments(argv):\\n-    parser = argparse.ArgumentParser()\\n-    \\n-    parser.add_argument(\\\'--logs_base_dir\\\', type=str, \\n-        help=\\\'Directory where to write event logs.\\\', default=\\\'~/logs/facenet\\\')\\n-    parser.add_argument(\\\'--models_base_dir\\\', type=str,\\n-        help=\\\'Directory where to write trained models and checkpoints.\\\', default=\\\'~/models/facenet\\\')\\n-    parser.add_argument(\\\'--gpu_memory_fraction\\\', type=float,\\n-        help=\\\'Upper bound on the amount of GPU memory that will be used by the process.\\\', default=1.0)\\n-    parser.add_argument(\\\'--pretrained_model\\\', type=str,\\n-        help=\\\'Load a pretrained model before training starts.\\\')\\n-    parser.add_argument(\\\'--data_dir\\\', type=str,\\n-        help=\\\'Path to the data directory containing aligned face patches.\\\',\\n-        default=\\\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\\\')\\n-    parser.add_argument(\\\'--model_def\\\', type=str,\\n-        help=\\\'Model definition. Points to a module containing the definition of the inference graph.\\\', default=\\\'models.inception_resnet_v1\\\')\\n-    parser.add_argument(\\\'--max_nrof_epochs\\\', type=int,\\n-        help=\\\'Number of epochs to run.\\\', default=500)\\n-    parser.add_argument(\\\'--batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch.\\\', default=90)\\n-    parser.add_argument(\\\'--image_size\\\', type=int,\\n-        help=\\\'Image size (height, width) in pixels.\\\', default=160)\\n-    parser.add_argument(\\\'--epoch_size\\\', type=int,\\n-        help=\\\'Number of batches per epoch.\\\', default=1000)\\n-    parser.add_argument(\\\'--embedding_size\\\', type=int,\\n-        help=\\\'Dimensionality of the embedding.\\\', default=128)\\n-    parser.add_argument(\\\'--random_crop\\\', \\n-        help=\\\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \\\' +\\n-         \\\'If the size of the images in the data directory is equal to image_size no cropping is performed\\\', action=\\\'store_true\\\')\\n-    parser.add_argument(\\\'--random_flip\\\', \\n-        help=\\\'Performs random horizontal flipping of training images.\\\', action=\\\'store_true\\\')\\n-    parser.add_argument(\\\'--random_rotate\\\', \\n-        help=\\\'Performs random rotations of training images.\\\', action=\\\'store_true\\\')\\n-    parser.add_argument(\\\'--use_fixed_image_standardization\\\', \\n-        help=\\\'Performs fixed standardization of images.\\\', action=\\\'store_true\\\')\\n-    parser.add_argument(\\\'--keep_probability\\\', type=float,\\n-        help=\\\'Keep probability of dropout for the fully connected layer(s).\\\', default=1.0)\\n-    parser.add_argument(\\\'--weight_decay\\\', type=float,\\n-        help=\\\'L2 weight regularization.\\\', default=0.0)\\n-    parser.add_argument(\\\'--center_loss_factor\\\', type=float,\\n-        help=\\\'Center loss factor.\\\', default=0.0)\\n-    parser.add_argument(\\\'--center_loss_alfa\\\', type=float,\\n-        help=\\\'Center update rate for center loss.\\\', default=0.95)\\n-    parser.add_argument(\\\'--prelogits_norm_loss_factor\\\', type=float,\\n-        help=\\\'Loss based on the norm of the activations in the prelogits layer.\\\', default=0.0)\\n-    parser.add_argument(\\\'--prelogits_norm_p\\\', type=float,\\n-        help=\\\'Norm to use for prelogits norm loss.\\\', default=1.0)\\n-    parser.add_argument(\\\'--prelogits_hist_max\\\', type=float,\\n-        help=\\\'The max value for the prelogits histogram.\\\', default=10.0)\\n-    parser.add_argument(\\\'--optimizer\\\', type=str, choices=[\\\'ADAGRAD\\\', \\\'ADADELTA\\\', \\\'ADAM\\\', \\\'RMSPROP\\\', \\\'MOM\\\'],\\n-        help=\\\'The optimization algorithm to use\\\', default=\\\'ADAGRAD\\\')\\n-    parser.add_argument(\\\'--learning_rate\\\', type=float,\\n-        help=\\\'Initial learning rate. If set to a negative value a learning rate \\\' +\\n-        \\\'schedule can be specified in the file "learning_rate_schedule.txt"\\\', default=0.1)\\n-    parser.add_argument(\\\'--learning_rate_decay_epochs\\\', type=int,\\n-        help=\\\'Number of epochs between learning rate decay.\\\', default=100)\\n-    parser.add_argument(\\\'--learning_rate_decay_factor\\\', type=float,\\n-        help=\\\'Learning rate decay factor.\\\', default=1.0)\\n-    parser.add_argument(\\\'--moving_average_decay\\\', type=float,\\n-        help=\\\'Exponential decay for tracking of training parameters.\\\', default=0.9999)\\n-    parser.add_argument(\\\'--seed\\\', type=int,\\n-        help=\\\'Random seed.\\\', default=666)\\n-    parser.add_argument(\\\'--nrof_preprocess_threads\\\', type=int,\\n-        help=\\\'Number of preprocessing (data loading and augmentation) threads.\\\', default=4)\\n-    parser.add_argument(\\\'--log_histograms\\\', \\n-        help=\\\'Enables logging of weight/bias histograms in tensorboard.\\\', action=\\\'store_true\\\')\\n-    parser.add_argument(\\\'--learning_rate_schedule_file\\\', type=str,\\n-        help=\\\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\\\', default=\\\'data/learning_rate_schedule.txt\\\')\\n-    parser.add_argument(\\\'--filter_filename\\\', type=str,\\n-        help=\\\'File containing image data used for dataset filtering\\\', default=\\\'\\\')\\n-    parser.add_argument(\\\'--filter_percentile\\\', type=float,\\n-        help=\\\'Keep only the percentile images closed to its class center\\\', default=100.0)\\n-    parser.add_argument(\\\'--filter_min_nrof_images_per_class\\\', type=int,\\n-        help=\\\'Keep only the classes with this number of examples or more\\\', default=0)\\n-    parser.add_argument(\\\'--validate_every_n_epochs\\\', type=int,\\n-        help=\\\'Number of epoch between validation\\\', default=5)\\n-    parser.add_argument(\\\'--validation_set_split_ratio\\\', type=float,\\n-        help=\\\'The ratio of the total dataset to use for validation\\\', default=0.0)\\n-    parser.add_argument(\\\'--min_nrof_val_images_per_class\\\', type=float,\\n-        help=\\\'Classes with fewer images will be removed from the validation set\\\', default=0)\\n- \\n-    # Parameters for validation on LFW\\n-    parser.add_argument(\\\'--lfw_pairs\\\', type=str,\\n-        help=\\\'The file containing the pairs to use for validation.\\\', default=\\\'data/pairs.txt\\\')\\n-    parser.add_argument(\\\'--lfw_dir\\\', type=str,\\n-        help=\\\'Path to the data directory containing aligned face patches.\\\', default=\\\'\\\')\\n-    parser.add_argument(\\\'--lfw_batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch in the LFW test set.\\\', default=100)\\n-    parser.add_argument(\\\'--lfw_nrof_folds\\\', type=int,\\n-        help=\\\'Number of folds to use for cross validation. Mainly used for testing.\\\', default=10)\\n-    parser.add_argument(\\\'--lfw_distance_metric\\\', type=int,\\n-        help=\\\'Type of distance metric to use. 0: Euclidian, 1:Cosine similarity distance.\\\', default=0)\\n-    parser.add_argument(\\\'--lfw_use_flipped_images\\\', \\n-        help=\\\'Concatenates embeddings for the image and its horizontally flipped counterpart.\\\', action=\\\'store_true\\\')\\n-    parser.add_argument(\\\'--lfw_subtract_mean\\\', \\n-        help=\\\'Subtract feature mean before calculating distance.\\\', action=\\\'store_true\\\')\\n-    return parser.parse_args(argv)\\n-  \\n-\\n-if __name__ == \\\'__main__\\\':\\n-    main(parse_arguments(sys.argv[1:]))\\ndiff --git a/src/train_tripletloss.py b/src/train_tripletloss.py\\ndeleted file mode 100644\\nindex d6df19a..0000000\\n--- a/src/train_tripletloss.py\\n+++ /dev/null\\n@@ -1,486 +0,0 @@\\n-"""Training a face recognizer with TensorFlow based on the FaceNet paper\\n-FaceNet: A Unified Embedding for Face Recognition and Clustering: http://arxiv.org/abs/1503.03832\\n-"""\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-from datetime import datetime\\n-import os.path\\n-import time\\n-import sys\\n-import tensorflow as tf\\n-import numpy as np\\n-import importlib\\n-import itertools\\n-import argparse\\n-import facenet\\n-import lfw\\n-\\n-from tensorflow.python.ops import data_flow_ops\\n-\\n-from six.moves import xrange  # @UnresolvedImport\\n-\\n-def main(args):\\n-  \\n-    network = importlib.import_module(args.model_def)\\n-\\n-    subdir = datetime.strftime(datetime.now(), \\\'%Y%m%d-%H%M%S\\\')\\n-    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\\n-    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\\\'t exist\\n-        os.makedirs(log_dir)\\n-    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\\n-    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\\\'t exist\\n-        os.makedirs(model_dir)\\n-\\n-    # Write arguments to a text file\\n-    facenet.write_arguments_to_file(args, os.path.join(log_dir, \\\'arguments.txt\\\'))\\n-        \\n-    # Store some git revision info in a text file in the log directory\\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\\n-    facenet.store_revision_info(src_path, log_dir, \\\' \\\'.join(sys.argv))\\n-\\n-    np.random.seed(seed=args.seed)\\n-    train_set = facenet.get_dataset(args.data_dir)\\n-    \\n-    print(\\\'Model directory: %s\\\' % model_dir)\\n-    print(\\\'Log directory: %s\\\' % log_dir)\\n-    if args.pretrained_model:\\n-        print(\\\'Pre-trained model: %s\\\' % os.path.expanduser(args.pretrained_model))\\n-    \\n-    if args.lfw_dir:\\n-        print(\\\'LFW directory: %s\\\' % args.lfw_dir)\\n-        # Read the file containing the pairs used for testing\\n-        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\\n-        # Get the paths for the corresponding images\\n-        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\\n-        \\n-    \\n-    with tf.Graph().as_default():\\n-        tf.set_random_seed(args.seed)\\n-        global_step = tf.Variable(0, trainable=False)\\n-\\n-        # Placeholder for the learning rate\\n-        learning_rate_placeholder = tf.placeholder(tf.float32, name=\\\'learning_rate\\\')\\n-        \\n-        batch_size_placeholder = tf.placeholder(tf.int32, name=\\\'batch_size\\\')\\n-        \\n-        phase_train_placeholder = tf.placeholder(tf.bool, name=\\\'phase_train\\\')\\n-        \\n-        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,3), name=\\\'image_paths\\\')\\n-        labels_placeholder = tf.placeholder(tf.int64, shape=(None,3), name=\\\'labels\\\')\\n-        \\n-        input_queue = data_flow_ops.FIFOQueue(capacity=100000,\\n-                                    dtypes=[tf.string, tf.int64],\\n-                                    shapes=[(3,), (3,)],\\n-                                    shared_name=None, name=None)\\n-        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder])\\n-        \\n-        nrof_preprocess_threads = 4\\n-        images_and_labels = []\\n-        for _ in range(nrof_preprocess_threads):\\n-            filenames, label = input_queue.dequeue()\\n-            images = []\\n-            for filename in tf.unstack(filenames):\\n-                file_contents = tf.read_file(filename)\\n-                image = tf.image.decode_image(file_contents, channels=3)\\n-                \\n-                if args.random_crop:\\n-                    image = tf.random_crop(image, [args.image_size, args.image_size, 3])\\n-                else:\\n-                    image = tf.image.resize_image_with_crop_or_pad(image, args.image_size, args.image_size)\\n-                if args.random_flip:\\n-                    image = tf.image.random_flip_left_right(image)\\n-    \\n-                #pylint: disable=no-member\\n-                image.set_shape((args.image_size, args.image_size, 3))\\n-                images.append(tf.image.per_image_standardization(image))\\n-            images_and_labels.append([images, label])\\n-    \\n-        image_batch, labels_batch = tf.train.batch_join(\\n-            images_and_labels, batch_size=batch_size_placeholder, \\n-            shapes=[(args.image_size, args.image_size, 3), ()], enqueue_many=True,\\n-            capacity=4 * nrof_preprocess_threads * args.batch_size,\\n-            allow_smaller_final_batch=True)\\n-        image_batch = tf.identity(image_batch, \\\'image_batch\\\')\\n-        image_batch = tf.identity(image_batch, \\\'input\\\')\\n-        labels_batch = tf.identity(labels_batch, \\\'label_batch\\\')\\n-\\n-        # Build the inference graph\\n-        prelogits, _ = network.inference(image_batch, args.keep_probability, \\n-            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size,\\n-            weight_decay=args.weight_decay)\\n-        \\n-        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\\\'embeddings\\\')\\n-        # Split embeddings into anchor, positive and negative and calculate triplet loss\\n-        anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\\n-        triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\\n-        \\n-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\\n-            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\\n-        tf.summary.scalar(\\\'learning_rate\\\', learning_rate)\\n-\\n-        # Calculate the total losses\\n-        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\\n-        total_loss = tf.add_n([triplet_loss] + regularization_losses, name=\\\'total_loss\\\')\\n-\\n-        # Build a Graph that trains the model with one batch of examples and updates the model parameters\\n-        train_op = facenet.train(total_loss, global_step, args.optimizer, \\n-            learning_rate, args.moving_average_decay, tf.global_variables())\\n-        \\n-        # Create a saver\\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\\n-\\n-        # Build the summary operation based on the TF collection of Summaries.\\n-        summary_op = tf.summary.merge_all()\\n-\\n-        # Start running operations on the Graph.\\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))        \\n-\\n-        # Initialize variables\\n-        sess.run(tf.global_variables_initializer(), feed_dict={phase_train_placeholder:True})\\n-        sess.run(tf.local_variables_initializer(), feed_dict={phase_train_placeholder:True})\\n-\\n-        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\\n-        coord = tf.train.Coordinator()\\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\\n-\\n-        with sess.as_default():\\n-\\n-            if args.pretrained_model:\\n-                print(\\\'Restoring pretrained model: %s\\\' % args.pretrained_model)\\n-                saver.restore(sess, os.path.expanduser(args.pretrained_model))\\n-\\n-            # Training and validation loop\\n-            epoch = 0\\n-            while epoch < args.max_nrof_epochs:\\n-                step = sess.run(global_step, feed_dict=None)\\n-                epoch = step // args.epoch_size\\n-                # Train for one epoch\\n-                train(args, sess, train_set, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\\n-                    batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \\n-                    embeddings, total_loss, train_op, summary_op, summary_writer, args.learning_rate_schedule_file,\\n-                    args.embedding_size, anchor, positive, negative, triplet_loss)\\n-\\n-                # Save variables and the metagraph if it doesn\\\'t exist already\\n-                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, step)\\n-\\n-                # Evaluate on LFW\\n-                if args.lfw_dir:\\n-                    evaluate(sess, lfw_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \\n-                            batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, args.batch_size, \\n-                            args.lfw_nrof_folds, log_dir, step, summary_writer, args.embedding_size)\\n-\\n-    return model_dir\\n-\\n-\\n-def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\\n-          batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \\n-          embeddings, loss, train_op, summary_op, summary_writer, learning_rate_schedule_file,\\n-          embedding_size, anchor, positive, negative, triplet_loss):\\n-    batch_number = 0\\n-    \\n-    if args.learning_rate>0.0:\\n-        lr = args.learning_rate\\n-    else:\\n-        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\\n-    while batch_number < args.epoch_size:\\n-        # Sample people randomly from the dataset\\n-        image_paths, num_per_class = sample_people(dataset, args.people_per_batch, args.images_per_person)\\n-        \\n-        print(\\\'Running forward pass on sampled images: \\\', end=\\\'\\\')\\n-        start_time = time.time()\\n-        nrof_examples = args.people_per_batch * args.images_per_person\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\n-        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n-        sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n-        emb_array = np.zeros((nrof_examples, embedding_size))\\n-        nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\n-        for i in range(nrof_batches):\\n-            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\\n-            emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size, \\n-                learning_rate_placeholder: lr, phase_train_placeholder: True})\\n-            emb_array[lab,:] = emb\\n-        print(\\\'%.3f\\\' % (time.time()-start_time))\\n-\\n-        # Select triplets based on the embeddings\\n-        print(\\\'Selecting suitable triplets for training\\\')\\n-        triplets, nrof_random_negs, nrof_triplets = select_triplets(emb_array, num_per_class, \\n-            image_paths, args.people_per_batch, args.alpha)\\n-        selection_time = time.time() - start_time\\n-        print(\\\'(nrof_random_negs, nrof_triplets) = (%d, %d): time=%.3f seconds\\\' % \\n-            (nrof_random_negs, nrof_triplets, selection_time))\\n-\\n-        # Perform training on the selected triplets\\n-        nrof_batches = int(np.ceil(nrof_triplets*3/args.batch_size))\\n-        triplet_paths = list(itertools.chain(*triplets))\\n-        labels_array = np.reshape(np.arange(len(triplet_paths)),(-1,3))\\n-        triplet_paths_array = np.reshape(np.expand_dims(np.array(triplet_paths),1), (-1,3))\\n-        sess.run(enqueue_op, {image_paths_placeholder: triplet_paths_array, labels_placeholder: labels_array})\\n-        nrof_examples = len(triplet_paths)\\n-        train_time = 0\\n-        i = 0\\n-        emb_array = np.zeros((nrof_examples, embedding_size))\\n-        loss_array = np.zeros((nrof_triplets,))\\n-        summary = tf.Summary()\\n-        step = 0\\n-        while i < nrof_batches:\\n-            start_time = time.time()\\n-            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\\n-            feed_dict = {batch_size_placeholder: batch_size, learning_rate_placeholder: lr, phase_train_placeholder: True}\\n-            err, _, step, emb, lab = sess.run([loss, train_op, global_step, embeddings, labels_batch], feed_dict=feed_dict)\\n-            emb_array[lab,:] = emb\\n-            loss_array[i] = err\\n-            duration = time.time() - start_time\\n-            print(\\\'Epoch: [%d][%d/%d]\\\\tTime %.3f\\\\tLoss %2.3f\\\' %\\n-                  (epoch, batch_number+1, args.epoch_size, duration, err))\\n-            batch_number += 1\\n-            i += 1\\n-            train_time += duration\\n-            summary.value.add(tag=\\\'loss\\\', simple_value=err)\\n-            \\n-        # Add validation loss and accuracy to summary\\n-        #pylint: disable=maybe-no-member\\n-        summary.value.add(tag=\\\'time/selection\\\', simple_value=selection_time)\\n-        summary_writer.add_summary(summary, step)\\n-    return step\\n-  \\n-def select_triplets(embeddings, nrof_images_per_class, image_paths, people_per_batch, alpha):\\n-    """ Select the triplets for training\\n-    """\\n-    trip_idx = 0\\n-    emb_start_idx = 0\\n-    num_trips = 0\\n-    triplets = []\\n-    \\n-    # VGG Face: Choosing good triplets is crucial and should strike a balance between\\n-    #  selecting informative (i.e. challenging) examples and swamping training with examples that\\n-    #  are too hard. This is achieve by extending each pair (a, p) to a triplet (a, p, n) by sampling\\n-    #  the image n at random, but only between the ones that violate the triplet loss margin. The\\n-    #  latter is a form of hard-negative mining, but it is not as aggressive (and much cheaper) than\\n-    #  choosing the maximally violating example, as often done in structured output learning.\\n-\\n-    for i in xrange(people_per_batch):\\n-        nrof_images = int(nrof_images_per_class[i])\\n-        for j in xrange(1,nrof_images):\\n-            a_idx = emb_start_idx + j - 1\\n-            neg_dists_sqr = np.sum(np.square(embeddings[a_idx] - embeddings), 1)\\n-            for pair in xrange(j, nrof_images): # For every possible positive pair.\\n-                p_idx = emb_start_idx + pair\\n-                pos_dist_sqr = np.sum(np.square(embeddings[a_idx]-embeddings[p_idx]))\\n-                neg_dists_sqr[emb_start_idx:emb_start_idx+nrof_images] = np.NaN\\n-                #all_neg = np.where(np.logical_and(neg_dists_sqr-pos_dist_sqr<alpha, pos_dist_sqr<neg_dists_sqr))[0]  # FaceNet selection\\n-                all_neg = np.where(neg_dists_sqr-pos_dist_sqr<alpha)[0] # VGG Face selecction\\n-                nrof_random_negs = all_neg.shape[0]\\n-                if nrof_random_negs>0:\\n-                    rnd_idx = np.random.randint(nrof_random_negs)\\n-                    n_idx = all_neg[rnd_idx]\\n-                    triplets.append((image_paths[a_idx], image_paths[p_idx], image_paths[n_idx]))\\n-                    #print(\\\'Triplet %d: (%d, %d, %d), pos_dist=%2.6f, neg_dist=%2.6f (%d, %d, %d, %d, %d)\\\' % \\n-                    #    (trip_idx, a_idx, p_idx, n_idx, pos_dist_sqr, neg_dists_sqr[n_idx], nrof_random_negs, rnd_idx, i, j, emb_start_idx))\\n-                    trip_idx += 1\\n-\\n-                num_trips += 1\\n-\\n-        emb_start_idx += nrof_images\\n-\\n-    np.random.shuffle(triplets)\\n-    return triplets, num_trips, len(triplets)\\n-\\n-def sample_people(dataset, people_per_batch, images_per_person):\\n-    nrof_images = people_per_batch * images_per_person\\n-  \\n-    # Sample classes from the dataset\\n-    nrof_classes = len(dataset)\\n-    class_indices = np.arange(nrof_classes)\\n-    np.random.shuffle(class_indices)\\n-    \\n-    i = 0\\n-    image_paths = []\\n-    num_per_class = []\\n-    sampled_class_indices = []\\n-    # Sample images from these classes until we have enough\\n-    while len(image_paths)<nrof_images:\\n-        class_index = class_indices[i]\\n-        nrof_images_in_class = len(dataset[class_index])\\n-        image_indices = np.arange(nrof_images_in_class)\\n-        np.random.shuffle(image_indices)\\n-        nrof_images_from_class = min(nrof_images_in_class, images_per_person, nrof_images-len(image_paths))\\n-        idx = image_indices[0:nrof_images_from_class]\\n-        image_paths_for_class = [dataset[class_index].image_paths[j] for j in idx]\\n-        sampled_class_indices += [class_index]*nrof_images_from_class\\n-        image_paths += image_paths_for_class\\n-        num_per_class.append(nrof_images_from_class)\\n-        i+=1\\n-  \\n-    return image_paths, num_per_class\\n-\\n-def evaluate(sess, image_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \\n-        batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, batch_size, \\n-        nrof_folds, log_dir, step, summary_writer, embedding_size):\\n-    start_time = time.time()\\n-    # Run forward pass to calculate embeddings\\n-    print(\\\'Running forward pass on LFW images: \\\', end=\\\'\\\')\\n-    \\n-    nrof_images = len(actual_issame)*2\\n-    assert(len(image_paths)==nrof_images)\\n-    labels_array = np.reshape(np.arange(nrof_images),(-1,3))\\n-    image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\n-    emb_array = np.zeros((nrof_images, embedding_size))\\n-    nrof_batches = int(np.ceil(nrof_images / batch_size))\\n-    label_check_array = np.zeros((nrof_images,))\\n-    for i in xrange(nrof_batches):\\n-        batch_size = min(nrof_images-i*batch_size, batch_size)\\n-        emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size,\\n-            learning_rate_placeholder: 0.0, phase_train_placeholder: False})\\n-        emb_array[lab,:] = emb\\n-        label_check_array[lab] = 1\\n-    print(\\\'%.3f\\\' % (time.time()-start_time))\\n-    \\n-    assert(np.all(label_check_array==1))\\n-    \\n-    _, _, accuracy, val, val_std, far = lfw.evaluate(emb_array, actual_issame, nrof_folds=nrof_folds)\\n-    \\n-    print(\\\'Accuracy: %1.3f+-%1.3f\\\' % (np.mean(accuracy), np.std(accuracy)))\\n-    print(\\\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\\\' % (val, val_std, far))\\n-    lfw_time = time.time() - start_time\\n-    # Add validation loss and accuracy to summary\\n-    summary = tf.Summary()\\n-    #pylint: disable=maybe-no-member\\n-    summary.value.add(tag=\\\'lfw/accuracy\\\', simple_value=np.mean(accuracy))\\n-    summary.value.add(tag=\\\'lfw/val_rate\\\', simple_value=val)\\n-    summary.value.add(tag=\\\'time/lfw\\\', simple_value=lfw_time)\\n-    summary_writer.add_summary(summary, step)\\n-    with open(os.path.join(log_dir,\\\'lfw_result.txt\\\'),\\\'at\\\') as f:\\n-        f.write(\\\'%d\\\\t%.5f\\\\t%.5f\\\\n\\\' % (step, np.mean(accuracy), val))\\n-\\n-def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\\n-    # Save the model checkpoint\\n-    print(\\\'Saving variables\\\')\\n-    start_time = time.time()\\n-    checkpoint_path = os.path.join(model_dir, \\\'model-%s.ckpt\\\' % model_name)\\n-    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\\n-    save_time_variables = time.time() - start_time\\n-    print(\\\'Variables saved in %.2f seconds\\\' % save_time_variables)\\n-    metagraph_filename = os.path.join(model_dir, \\\'model-%s.meta\\\' % model_name)\\n-    save_time_metagraph = 0  \\n-    if not os.path.exists(metagraph_filename):\\n-        print(\\\'Saving metagraph\\\')\\n-        start_time = time.time()\\n-        saver.export_meta_graph(metagraph_filename)\\n-        save_time_metagraph = time.time() - start_time\\n-        print(\\\'Metagraph saved in %.2f seconds\\\' % save_time_metagraph)\\n-    summary = tf.Summary()\\n-    #pylint: disable=maybe-no-member\\n-    summary.value.add(tag=\\\'time/save_variables\\\', simple_value=save_time_variables)\\n-    summary.value.add(tag=\\\'time/save_metagraph\\\', simple_value=save_time_metagraph)\\n-    summary_writer.add_summary(summary, step)\\n-  \\n-  \\n-def get_learning_rate_from_file(filename, epoch):\\n-    with open(filename, \\\'r\\\') as f:\\n-        for line in f.readlines():\\n-            line = line.split(\\\'#\\\', 1)[0]\\n-            if line:\\n-                par = line.strip().split(\\\':\\\')\\n-                e = int(par[0])\\n-                lr = float(par[1])\\n-                if e <= epoch:\\n-                    learning_rate = lr\\n-                else:\\n-                    return learning_rate\\n-    \\n-\\n-def parse_arguments(argv):\\n-    parser = argparse.ArgumentParser()\\n-    \\n-    parser.add_argument(\\\'--logs_base_dir\\\', type=str, \\n-        help=\\\'Directory where to write event logs.\\\', default=\\\'~/logs/facenet\\\')\\n-    parser.add_argument(\\\'--models_base_dir\\\', type=str,\\n-        help=\\\'Directory where to write trained models and checkpoints.\\\', default=\\\'~/models/facenet\\\')\\n-    parser.add_argument(\\\'--gpu_memory_fraction\\\', type=float,\\n-        help=\\\'Upper bound on the amount of GPU memory that will be used by the process.\\\', default=1.0)\\n-    parser.add_argument(\\\'--pretrained_model\\\', type=str,\\n-        help=\\\'Load a pretrained model before training starts.\\\')\\n-    parser.add_argument(\\\'--data_dir\\\', type=str,\\n-        help=\\\'Path to the data directory containing aligned face patches.\\\',\\n-        default=\\\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\\\')\\n-    parser.add_argument(\\\'--model_def\\\', type=str,\\n-        help=\\\'Model definition. Points to a module containing the definition of the inference graph.\\\', default=\\\'models.inception_resnet_v1\\\')\\n-    parser.add_argument(\\\'--max_nrof_epochs\\\', type=int,\\n-        help=\\\'Number of epochs to run.\\\', default=500)\\n-    parser.add_argument(\\\'--batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch.\\\', default=90)\\n-    parser.add_argument(\\\'--image_size\\\', type=int,\\n-        help=\\\'Image size (height, width) in pixels.\\\', default=160)\\n-    parser.add_argument(\\\'--people_per_batch\\\', type=int,\\n-        help=\\\'Number of people per batch.\\\', default=45)\\n-    parser.add_argument(\\\'--images_per_person\\\', type=int,\\n-        help=\\\'Number of images per person.\\\', default=40)\\n-    parser.add_argument(\\\'--epoch_size\\\', type=int,\\n-        help=\\\'Number of batches per epoch.\\\', default=1000)\\n-    parser.add_argument(\\\'--alpha\\\', type=float,\\n-        help=\\\'Positive to negative triplet distance margin.\\\', default=0.2)\\n-    parser.add_argument(\\\'--embedding_size\\\', type=int,\\n-        help=\\\'Dimensionality of the embedding.\\\', default=128)\\n-    parser.add_argument(\\\'--random_crop\\\', \\n-        help=\\\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \\\' +\\n-         \\\'If the size of the images in the data directory is equal to image_size no cropping is performed\\\', action=\\\'store_true\\\')\\n-    parser.add_argument(\\\'--random_flip\\\', \\n-        help=\\\'Performs random horizontal flipping of training images.\\\', action=\\\'store_true\\\')\\n-    parser.add_argument(\\\'--keep_probability\\\', type=float,\\n-        help=\\\'Keep probability of dropout for the fully connected layer(s).\\\', default=1.0)\\n-    parser.add_argument(\\\'--weight_decay\\\', type=float,\\n-        help=\\\'L2 weight regularization.\\\', default=0.0)\\n-    parser.add_argument(\\\'--optimizer\\\', type=str, choices=[\\\'ADAGRAD\\\', \\\'ADADELTA\\\', \\\'ADAM\\\', \\\'RMSPROP\\\', \\\'MOM\\\'],\\n-        help=\\\'The optimization algorithm to use\\\', default=\\\'ADAGRAD\\\')\\n-    parser.add_argument(\\\'--learning_rate\\\', type=float,\\n-        help=\\\'Initial learning rate. If set to a negative value a learning rate \\\' +\\n-        \\\'schedule can be specified in the file "learning_rate_schedule.txt"\\\', default=0.1)\\n-    parser.add_argument(\\\'--learning_rate_decay_epochs\\\', type=int,\\n-        help=\\\'Number of epochs between learning rate decay.\\\', default=100)\\n-    parser.add_argument(\\\'--learning_rate_decay_factor\\\', type=float,\\n-        help=\\\'Learning rate decay factor.\\\', default=1.0)\\n-    parser.add_argument(\\\'--moving_average_decay\\\', type=float,\\n-        help=\\\'Exponential decay for tracking of training parameters.\\\', default=0.9999)\\n-    parser.add_argument(\\\'--seed\\\', type=int,\\n-        help=\\\'Random seed.\\\', default=666)\\n-    parser.add_argument(\\\'--learning_rate_schedule_file\\\', type=str,\\n-        help=\\\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\\\', default=\\\'data/learning_rate_schedule.txt\\\')\\n-\\n-    # Parameters for validation on LFW\\n-    parser.add_argument(\\\'--lfw_pairs\\\', type=str,\\n-        help=\\\'The file containing the pairs to use for validation.\\\', default=\\\'data/pairs.txt\\\')\\n-    parser.add_argument(\\\'--lfw_dir\\\', type=str,\\n-        help=\\\'Path to the data directory containing aligned face patches.\\\', default=\\\'\\\')\\n-    parser.add_argument(\\\'--lfw_nrof_folds\\\', type=int,\\n-        help=\\\'Number of folds to use for cross validation. Mainly used for testing.\\\', default=10)\\n-    return parser.parse_args(argv)\\n-  \\n-\\n-if __name__ == \\\'__main__\\\':\\n-    main(parse_arguments(sys.argv[1:]))\\ndiff --git a/src/validate_on_lfw.py b/src/validate_on_lfw.py\\ndeleted file mode 100644\\nindex ac456c5..0000000\\n--- a/src/validate_on_lfw.py\\n+++ /dev/null\\n@@ -1,164 +0,0 @@\\n-"""Validate a face recognizer on the "Labeled Faces in the Wild" dataset (http://vis-www.cs.umass.edu/lfw/).\\n-Embeddings are calculated using the pairs from http://vis-www.cs.umass.edu/lfw/pairs.txt and the ROC curve\\n-is calculated and plotted. Both the model metagraph and the model parameters need to exist\\n-in the same directory, and the metagraph should have the extension \\\'.meta\\\'.\\n-"""\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-import tensorflow as tf\\n-import numpy as np\\n-import argparse\\n-import facenet\\n-import lfw\\n-import os\\n-import sys\\n-from tensorflow.python.ops import data_flow_ops\\n-from sklearn import metrics\\n-from scipy.optimize import brentq\\n-from scipy import interpolate\\n-\\n-def main(args):\\n-  \\n-    with tf.Graph().as_default():\\n-      \\n-        with tf.Session() as sess:\\n-            \\n-            # Read the file containing the pairs used for testing\\n-            pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\\n-\\n-            # Get the paths for the corresponding images\\n-            paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\\n-            \\n-            image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\\\'image_paths\\\')\\n-            labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\\\'labels\\\')\\n-            batch_size_placeholder = tf.placeholder(tf.int32, name=\\\'batch_size\\\')\\n-            control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\\\'control\\\')\\n-            phase_train_placeholder = tf.placeholder(tf.bool, name=\\\'phase_train\\\')\\n- \\n-            nrof_preprocess_threads = 4\\n-            image_size = (args.image_size, args.image_size)\\n-            eval_input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\\n-                                        dtypes=[tf.string, tf.int32, tf.int32],\\n-                                        shapes=[(1,), (1,), (1,)],\\n-                                        shared_name=None, name=None)\\n-            eval_enqueue_op = eval_input_queue.enqueue_many([image_paths_placeholder, labels_placeholder, control_placeholder], name=\\\'eval_enqueue_op\\\')\\n-            image_batch, label_batch = facenet.create_input_pipeline(eval_input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder)\\n-     \\n-            # Load the model\\n-            input_map = {\\\'image_batch\\\': image_batch, \\\'label_batch\\\': label_batch, \\\'phase_train\\\': phase_train_placeholder}\\n-            facenet.load_model(args.model, input_map=input_map)\\n-\\n-            # Get output tensor\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\n-#              \\n-            coord = tf.train.Coordinator()\\n-            tf.train.start_queue_runners(coord=coord, sess=sess)\\n-\\n-            evaluate(sess, eval_enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\\n-                embeddings, label_batch, paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, args.distance_metric, args.subtract_mean,\\n-                args.use_flipped_images, args.use_fixed_image_standardization)\\n-\\n-              \\n-def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\\n-        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, distance_metric, subtract_mean, use_flipped_images, use_fixed_image_standardization):\\n-    # Run forward pass to calculate embeddings\\n-    print(\\\'Runnning forward pass on LFW images\\\')\\n-    \\n-    # Enqueue one epoch of image paths and labels\\n-    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\\n-    nrof_flips = 2 if use_flipped_images else 1\\n-    nrof_images = nrof_embeddings * nrof_flips\\n-    labels_array = np.expand_dims(np.arange(0,nrof_images),1)\\n-    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\\n-    control_array = np.zeros_like(labels_array, np.int32)\\n-    if use_fixed_image_standardization:\\n-        control_array += np.ones_like(labels_array)*facenet.FIXED_STANDARDIZATION\\n-    if use_flipped_images:\\n-        # Flip every second image\\n-        control_array += (labels_array % 2)*facenet.FLIP\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\n-    \\n-    embedding_size = int(embeddings.get_shape()[1])\\n-    assert nrof_images % batch_size == 0, \\\'The number of LFW images must be an integer multiple of the LFW batch size\\\'\\n-    nrof_batches = nrof_images // batch_size\\n-    emb_array = np.zeros((nrof_images, embedding_size))\\n-    lab_array = np.zeros((nrof_images,))\\n-    for i in range(nrof_batches):\\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}\\n-        emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)\\n-        lab_array[lab] = lab\\n-        emb_array[lab, :] = emb\\n-        if i % 10 == 9:\\n-            print(\\\'.\\\', end=\\\'\\\')\\n-            sys.stdout.flush()\\n-    print(\\\'\\\')\\n-    embeddings = np.zeros((nrof_embeddings, embedding_size*nrof_flips))\\n-    if use_flipped_images:\\n-        # Concatenate embeddings for flipped and non flipped version of the images\\n-        embeddings[:,:embedding_size] = emb_array[0::2,:]\\n-        embeddings[:,embedding_size:] = emb_array[1::2,:]\\n-    else:\\n-        embeddings = emb_array\\n-\\n-    assert np.array_equal(lab_array, np.arange(nrof_images))==True, \\\'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline\\\'\\n-    tpr, fpr, accuracy, val, val_std, far = lfw.evaluate(embeddings, actual_issame, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\\n-    \\n-    print(\\\'Accuracy: %2.5f+-%2.5f\\\' % (np.mean(accuracy), np.std(accuracy)))\\n-    print(\\\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\\\' % (val, val_std, far))\\n-    \\n-    auc = metrics.auc(fpr, tpr)\\n-    print(\\\'Area Under Curve (AUC): %1.3f\\\' % auc)\\n-    eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\\n-    print(\\\'Equal Error Rate (EER): %1.3f\\\' % eer)\\n-    \\n-def parse_arguments(argv):\\n-    parser = argparse.ArgumentParser()\\n-    \\n-    parser.add_argument(\\\'lfw_dir\\\', type=str,\\n-        help=\\\'Path to the data directory containing aligned LFW face patches.\\\')\\n-    parser.add_argument(\\\'--lfw_batch_size\\\', type=int,\\n-        help=\\\'Number of images to process in a batch in the LFW test set.\\\', default=100)\\n-    parser.add_argument(\\\'model\\\', type=str, \\n-        help=\\\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\\\')\\n-    parser.add_argument(\\\'--image_size\\\', type=int,\\n-        help=\\\'Image size (height, width) in pixels.\\\', default=160)\\n-    parser.add_argument(\\\'--lfw_pairs\\\', type=str,\\n-        help=\\\'The file containing the pairs to use for validation.\\\', default=\\\'data/pairs.txt\\\')\\n-    parser.add_argument(\\\'--lfw_nrof_folds\\\', type=int,\\n-        help=\\\'Number of folds to use for cross validation. Mainly used for testing.\\\', default=10)\\n-    parser.add_argument(\\\'--distance_metric\\\', type=int,\\n-        help=\\\'Distance metric  0:euclidian, 1:cosine similarity.\\\', default=0)\\n-    parser.add_argument(\\\'--use_flipped_images\\\', \\n-        help=\\\'Concatenates embeddings for the image and its horizontally flipped counterpart.\\\', action=\\\'store_true\\\')\\n-    parser.add_argument(\\\'--subtract_mean\\\', \\n-        help=\\\'Subtract feature mean before calculating distance.\\\', action=\\\'store_true\\\')\\n-    parser.add_argument(\\\'--use_fixed_image_standardization\\\', \\n-        help=\\\'Performs fixed standardization of images.\\\', action=\\\'store_true\\\')\\n-    return parser.parse_args(argv)\\n-\\n-if __name__ == \\\'__main__\\\':\\n-    main(parse_arguments(sys.argv[1:]))\\ndiff --git a/test/a b/test/a\\ndeleted file mode 100644\\nindex 8b13789..0000000\\n--- a/test/a\\n+++ /dev/null\\n@@ -1 +0,0 @@\\n-\\ndiff --git a/test/batch_norm_test.py b/test/batch_norm_test.py\\ndeleted file mode 100644\\nindex 48cfd55..0000000\\n--- a/test/batch_norm_test.py\\n+++ /dev/null\\n@@ -1,66 +0,0 @@\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-import unittest\\n-import tensorflow as tf\\n-import models\\n-import numpy as np\\n-import numpy.testing as testing\\n-\\n-class BatchNormTest(unittest.TestCase):\\n-\\n-\\n-    @unittest.skip("Skip batch norm test case")\\n-    def testBatchNorm(self):\\n-      \\n-        tf.set_random_seed(123)\\n-  \\n-        x = tf.placeholder(tf.float32, [None, 20, 20, 10], name=\\\'input\\\')\\n-        phase_train = tf.placeholder(tf.bool, name=\\\'phase_train\\\')\\n-        \\n-        # generate random noise to pass into batch norm\\n-        #x_gen = tf.random_normal([50,20,20,10])\\n-        \\n-        bn = models.network.batch_norm(x, phase_train)\\n-        \\n-        init = tf.global_variables_initializer()\\n-        sess = tf.Session(config=tf.ConfigProto())\\n-        sess.run(init)\\n-  \\n-        with sess.as_default():\\n-        \\n-            #generate a constant variable to pass into batch norm\\n-            y = np.random.normal(0, 1, size=(50,20,20,10))\\n-            \\n-            feed_dict = {x: y, phase_train: True}\\n-            sess.run(bn, feed_dict=feed_dict)\\n-            \\n-            feed_dict = {x: y, phase_train: False}\\n-            y1 = sess.run(bn, feed_dict=feed_dict)\\n-            y2 = sess.run(bn, feed_dict=feed_dict)\\n-            \\n-            testing.assert_almost_equal(y1, y2, 10, \\\'Output from two forward passes with phase_train==false should be equal\\\')\\n-\\n-\\n-if __name__ == "__main__":\\n-    unittest.main()\\n-    \\n\\\\ No newline at end of file\\ndiff --git a/test/center_loss_test.py b/test/center_loss_test.py\\ndeleted file mode 100644\\nindex 196cd11..0000000\\n--- a/test/center_loss_test.py\\n+++ /dev/null\\n@@ -1,87 +0,0 @@\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-import unittest\\n-import tensorflow as tf\\n-import numpy as np\\n-import facenet\\n-\\n-class CenterLossTest(unittest.TestCase):\\n-  \\n-\\n-\\n-    def testCenterLoss(self):\\n-        batch_size = 16\\n-        nrof_features = 2\\n-        nrof_classes = 16\\n-        alfa = 0.5\\n-        \\n-        with tf.Graph().as_default():\\n-        \\n-            features = tf.placeholder(tf.float32, shape=(batch_size, nrof_features), name=\\\'features\\\')\\n-            labels = tf.placeholder(tf.int32, shape=(batch_size,), name=\\\'labels\\\')\\n-\\n-            # Define center loss\\n-            center_loss, centers = facenet.center_loss(features, labels, alfa, nrof_classes)\\n-            \\n-            label_to_center = np.array( [ \\n-                 [-3,-3],  [-3,-1],  [-3,1],  [-3,3],\\n-                 [-1,-3],  [-1,-1],  [-1,1],  [-1,3],\\n-                 [ 1,-3],  [ 1,-1],  [ 1,1],  [ 1,3],\\n-                 [ 3,-3],  [ 3,-1],  [ 3,1],  [ 3,3] \\n-                 ])\\n-                \\n-            sess = tf.Session()\\n-            with sess.as_default():\\n-                sess.run(tf.global_variables_initializer())\\n-                np.random.seed(seed=666)\\n-                \\n-                for _ in range(0,100):\\n-                    # Create array of random labels\\n-                    lbls = np.random.randint(low=0, high=nrof_classes, size=(batch_size,))\\n-                    feats = create_features(label_to_center, batch_size, nrof_features, lbls)\\n-\\n-                    center_loss_, centers_ = sess.run([center_loss, centers], feed_dict={features:feats, labels:lbls})\\n-                    \\n-                # After a large number of updates the estimated centers should be close to the true ones\\n-                np.testing.assert_almost_equal(centers_, label_to_center, decimal=5, err_msg=\\\'Incorrect estimated centers\\\')\\n-                np.testing.assert_almost_equal(center_loss_, 0.0, decimal=5, err_msg=\\\'Incorrect center loss\\\')\\n-                \\n-\\n-def create_features(label_to_center, batch_size, nrof_features, labels):\\n-    # Map label to center\\n-#     label_to_center_dict = { \\n-#          0:(-3,-3),  1:(-3,-1),  2:(-3,1),  3:(-3,3),\\n-#          4:(-1,-3),  5:(-1,-1),  6:(-1,1),  7:(-1,3),\\n-#          8:( 1,-3),  9:( 1,-1), 10:( 1,1), 11:( 1,3),\\n-#         12:( 3,-3), 13:( 3,-1), 14:( 3,1), 15:( 3,3),\\n-#         }\\n-    # Create array of features corresponding to the labels\\n-    feats = np.zeros((batch_size, nrof_features))\\n-    for i in range(batch_size):\\n-        cntr =  label_to_center[labels[i]]\\n-        for j in range(nrof_features):\\n-            feats[i,j] = cntr[j]\\n-    return feats\\n-                      \\n-if __name__ == "__main__":\\n-    unittest.main()\\ndiff --git a/test/restore_test.py b/test/restore_test.py\\ndeleted file mode 100644\\nindex befb04d..0000000\\n--- a/test/restore_test.py\\n+++ /dev/null\\n@@ -1,181 +0,0 @@\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-import unittest\\n-import tempfile\\n-import os\\n-import shutil\\n-import tensorflow as tf\\n-import numpy as np\\n-\\n-class TrainTest(unittest.TestCase):\\n-  \\n-    @classmethod\\n-    def setUpClass(self):\\n-        self.tmp_dir = tempfile.mkdtemp()\\n-        \\n-    @classmethod\\n-    def tearDownClass(self):\\n-        # Recursively remove the temporary directory\\n-        shutil.rmtree(self.tmp_dir)\\n-\\n-    def test_restore_noema(self):\\n-        \\n-        # Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\\n-        x_data = np.random.rand(100).astype(np.float32)\\n-        y_data = x_data * 0.1 + 0.3\\n-        \\n-        # Try to find values for W and b that compute y_data = W * x_data + b\\n-        # (We know that W should be 0.1 and b 0.3, but TensorFlow will\\n-        # figure that out for us.)\\n-        W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name=\\\'W\\\')\\n-        b = tf.Variable(tf.zeros([1]), name=\\\'b\\\')\\n-        y = W * x_data + b\\n-        \\n-        # Minimize the mean squared errors.\\n-        loss = tf.reduce_mean(tf.square(y - y_data))\\n-        optimizer = tf.train.GradientDescentOptimizer(0.5)\\n-        train = optimizer.minimize(loss)\\n-        \\n-        # Before starting, initialize the variables.  We will \\\'run\\\' this first.\\n-        init = tf.global_variables_initializer()\\n-\\n-        saver = tf.train.Saver(tf.trainable_variables())\\n-        \\n-        # Launch the graph.\\n-        sess = tf.Session()\\n-        sess.run(init)\\n-        \\n-        # Fit the line.\\n-        for _ in range(201):\\n-            sess.run(train)\\n-        \\n-        w_reference = sess.run(\\\'W:0\\\')\\n-        b_reference = sess.run(\\\'b:0\\\')\\n-        \\n-        saver.save(sess, os.path.join(self.tmp_dir, "model_ex1"))\\n-        \\n-        tf.reset_default_graph()\\n-\\n-        saver = tf.train.import_meta_graph(os.path.join(self.tmp_dir, "model_ex1.meta"))\\n-        sess = tf.Session()\\n-        saver.restore(sess, os.path.join(self.tmp_dir, "model_ex1"))\\n-        \\n-        w_restored = sess.run(\\\'W:0\\\')\\n-        b_restored = sess.run(\\\'b:0\\\')\\n-        \\n-        self.assertAlmostEqual(w_reference, w_restored, \\\'Restored model use different weight than the original model\\\')\\n-        self.assertAlmostEqual(b_reference, b_restored, \\\'Restored model use different weight than the original model\\\')\\n-\\n-\\n-    @unittest.skip("Skip restore EMA test case for now")\\n-    def test_restore_ema(self):\\n-        \\n-        # Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\\n-        x_data = np.random.rand(100).astype(np.float32)\\n-        y_data = x_data * 0.1 + 0.3\\n-        \\n-        # Try to find values for W and b that compute y_data = W * x_data + b\\n-        # (We know that W should be 0.1 and b 0.3, but TensorFlow will\\n-        # figure that out for us.)\\n-        W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name=\\\'W\\\')\\n-        b = tf.Variable(tf.zeros([1]), name=\\\'b\\\')\\n-        y = W * x_data + b\\n-        \\n-        # Minimize the mean squared errors.\\n-        loss = tf.reduce_mean(tf.square(y - y_data))\\n-        optimizer = tf.train.GradientDescentOptimizer(0.5)\\n-        opt_op = optimizer.minimize(loss)\\n-\\n-        # Track the moving averages of all trainable variables.\\n-        ema = tf.train.ExponentialMovingAverage(decay=0.9999)\\n-        averages_op = ema.apply(tf.trainable_variables())\\n-        with tf.control_dependencies([opt_op]):\\n-            train_op = tf.group(averages_op)\\n-  \\n-        # Before starting, initialize the variables.  We will \\\'run\\\' this first.\\n-        init = tf.global_variables_initializer()\\n-\\n-        saver = tf.train.Saver(tf.trainable_variables())\\n-        \\n-        # Launch the graph.\\n-        sess = tf.Session()\\n-        sess.run(init)\\n-        \\n-        # Fit the line.\\n-        for _ in range(201):\\n-            sess.run(train_op)\\n-        \\n-        w_reference = sess.run(\\\'W/ExponentialMovingAverage:0\\\')\\n-        b_reference = sess.run(\\\'b/ExponentialMovingAverage:0\\\')\\n-        \\n-        saver.save(sess, os.path.join(self.tmp_dir, "model_ex1"))\\n-                \\n-        tf.reset_default_graph()\\n-\\n-        tf.train.import_meta_graph(os.path.join(self.tmp_dir, "model_ex1.meta"))\\n-        sess = tf.Session()\\n-        \\n-        print(\\\'------------------------------------------------------\\\')\\n-        for var in tf.global_variables():\\n-            print(\\\'all variables: \\\' + var.op.name)\\n-        for var in tf.trainable_variables():\\n-            print(\\\'normal variable: \\\' + var.op.name)\\n-        for var in tf.moving_average_variables():\\n-            print(\\\'ema variable: \\\' + var.op.name)\\n-        print(\\\'------------------------------------------------------\\\')\\n-\\n-        mode = 1\\n-        restore_vars = {}\\n-        if mode == 0:\\n-            ema = tf.train.ExponentialMovingAverage(1.0)\\n-            for var in tf.trainable_variables():\\n-                print(\\\'%s: %s\\\' % (ema.average_name(var), var.op.name))\\n-                restore_vars[ema.average_name(var)] = var\\n-        elif mode == 1:\\n-            for var in tf.trainable_variables():\\n-                ema_name = var.op.name + \\\'/ExponentialMovingAverage\\\'\\n-                print(\\\'%s: %s\\\' % (ema_name, var.op.name))\\n-                restore_vars[ema_name] = var\\n-            \\n-        saver = tf.train.Saver(restore_vars, name=\\\'ema_restore\\\')\\n-        \\n-        saver.restore(sess, os.path.join(self.tmp_dir, "model_ex1"))\\n-        \\n-        w_restored = sess.run(\\\'W:0\\\')\\n-        b_restored = sess.run(\\\'b:0\\\')\\n-        \\n-        self.assertAlmostEqual(w_reference, w_restored, \\\'Restored model modes not use the EMA filtered weight\\\')\\n-        self.assertAlmostEqual(b_reference, b_restored, \\\'Restored model modes not use the EMA filtered bias\\\')\\n-\\n-        \\n-# Create a checkpoint file pointing to the model\\n-def create_checkpoint_file(model_dir, model_file):\\n-    checkpoint_filename = os.path.join(model_dir, \\\'checkpoint\\\')\\n-    full_model_filename = os.path.join(model_dir, model_file)\\n-    with open(checkpoint_filename, \\\'w\\\') as f:\\n-        f.write(\\\'model_checkpoint_path: "%s"\\\\n\\\' % full_model_filename)\\n-        f.write(\\\'all_model_checkpoint_paths: "%s"\\\\n\\\' % full_model_filename)\\n-        \\n-if __name__ == "__main__":\\n-    unittest.main()\\n-    \\n\\\\ No newline at end of file\\ndiff --git a/test/train_test.py b/test/train_test.py\\ndeleted file mode 100644\\nindex 12cd663..0000000\\n--- a/test/train_test.py\\n+++ /dev/null\\n@@ -1,246 +0,0 @@\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-import unittest\\n-import tempfile\\n-import numpy as np\\n-import cv2\\n-import os\\n-import shutil\\n-import download_and_extract  # @UnresolvedImport\\n-import subprocess\\n-\\n-def memory_usage_psutil():\\n-    # return the memory usage in MB\\n-    import psutil\\n-    process = psutil.Process(os.getpid())\\n-    mem = process.memory_info()[0] / float(2 ** 20)\\n-    return mem\\n-\\n-def align_dataset_if_needed(self):\\n-    if not os.path.exists(\\\'data/lfw_aligned\\\'):\\n-        argv = [\\\'python\\\',\\n-                \\\'src/align/align_dataset_mtcnn.py\\\',\\n-                \\\'data/lfw\\\',\\n-                \\\'data/lfw_aligned\\\',\\n-                \\\'--image_size\\\', \\\'160\\\',\\n-                \\\'--margin\\\', \\\'32\\\' ]\\n-        subprocess.call(argv)\\n-        \\n-        \\n-class TrainTest(unittest.TestCase):\\n-  \\n-    @classmethod\\n-    def setUpClass(self):\\n-        self.tmp_dir = tempfile.mkdtemp()\\n-        self.dataset_dir = os.path.join(self.tmp_dir, \\\'dataset\\\')\\n-        create_mock_dataset(self.dataset_dir, 160)\\n-        self.lfw_pairs_file = create_mock_lfw_pairs(self.tmp_dir)\\n-        print(self.lfw_pairs_file)\\n-        self.pretrained_model_name = \\\'20180402-114759\\\'\\n-        download_and_extract.download_and_extract_file(self.pretrained_model_name, \\\'data/\\\')\\n-        download_and_extract.download_and_extract_file(\\\'lfw-subset\\\', \\\'data/\\\')\\n-        self.model_file = os.path.join(\\\'data\\\', self.pretrained_model_name, \\\'model-%s.ckpt-275\\\' % self.pretrained_model_name)\\n-        self.pretrained_model = os.path.join(\\\'data\\\', self.pretrained_model_name)\\n-        self.frozen_graph_filename = os.path.join(\\\'data\\\', self.pretrained_model_name+\\\'.pb\\\')\\n-        print(\\\'Memory utilization (SetUpClass): %.3f MB\\\' % memory_usage_psutil())\\n-\\n-    @classmethod\\n-    def tearDownClass(self):\\n-        # Recursively remove the temporary directory\\n-        shutil.rmtree(self.tmp_dir)\\n-\\n-    def tearDown(self):\\n-        print(\\\'Memory utilization (TearDown): %.3f MB\\\' % memory_usage_psutil())\\n-\\n-    def test_training_classifier_inception_resnet_v1(self):\\n-        print(\\\'test_training_classifier_inception_resnet_v1\\\')\\n-        argv = [\\\'python\\\',\\n-                \\\'src/train_softmax.py\\\',\\n-                \\\'--logs_base_dir\\\', self.tmp_dir,\\n-                \\\'--models_base_dir\\\', self.tmp_dir,\\n-                \\\'--data_dir\\\', self.dataset_dir,\\n-                \\\'--model_def\\\', \\\'models.inception_resnet_v1\\\',\\n-                \\\'--epoch_size\\\', \\\'1\\\',\\n-                \\\'--max_nrof_epochs\\\', \\\'1\\\',\\n-                \\\'--batch_size\\\', \\\'1\\\',\\n-                \\\'--lfw_pairs\\\', self.lfw_pairs_file,\\n-                \\\'--lfw_dir\\\', self.dataset_dir,\\n-                \\\'--lfw_nrof_folds\\\', \\\'2\\\',\\n-                \\\'--lfw_batch_size\\\', \\\'1\\\',\\n-                \\\'--nrof_preprocess_threads\\\', \\\'1\\\' ]\\n-        subprocess.call(argv)\\n-\\n-    def test_training_classifier_inception_resnet_v2(self):\\n-        print(\\\'test_training_classifier_inception_resnet_v2\\\')\\n-        argv = [\\\'python\\\',\\n-                \\\'src/train_softmax.py\\\',\\n-                \\\'--logs_base_dir\\\', self.tmp_dir,\\n-                \\\'--models_base_dir\\\', self.tmp_dir,\\n-                \\\'--data_dir\\\', self.dataset_dir,\\n-                \\\'--model_def\\\', \\\'models.inception_resnet_v2\\\',\\n-                \\\'--epoch_size\\\', \\\'1\\\',\\n-                \\\'--max_nrof_epochs\\\', \\\'1\\\',\\n-                \\\'--batch_size\\\', \\\'1\\\',\\n-                \\\'--lfw_pairs\\\', self.lfw_pairs_file,\\n-                \\\'--lfw_dir\\\', self.dataset_dir,\\n-                \\\'--lfw_nrof_folds\\\', \\\'2\\\',\\n-                \\\'--lfw_batch_size\\\', \\\'1\\\' ]\\n-        subprocess.call(argv)\\n-  \\n-    def test_training_classifier_squeezenet(self):\\n-        print(\\\'test_training_classifier_squeezenet\\\')\\n-        argv = [\\\'python\\\',\\n-                \\\'src/train_softmax.py\\\',\\n-                \\\'--logs_base_dir\\\', self.tmp_dir,\\n-                \\\'--models_base_dir\\\', self.tmp_dir,\\n-                \\\'--data_dir\\\', self.dataset_dir,\\n-                \\\'--model_def\\\', \\\'models.squeezenet\\\',\\n-                \\\'--epoch_size\\\', \\\'1\\\',\\n-                \\\'--max_nrof_epochs\\\', \\\'1\\\',\\n-                \\\'--batch_size\\\', \\\'1\\\',\\n-                \\\'--lfw_pairs\\\', self.lfw_pairs_file,\\n-                \\\'--lfw_dir\\\', self.dataset_dir,\\n-                \\\'--lfw_nrof_folds\\\', \\\'2\\\',\\n-                \\\'--lfw_batch_size\\\', \\\'1\\\',\\n-                \\\'--nrof_preprocess_threads\\\', \\\'1\\\' ]\\n-        subprocess.call(argv)\\n- \\n-    def test_train_tripletloss_inception_resnet_v1(self):\\n-        print(\\\'test_train_tripletloss_inception_resnet_v1\\\')\\n-        argv = [\\\'python\\\',\\n-                \\\'src/train_tripletloss.py\\\',\\n-                \\\'--logs_base_dir\\\', self.tmp_dir,\\n-                \\\'--models_base_dir\\\', self.tmp_dir,\\n-                \\\'--data_dir\\\', self.dataset_dir,\\n-                \\\'--model_def\\\', \\\'models.inception_resnet_v1\\\',\\n-                \\\'--epoch_size\\\', \\\'1\\\',\\n-                \\\'--max_nrof_epochs\\\', \\\'1\\\',\\n-                \\\'--batch_size\\\', \\\'6\\\',\\n-                \\\'--people_per_batch\\\', \\\'2\\\',\\n-                \\\'--images_per_person\\\', \\\'3\\\',\\n-                \\\'--lfw_pairs\\\', self.lfw_pairs_file,\\n-                \\\'--lfw_dir\\\', self.dataset_dir,\\n-                \\\'--lfw_nrof_folds\\\', \\\'2\\\' ]\\n-        subprocess.call(argv)\\n-  \\n-    def test_finetune_tripletloss_inception_resnet_v1(self):\\n-        print(\\\'test_finetune_tripletloss_inception_resnet_v1\\\')\\n-        argv = [\\\'python\\\',\\n-                \\\'src/train_tripletloss.py\\\',\\n-                \\\'--logs_base_dir\\\', self.tmp_dir,\\n-                \\\'--models_base_dir\\\', self.tmp_dir,\\n-                \\\'--data_dir\\\', self.dataset_dir,\\n-                \\\'--model_def\\\', \\\'models.inception_resnet_v1\\\',\\n-                \\\'--pretrained_model\\\', self.model_file,\\n-                \\\'--embedding_size\\\', \\\'512\\\',\\n-                \\\'--epoch_size\\\', \\\'1\\\',\\n-                \\\'--max_nrof_epochs\\\', \\\'1\\\',\\n-                \\\'--batch_size\\\', \\\'6\\\',\\n-                \\\'--people_per_batch\\\', \\\'2\\\',\\n-                \\\'--images_per_person\\\', \\\'3\\\',\\n-                \\\'--lfw_pairs\\\', self.lfw_pairs_file,\\n-                \\\'--lfw_dir\\\', self.dataset_dir,\\n-                \\\'--lfw_nrof_folds\\\', \\\'2\\\' ]\\n-        subprocess.call(argv)\\n-  \\n-    def test_compare(self):\\n-        print(\\\'test_compare\\\')\\n-        argv = [\\\'python\\\',\\n-                \\\'src/compare.py\\\',\\n-                os.path.join(\\\'data/\\\', self.pretrained_model_name),\\n-                \\\'data/images/Anthony_Hopkins_0001.jpg\\\',\\n-                \\\'data/images/Anthony_Hopkins_0002.jpg\\\' ]\\n-        subprocess.call(argv)\\n-         \\n-    def test_validate_on_lfw(self):\\n-        print(\\\'test_validate_on_lfw\\\')\\n-        align_dataset_if_needed(self)\\n-        argv = [\\\'python\\\',\\n-                \\\'src/validate_on_lfw.py\\\', \\n-                \\\'data/lfw_aligned\\\',\\n-                self.pretrained_model,\\n-                \\\'--lfw_pairs\\\', \\\'data/lfw/pairs_small.txt\\\',\\n-                \\\'--lfw_nrof_folds\\\', \\\'2\\\',\\n-                \\\'--lfw_batch_size\\\', \\\'6\\\']\\n-        subprocess.call(argv)\\n- \\n-    def test_validate_on_lfw_frozen_graph(self):\\n-        print(\\\'test_validate_on_lfw_frozen_graph\\\')\\n-        self.pretrained_model = os.path.join(\\\'data\\\', self.pretrained_model_name)\\n-        frozen_model = os.path.join(self.pretrained_model, self.pretrained_model_name+\\\'.pb\\\')\\n-        argv = [\\\'python\\\',\\n-                \\\'src/validate_on_lfw.py\\\',\\n-                self.dataset_dir,\\n-                frozen_model,\\n-                \\\'--lfw_pairs\\\', self.lfw_pairs_file,\\n-                \\\'--lfw_nrof_folds\\\', \\\'2\\\',\\n-                \\\'--lfw_batch_size\\\', \\\'6\\\']\\n-        subprocess.call(argv)\\n- \\n-    def test_freeze_graph(self):\\n-        print(\\\'test_freeze_graph\\\')\\n-        argv = [\\\'python\\\',\\n-                \\\'src/freeze_graph.py\\\',\\n-                self.pretrained_model,\\n-                self.frozen_graph_filename ]\\n-        subprocess.call(argv)\\n-\\n-# Create a mock dataset with random pixel images\\n-def create_mock_dataset(dataset_dir, image_size):\\n-   \\n-    nrof_persons = 3\\n-    nrof_images_per_person = 2\\n-    np.random.seed(seed=666)\\n-    os.mkdir(dataset_dir)\\n-    for i in range(nrof_persons):\\n-        class_name = \\\'%04d\\\' % (i+1)\\n-        class_dir = os.path.join(dataset_dir, class_name)\\n-        os.mkdir(class_dir)\\n-        for j in range(nrof_images_per_person):\\n-            img_name = \\\'%04d\\\' % (j+1)\\n-            img_path = os.path.join(class_dir, class_name+\\\'_\\\'+img_name + \\\'.png\\\')\\n-            img = np.random.uniform(low=0.0, high=255.0, size=(image_size,image_size,3))\\n-            cv2.imwrite(img_path, img) #@UndefinedVariable\\n-\\n-# Create a mock LFW pairs file\\n-def create_mock_lfw_pairs(tmp_dir):\\n-    pairs_filename = os.path.join(tmp_dir, \\\'pairs_mock.txt\\\')\\n-    with open(pairs_filename, \\\'w\\\') as f:\\n-        f.write(\\\'10 300\\\\n\\\')\\n-        f.write(\\\'0001 1 2\\\\n\\\')\\n-        f.write(\\\'0001 1 0002 1\\\\n\\\')\\n-        f.write(\\\'0002 1 0003 1\\\\n\\\')\\n-        f.write(\\\'0001 1 0003 1\\\\n\\\')\\n-        f.write(\\\'0002 1 2\\\\n\\\')\\n-        f.write(\\\'0001 2 0002 2\\\\n\\\')\\n-        f.write(\\\'0002 2 0003 2\\\\n\\\')\\n-        f.write(\\\'0001 2 0003 2\\\\n\\\')\\n-        f.write(\\\'0003 1 2\\\\n\\\')\\n-        f.write(\\\'0001 1 0002 2\\\\n\\\')\\n-        f.write(\\\'0002 1 0003 2\\\\n\\\')\\n-        f.write(\\\'0001 1 0003 2\\\\n\\\')\\n-    return pairs_filename\\n-\\n-if __name__ == "__main__":\\n-    unittest.main()\\n-    \\n\\\\ No newline at end of file\\ndiff --git a/test/triplet_loss_test.py b/test/triplet_loss_test.py\\ndeleted file mode 100644\\nindex 2648b30..0000000\\n--- a/test/triplet_loss_test.py\\n+++ /dev/null\\n@@ -1,54 +0,0 @@\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-import unittest\\n-import tensorflow as tf\\n-import numpy as np\\n-import facenet\\n-\\n-class DemuxEmbeddingsTest(unittest.TestCase):\\n-  \\n-    def testDemuxEmbeddings(self):\\n-        batch_size = 3*12\\n-        embedding_size = 16\\n-        alpha = 0.2\\n-        \\n-        with tf.Graph().as_default():\\n-        \\n-            embeddings = tf.placeholder(tf.float64, shape=(batch_size, embedding_size), name=\\\'embeddings\\\')\\n-            anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,embedding_size]), 3, 1)\\n-            triplet_loss = facenet.triplet_loss(anchor, positive, negative, alpha)\\n-                \\n-            sess = tf.Session()\\n-            with sess.as_default():\\n-                np.random.seed(seed=666)\\n-                emb = np.random.uniform(size=(batch_size, embedding_size))\\n-                tf_triplet_loss = sess.run(triplet_loss, feed_dict={embeddings:emb})\\n-\\n-                pos_dist_sqr = np.sum(np.square(emb[0::3,:]-emb[1::3,:]),1)\\n-                neg_dist_sqr = np.sum(np.square(emb[0::3,:]-emb[2::3,:]),1)\\n-                np_triplet_loss = np.mean(np.maximum(0.0, pos_dist_sqr - neg_dist_sqr + alpha))\\n-                \\n-                np.testing.assert_almost_equal(tf_triplet_loss, np_triplet_loss, decimal=5, err_msg=\\\'Triplet loss is incorrect\\\')\\n-                      \\n-if __name__ == "__main__":\\n-    unittest.main()\\ndiff --git a/video/a b/video/a\\ndeleted file mode 100644\\nindex 8b13789..0000000\\n--- a/video/a\\n+++ /dev/null\\n@@ -1 +0,0 @@\\n-\\ndiff --git a/video/camtest.mp4 b/video/camtest.mp4\\ndeleted file mode 100644\\nindex a503c89..0000000\\nBinary files a/video/camtest.mp4 and /dev/null differ\'\n\\ No newline at end of file\n+b\'diff --git a/.gitattributes b/.gitattributes\\ndeleted file mode 100644\\nindex 64e2803..0000000\\n--- a/.gitattributes\\n+++ /dev/null\\n@@ -1,3 +0,0 @@\\n-*.zip filter=lfs diff=lfs merge=lfs -text\\n-*.ckpt* filter=lfs diff=lfs merge=lfs -text\\n-*.pb filter=lfs diff=lfs merge=lfs -text\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3) copy.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3) copy.png\\ndeleted file mode 100644\\nindex 91be536..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3) copy.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3).png\\ndeleted file mode 100644\\nindex 91be536..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro.png\\ndeleted file mode 100644\\nindex 0540b28..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro (2).png\\ndeleted file mode 100644\\nindex e9ef9f0..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro.png\\ndeleted file mode 100644\\nindex 04ff255..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (2).png\\ndeleted file mode 100644\\nindex 0c37c23..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (3).png\\ndeleted file mode 100644\\nindex 8e7842e..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (3).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro.png\\ndeleted file mode 100644\\nindex 91b18ff..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (2).png\\ndeleted file mode 100644\\nindex 881520b..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (3).png\\ndeleted file mode 100644\\nindex 03ab7d3..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (3).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro.png\\ndeleted file mode 100644\\nindex fa1f42a..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro (2).png\\ndeleted file mode 100644\\nindex d4d9e24..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro.png\\ndeleted file mode 100644\\nindex 7467151..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_36_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_36_Pro.png\\ndeleted file mode 100644\\nindex 4336cd9..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_36_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro (2).png\\ndeleted file mode 100644\\nindex c3131df..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro.png\\ndeleted file mode 100644\\nindex e010c70..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (2).png\\ndeleted file mode 100644\\nindex 2a5ed54..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (3).png\\ndeleted file mode 100644\\nindex 0d89ab7..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (3).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro.png\\ndeleted file mode 100644\\nindex 669d82a..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (2).png\\ndeleted file mode 100644\\nindex 38b750a..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (3).png\\ndeleted file mode 100644\\nindex c35fb76..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (3).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro.png\\ndeleted file mode 100644\\nindex b1eb45c..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (2).png\\ndeleted file mode 100644\\nindex b331c99..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (3).png\\ndeleted file mode 100644\\nindex 90c13a1..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (3).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro.png\\ndeleted file mode 100644\\nindex 49b56f8..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro (2).png\\ndeleted file mode 100644\\nindex 10d6da8..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro.png\\ndeleted file mode 100644\\nindex 731c604..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (2).png\\ndeleted file mode 100644\\nindex ce6f384..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (3).png\\ndeleted file mode 100644\\nindex 93075b3..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (3).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro.png\\ndeleted file mode 100644\\nindex 2df0742..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (2).png\\ndeleted file mode 100644\\nindex c609125..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (3).png\\ndeleted file mode 100644\\nindex 4fb46a0..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (3).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro.png\\ndeleted file mode 100644\\nindex ccacd60..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro (2).png\\ndeleted file mode 100644\\nindex 79dc7da..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro.png\\ndeleted file mode 100644\\nindex 77e1025..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (2).png\\ndeleted file mode 100644\\nindex 81a3125..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (3).png\\ndeleted file mode 100644\\nindex 70fab4b..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (3).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro.png\\ndeleted file mode 100644\\nindex 628e2ce..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro (2).png\\ndeleted file mode 100644\\nindex df9dc1f..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro.png\\ndeleted file mode 100644\\nindex ec12b5d..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_22_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_22_Pro.png\\ndeleted file mode 100644\\nindex 93da5e6..0000000\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_22_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_54_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_54_Pro.png\\ndeleted file mode 100644\\nindex 5197b4f..0000000\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_54_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_55_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_55_Pro.png\\ndeleted file mode 100644\\nindex da2d35a..0000000\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_55_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro (2).png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro (2).png\\ndeleted file mode 100644\\nindex b458418..0000000\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro.png\\ndeleted file mode 100644\\nindex c9ded27..0000000\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro (2).png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro (2).png\\ndeleted file mode 100644\\nindex 12980b8..0000000\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro.png\\ndeleted file mode 100644\\nindex 36eeeb0..0000000\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro (2).png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro (2).png\\ndeleted file mode 100644\\nindex 3ca9822..0000000\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro.png\\ndeleted file mode 100644\\nindex 7a34d00..0000000\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_59_Pro (2).png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_59_Pro (2).png\\ndeleted file mode 100644\\nindex 5bfc7ed..0000000\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_59_Pro (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png b/Dataset/FaceData/processed/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png\\ndeleted file mode 100644\\nindex 8257ab5..0000000\\nBinary files a/Dataset/FaceData/processed/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png b/Dataset/FaceData/processed/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png\\ndeleted file mode 100644\\nindex 45870ff..0000000\\nBinary files a/Dataset/FaceData/processed/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png b/Dataset/FaceData/processed/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png\\ndeleted file mode 100644\\nindex f4593ba..0000000\\nBinary files a/Dataset/FaceData/processed/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/processed/bounding_boxes_75121.txt b/Dataset/FaceData/processed/bounding_boxes_75121.txt\\ndeleted file mode 100644\\nindex c490bf8..0000000\\n--- a/Dataset/FaceData/processed/bounding_boxes_75121.txt\\n+++ /dev/null\\n@@ -1,9 +0,0 @@\\n-Dataset/FaceData/processed\\\\Quang\\\\WIN_20250411_21_48_55_Pro.png 573 373 798 664\\n-Dataset/FaceData/processed\\\\Quang\\\\WIN_20250411_21_48_56_Pro (2).png 572 373 797 658\\n-Dataset/FaceData/processed\\\\Quang\\\\WIN_20250411_21_48_57_Pro (2).png 571 370 804 659\\n-Dataset/FaceData/processed\\\\Quang\\\\WIN_20250411_21_48_56_Pro.png 570 376 805 663\\n-Dataset/FaceData/processed\\\\Quang\\\\WIN_20250411_21_48_58_Pro.png 574 376 802 659\\n-Dataset/FaceData/processed\\\\Quang\\\\WIN_20250411_21_48_57_Pro.png 569 368 805 662\\n-Dataset/FaceData/processed\\\\Quang\\\\WIN_20250411_21_48_59_Pro (2).png 568 370 806 663\\n-Dataset/FaceData/processed\\\\Quang\\\\WIN_20250411_21_48_58_Pro (2).png 569 366 808 663\\n-Dataset/FaceData/processed\\\\Quang\\\\WIN_20250411_21_48_54_Pro.png 574 380 800 659\\ndiff --git a/Dataset/FaceData/processed/bounding_boxes_79144.txt b/Dataset/FaceData/processed/bounding_boxes_79144.txt\\ndeleted file mode 100644\\nindex ae1c6a8..0000000\\n--- a/Dataset/FaceData/processed/bounding_boxes_79144.txt\\n+++ /dev/null\\n@@ -1,46 +0,0 @@\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_21_Pro.png 506 335 795 706\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_18_Pro (2).png 516 299 808 645\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_18_Pro (3).png 517 303 794 608\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_07_34_Pro.png 569 297 864 704\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_15_Pro.png 493 335 781 698\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_20_Pro.png 521 329 813 711\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_07_33_Pro.png 498 331 803 720\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_15_Pro (2).png 490 340 775 691\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_20_Pro (3).png 518 353 824 720\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_18_Pro.png 521 310 822 683\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_14_Pro (3).png 506 334 807 720\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_13_Pro.png 527 330 824 707\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_15_Pro (3).png 501 325 799 702\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_07_35_Pro.png 491 307 794 681\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_19_Pro (2).png 523 313 803 651\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_07_32_Pro.png 474 344 776 720\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_16_Pro.png 515 323 812 708\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_07_36_Pro.png 482 331 798 720\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_07_32_Pro (2).png 495 343 788 720\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_12_Pro (2).png 529 332 824 708\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_17_Pro (2).png 581 334 858 709\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_16_Pro (2).png 534 325 821 709\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_07_34_Pro (3).png 526 302 825 681\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_07_34_Pro (2).png 563 299 860 702\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_21_Pro (2).png 496 333 771 680\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_19_Pro.png 521 316 793 615\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_22_Pro.png 540 299 826 682\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_13_Pro (3).png 519 341 815 715\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_20_Pro (2).png 526 369 812 720\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_07_33_Pro (2).png 518 321 825 720\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_14_Pro.png 522 338 806 720\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_14_Pro (2).png 517 343 817 720\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_17_Pro.png 574 326 856 720\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_07_35_Pro (2).png 481 331 800 720\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_12_Pro.png 518 322 811 699\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_07_31_Pro.png 521 336 819 720\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_13_Pro (2).png 523 332 818 710\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_07_33_Pro (3).png 532 314 834 719\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_07_31_Pro (3).png 505 324 815 720\\n-Dataset/FaceData/processed\\\\LVQuang\\\\WIN_20250331_09_26_17_Pro (3).png 561 315 848 708\\n-Dataset/FaceData/processed\\\\TranDangHieu\\\\download (1).png 262 672 767 1357\\n-Dataset/FaceData/processed\\\\TranDangHieu\\\\download (2).png 152 762 696 1478\\n-Dataset/FaceData/processed\\\\TranDangHieu\\\\download.png 202 705 750 1444\\n-Dataset/FaceData/processed\\\\Viruss\\\\Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png 50 121 926 1264\\n-Dataset/FaceData/processed\\\\Viruss\\\\Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png 154 108 1417 1799\\n-Dataset/FaceData/processed\\\\Viruss\\\\Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png 364 442 910 1148\\ndiff --git a/Dataset/FaceData/processed/revision_info.txt b/Dataset/FaceData/processed/revision_info.txt\\nindex f0f591b..43cb74a 100644\\n--- a/Dataset/FaceData/processed/revision_info.txt\\n+++ b/Dataset/FaceData/processed/revision_info.txt\\n@@ -2,6 +2,6 @@ arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/proc\\n --------------------\\n tensorflow version: 2.19.0\\n --------------------\\n-git hash: b\\\'69ff1e149c0d84a123d6516ddd82970e65392608\\\'\\n+git hash: b\\\'860368ceb20db792b6b943d52ff5dfb99fcbb23b\\\'\\n --------------------\\n-b\\\'diff --git a/README.md b/README.md\\\\ndeleted file mode 100644\\\\nindex 16ec29c..0000000\\\\n--- a/README.md\\\\n+++ /dev/null\\\\n@@ -1,11 +0,0 @@\\\\n-# MiAI_FaceRecog_3\\\\n-Nh\\\\xe1\\\\xba\\\\xadn di\\\\xe1\\\\xbb\\\\x87n khu\\\\xc3\\\\xb4n m\\\\xe1\\\\xba\\\\xb7t kh\\\\xc3\\\\xa1 chu\\\\xe1\\\\xba\\\\xa9n x\\\\xc3\\\\xa1c b\\\\xe1\\\\xba\\\\xb1ng MTCNN v\\\\xc3\\\\xa0 Facenet!\\\\n-Ch\\\\xe1\\\\xba\\\\xa1y tr\\\\xc3\\\\xaan Tensorflow 2.x\\\\n-\\\\n-Article link: http://miai.vn/2019/09/11/face-recog-2-0-nhan-dien-khuon-mat-trong-video-bang-mtcnn-va-facenet/\\\\n-\\\\n-#M\\\\xc3\\\\xacAI \\\\n-Fanpage: http://facebook.com/miaiblog<br>\\\\n-Group trao \\\\xc4\\\\x91\\\\xe1\\\\xbb\\\\x95i, chia s\\\\xe1\\\\xba\\\\xbb: https://www.facebook.com/groups/miaigroup<br>\\\\n-Website: http://ainoodle.tech<br>\\\\n-Youtube: http://bit.ly/miaiyoutube<br>\\\\ndiff --git a/src/a b/src/a\\\\ndeleted file mode 100644\\\\nindex 8b13789..0000000\\\\n--- a/src/a\\\\n+++ /dev/null\\\\n@@ -1 +0,0 @@\\\\n-\\\\ndiff --git a/src/calculate_filtering_metrics.py b/src/calculate_filtering_metrics.py\\\\ndeleted file mode 100644\\\\nindex f60b9ae..0000000\\\\n--- a/src/calculate_filtering_metrics.py\\\\n+++ /dev/null\\\\n@@ -1,128 +0,0 @@\\\\n-"""Calculate filtering metrics for a dataset and store in a .hdf file.\\\\n-"""\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-import tensorflow as tf\\\\n-import numpy as np\\\\n-import argparse\\\\n-import facenet\\\\n-import os\\\\n-import sys\\\\n-import time\\\\n-import h5py\\\\n-import math\\\\n-from tensorflow.python.platform import gfile\\\\n-from six import iteritems\\\\n-\\\\n-def main(args):\\\\n-    dataset = facenet.get_dataset(args.dataset_dir)\\\\n-  \\\\n-    with tf.Graph().as_default():\\\\n-      \\\\n-        # Get a list of image paths and their labels\\\\n-        image_list, label_list = facenet.get_image_paths_and_labels(dataset)\\\\n-        nrof_images = len(image_list)\\\\n-        image_indices = range(nrof_images)\\\\n-\\\\n-        image_batch, label_batch = facenet.read_and_augment_data(image_list,\\\\n-            image_indices, args.image_size, args.batch_size, None, \\\\n-            False, False, False, nrof_preprocess_threads=4, shuffle=False)\\\\n-        \\\\n-        model_exp = os.path.expanduser(args.model_file)\\\\n-        with gfile.FastGFile(model_exp,\\\\\\\'rb\\\\\\\') as f:\\\\n-            graph_def = tf.GraphDef()\\\\n-            graph_def.ParseFromString(f.read())\\\\n-            input_map={\\\\\\\'input\\\\\\\':image_batch, \\\\\\\'phase_train\\\\\\\':False}\\\\n-            tf.import_graph_def(graph_def, input_map=input_map, name=\\\\\\\'net\\\\\\\')\\\\n-        \\\\n-        embeddings = tf.get_default_graph().get_tensor_by_name("net/embeddings:0")\\\\n-\\\\n-        with tf.Session() as sess:\\\\n-            tf.train.start_queue_runners(sess=sess)\\\\n-                \\\\n-            embedding_size = int(embeddings.get_shape()[1])\\\\n-            nrof_batches = int(math.ceil(nrof_images / args.batch_size))\\\\n-            nrof_classes = len(dataset)\\\\n-            label_array = np.array(label_list)\\\\n-            class_names = [cls.name for cls in dataset]\\\\n-            nrof_examples_per_class = [ len(cls.image_paths) for cls in dataset ]\\\\n-            class_variance = np.zeros((nrof_classes,))\\\\n-            class_center = np.zeros((nrof_classes,embedding_size))\\\\n-            distance_to_center = np.ones((len(label_list),))*np.NaN\\\\n-            emb_array = np.zeros((0,embedding_size))\\\\n-            idx_array = np.zeros((0,), dtype=np.int32)\\\\n-            lab_array = np.zeros((0,), dtype=np.int32)\\\\n-            index_arr = np.append(0, np.cumsum(nrof_examples_per_class))\\\\n-            for i in range(nrof_batches):\\\\n-                t = time.time()\\\\n-                emb, idx = sess.run([embeddings, label_batch])\\\\n-                emb_array = np.append(emb_array, emb, axis=0)\\\\n-                idx_array = np.append(idx_array, idx, axis=0)\\\\n-                lab_array = np.append(lab_array, label_array[idx], axis=0)\\\\n-                for cls in set(lab_array):\\\\n-                    cls_idx = np.where(lab_array==cls)[0]\\\\n-                    if cls_idx.shape[0]==nrof_examples_per_class[cls]:\\\\n-                        # We have calculated all the embeddings for this class\\\\n-                        i2 = np.argsort(idx_array[cls_idx])\\\\n-                        emb_class = emb_array[cls_idx,:]\\\\n-                        emb_sort = emb_class[i2,:]\\\\n-                        center = np.mean(emb_sort, axis=0)\\\\n-                        diffs = emb_sort - center\\\\n-                        dists_sqr = np.sum(np.square(diffs), axis=1)\\\\n-                        class_variance[cls] = np.mean(dists_sqr)\\\\n-                        class_center[cls,:] = center\\\\n-                        distance_to_center[index_arr[cls]:index_arr[cls+1]] = np.sqrt(dists_sqr)\\\\n-                        emb_array = np.delete(emb_array, cls_idx, axis=0)\\\\n-                        idx_array = np.delete(idx_array, cls_idx, axis=0)\\\\n-                        lab_array = np.delete(lab_array, cls_idx, axis=0)\\\\n-\\\\n-                        \\\\n-                print(\\\\\\\'Batch %d in %.3f seconds\\\\\\\' % (i, time.time()-t))\\\\n-                \\\\n-            print(\\\\\\\'Writing filtering data to %s\\\\\\\' % args.data_file_name)\\\\n-            mdict = {\\\\\\\'class_names\\\\\\\':class_names, \\\\\\\'image_list\\\\\\\':image_list, \\\\\\\'label_list\\\\\\\':label_list, \\\\\\\'distance_to_center\\\\\\\':distance_to_center }\\\\n-            with h5py.File(args.data_file_name, \\\\\\\'w\\\\\\\') as f:\\\\n-                for key, value in iteritems(mdict):\\\\n-                    f.create_dataset(key, data=value)\\\\n-                        \\\\n-def parse_arguments(argv):\\\\n-    parser = argparse.ArgumentParser()\\\\n-    \\\\n-    parser.add_argument(\\\\\\\'dataset_dir\\\\\\\', type=str,\\\\n-        help=\\\\\\\'Path to the directory containing aligned dataset.\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'model_file\\\\\\\', type=str,\\\\n-        help=\\\\\\\'File containing the frozen model in protobuf (.pb) format to use for feature extraction.\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'data_file_name\\\\\\\', type=str,\\\\n-        help=\\\\\\\'The name of the file to store filtering data in.\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Image size.\\\\\\\', default=160)\\\\n-    parser.add_argument(\\\\\\\'--batch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=90)\\\\n-    return parser.parse_args(argv)\\\\n-\\\\n-if __name__ == \\\\\\\'__main__\\\\\\\':\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\ndiff --git a/src/decode_msceleb_dataset.py b/src/decode_msceleb_dataset.py\\\\ndeleted file mode 100644\\\\nindex 4556bfa..0000000\\\\n--- a/src/decode_msceleb_dataset.py\\\\n+++ /dev/null\\\\n@@ -1,87 +0,0 @@\\\\n-"""Decode the MsCelebV1 dataset in TSV (tab separated values) format downloaded from\\\\n-https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/\\\\n-"""\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-from scipy import misc\\\\n-import numpy as np\\\\n-import base64\\\\n-import sys\\\\n-import os\\\\n-import cv2\\\\n-import argparse\\\\n-import facenet\\\\n-\\\\n-\\\\n-# File format: text files, each line is an image record containing 6 columns, delimited by TAB.\\\\n-# Column1: Freebase MID\\\\n-# Column2: Query/Name\\\\n-# Column3: ImageSearchRank\\\\n-# Column4: ImageURL\\\\n-# Column5: PageURL\\\\n-# Column6: ImageData_Base64Encoded\\\\n-\\\\n-def main(args):\\\\n-    output_dir = os.path.expanduser(args.output_dir)\\\\n-  \\\\n-    if not os.path.exists(output_dir):\\\\n-        os.mkdir(output_dir)\\\\n-  \\\\n-    # Store some git revision info in a text file in the output directory\\\\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\\\\n-    facenet.store_revision_info(src_path, output_dir, \\\\\\\' \\\\\\\'.join(sys.argv))\\\\n-    \\\\n-    i = 0\\\\n-    for f in args.tsv_files:\\\\n-        for line in f:\\\\n-            fields = line.split(\\\\\\\'\\\\\\\\t\\\\\\\')\\\\n-            class_dir = fields[0]\\\\n-            img_name = fields[1] + \\\\\\\'-\\\\\\\' + fields[4] + \\\\\\\'.\\\\\\\' + args.output_format\\\\n-            img_string = fields[5]\\\\n-            img_dec_string = base64.b64decode(img_string)\\\\n-            img_data = np.fromstring(img_dec_string, dtype=np.uint8)\\\\n-            img = cv2.imdecode(img_data, cv2.IMREAD_COLOR) #pylint: disable=maybe-no-member\\\\n-            if args.size:\\\\n-                img = misc.imresize(img, (args.size, args.size), interp=\\\\\\\'bilinear\\\\\\\')\\\\n-            full_class_dir = os.path.join(output_dir, class_dir)\\\\n-            if not os.path.exists(full_class_dir):\\\\n-                os.mkdir(full_class_dir)\\\\n-            full_path = os.path.join(full_class_dir, img_name.replace(\\\\\\\'/\\\\\\\',\\\\\\\'_\\\\\\\'))\\\\n-            cv2.imwrite(full_path, img) #pylint: disable=maybe-no-member\\\\n-            print(\\\\\\\'%8d: %s\\\\\\\' % (i, full_path))\\\\n-            i += 1\\\\n-  \\\\n-if __name__ == \\\\\\\'__main__\\\\\\\':\\\\n-    parser = argparse.ArgumentParser()\\\\n-\\\\n-    parser.add_argument(\\\\\\\'output_dir\\\\\\\', type=str, help=\\\\\\\'Output base directory for the image dataset\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'tsv_files\\\\\\\', type=argparse.FileType(\\\\\\\'r\\\\\\\'), nargs=\\\\\\\'+\\\\\\\', help=\\\\\\\'Input TSV file name(s)\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--size\\\\\\\', type=int, help=\\\\\\\'Images are resized to the given size\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--output_format\\\\\\\', type=str, help=\\\\\\\'Format of the output images\\\\\\\', default=\\\\\\\'png\\\\\\\', choices=[\\\\\\\'png\\\\\\\', \\\\\\\'jpg\\\\\\\'])\\\\n-\\\\n-    main(parser.parse_args())\\\\n-\\\\ndiff --git a/src/download_and_extract.py b/src/download_and_extract.py\\\\ndeleted file mode 100644\\\\nindex a835ac2..0000000\\\\n--- a/src/download_and_extract.py\\\\n+++ /dev/null\\\\n@@ -1,51 +0,0 @@\\\\n-import requests\\\\n-import zipfile\\\\n-import os\\\\n-\\\\n-model_dict = {\\\\n-    \\\\\\\'lfw-subset\\\\\\\':      \\\\\\\'1B5BQUZuJO-paxdN8UclxeHAR1WnR_Tzi\\\\\\\', \\\\n-    \\\\\\\'20170131-234652\\\\\\\': \\\\\\\'0B5MzpY9kBtDVSGM0RmVET2EwVEk\\\\\\\',\\\\n-    \\\\\\\'20170216-091149\\\\\\\': \\\\\\\'0B5MzpY9kBtDVTGZjcWkzT3pldDA\\\\\\\',\\\\n-    \\\\\\\'20170512-110547\\\\\\\': \\\\\\\'0B5MzpY9kBtDVZ2RpVDYwWmxoSUk\\\\\\\',\\\\n-    \\\\\\\'20180402-114759\\\\\\\': \\\\\\\'1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-\\\\\\\'\\\\n-    }\\\\n-\\\\n-def download_and_extract_file(model_name, data_dir):\\\\n-    file_id = model_dict[model_name]\\\\n-    destination = os.path.join(data_dir, model_name + \\\\\\\'.zip\\\\\\\')\\\\n-    if not os.path.exists(destination):\\\\n-        print(\\\\\\\'Downloading file to %s\\\\\\\' % destination)\\\\n-        download_file_from_google_drive(file_id, destination)\\\\n-        with zipfile.ZipFile(destination, \\\\\\\'r\\\\\\\') as zip_ref:\\\\n-            print(\\\\\\\'Extracting file to %s\\\\\\\' % data_dir)\\\\n-            zip_ref.extractall(data_dir)\\\\n-\\\\n-def download_file_from_google_drive(file_id, destination):\\\\n-    \\\\n-        URL = "https://drive.google.com/uc?export=download"\\\\n-    \\\\n-        session = requests.Session()\\\\n-    \\\\n-        response = session.get(URL, params = { \\\\\\\'id\\\\\\\' : file_id }, stream = True)\\\\n-        token = get_confirm_token(response)\\\\n-    \\\\n-        if token:\\\\n-            params = { \\\\\\\'id\\\\\\\' : file_id, \\\\\\\'confirm\\\\\\\' : token }\\\\n-            response = session.get(URL, params = params, stream = True)\\\\n-    \\\\n-        save_response_content(response, destination)    \\\\n-\\\\n-def get_confirm_token(response):\\\\n-    for key, value in response.cookies.items():\\\\n-        if key.startswith(\\\\\\\'download_warning\\\\\\\'):\\\\n-            return value\\\\n-\\\\n-    return None\\\\n-\\\\n-def save_response_content(response, destination):\\\\n-    CHUNK_SIZE = 32768\\\\n-\\\\n-    with open(destination, "wb") as f:\\\\n-        for chunk in response.iter_content(CHUNK_SIZE):\\\\n-            if chunk: # filter out keep-alive new chunks\\\\n-                f.write(chunk)\\\\ndiff --git a/src/face_rec.py b/src/face_rec.py\\\\ndeleted file mode 100644\\\\nindex f92cccf..0000000\\\\n--- a/src/face_rec.py\\\\n+++ /dev/null\\\\n@@ -1,135 +0,0 @@\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-import tensorflow as tf\\\\n-import argparse\\\\n-import facenet\\\\n-import os\\\\n-import sys\\\\n-import math\\\\n-import pickle\\\\n-import align.detect_face\\\\n-import numpy as np\\\\n-import cv2\\\\n-import collections\\\\n-from sklearn.svm import SVC\\\\n-\\\\n-\\\\n-def main():\\\\n-    parser = argparse.ArgumentParser()\\\\n-    parser.add_argument(\\\\\\\'--path\\\\\\\', help=\\\\\\\'Path of the video you want to test on.\\\\\\\', default=0)\\\\n-    args = parser.parse_args()\\\\n-    \\\\n-    # Cai dat cac tham so can thiet\\\\n-    MINSIZE = 20\\\\n-    THRESHOLD = [0.6, 0.7, 0.7]\\\\n-    FACTOR = 0.709\\\\n-    IMAGE_SIZE = 182\\\\n-    INPUT_IMAGE_SIZE = 160\\\\n-    CLASSIFIER_PATH = \\\\\\\'Models/facemodel.pkl\\\\\\\'\\\\n-    VIDEO_PATH = args.path\\\\n-    FACENET_MODEL_PATH = \\\\\\\'Models/20180402-114759.pb\\\\\\\'\\\\n-\\\\n-    # Load model da train de nhan dien khuon mat - thuc chat la classifier\\\\n-    with open(CLASSIFIER_PATH, \\\\\\\'rb\\\\\\\') as file:\\\\n-        model, class_names = pickle.load(file)\\\\n-    print("Custom Classifier, Successfully loaded")\\\\n-\\\\n-    with tf.Graph().as_default():\\\\n-\\\\n-        # Cai dat GPU neu co\\\\n-        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\n-\\\\n-        with sess.as_default():\\\\n-\\\\n-            # Load model MTCNN phat hien khuon mat\\\\n-            print(\\\\\\\'Loading feature extraction model\\\\\\\')\\\\n-            facenet.load_model(FACENET_MODEL_PATH)\\\\n-\\\\n-            # Lay tensor input va output\\\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\n-            embedding_size = embeddings.get_shape()[1]\\\\n-\\\\n-            # Cai dat cac mang con\\\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\n-\\\\n-            people_detected = set()\\\\n-            person_detected = collections.Counter()\\\\n-\\\\n-            # Lay hinh anh tu file video\\\\n-            cap = cv2.VideoCapture(VIDEO_PATH)\\\\n-\\\\n-            while (cap.isOpened()):\\\\n-                # Doc tung frame\\\\n-                ret, frame = cap.read()\\\\n-\\\\n-                # Phat hien khuon mat, tra ve vi tri trong bounding_boxes\\\\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\n-\\\\n-                faces_found = bounding_boxes.shape[0]\\\\n-                try:\\\\n-                    # Neu co it nhat 1 khuon mat trong frame\\\\n-                    if faces_found > 0:\\\\n-                        det = bounding_boxes[:, 0:4]\\\\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\n-                        for i in range(faces_found):\\\\n-                            bb[i][0] = det[i][0]\\\\n-                            bb[i][1] = det[i][1]\\\\n-                            bb[i][2] = det[i][2]\\\\n-                            bb[i][3] = det[i][3]\\\\n-\\\\n-                            # Cat phan khuon mat tim duoc\\\\n-                            cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\n-                            scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\n-                                                interpolation=cv2.INTER_CUBIC)\\\\n-                            scaled = facenet.prewhiten(scaled)\\\\n-                            scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\n-                            feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\n-                            emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\n-                            \\\\n-                            # Dua vao model de classifier\\\\n-                            predictions = model.predict_proba(emb_array)\\\\n-                            best_class_indices = np.argmax(predictions, axis=1)\\\\n-                            best_class_probabilities = predictions[\\\\n-                                np.arange(len(best_class_indices)), best_class_indices]\\\\n-                            \\\\n-                            # Lay ra ten va ty le % cua class co ty le cao nhat\\\\n-                            best_name = class_names[best_class_indices[0]]\\\\n-                            print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\n-\\\\n-                            # Ve khung mau xanh quanh khuon mat\\\\n-                            cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\\\n-                            text_x = bb[i][0]\\\\n-                            text_y = bb[i][3] + 20\\\\n-\\\\n-                            # Neu ty le nhan dang > 0.5 thi hien thi ten\\\\n-                            if best_class_probabilities > 0.5:\\\\n-                                name = class_names[best_class_indices[0]]\\\\n-                            else:\\\\n-                                # Con neu <=0.5 thi hien thi Unknow\\\\n-                                name = "Unknown"\\\\n-                                \\\\n-                            # Viet text len tren frame    \\\\n-                            cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\n-                                        1, (255, 255, 255), thickness=1, lineType=2)\\\\n-                            cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\\\\n-                                        cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\n-                                        1, (255, 255, 255), thickness=1, lineType=2)\\\\n-                            person_detected[best_name] += 1\\\\n-                except:\\\\n-                    pass\\\\n-\\\\n-                # Hien thi frame len man hinh\\\\n-                cv2.imshow(\\\\\\\'Face Recognition\\\\\\\', frame)\\\\n-                if cv2.waitKey(1) & 0xFF == ord(\\\\\\\'q\\\\\\\'):\\\\n-                    break\\\\n-\\\\n-            cap.release()\\\\n-            cv2.destroyAllWindows()\\\\n-\\\\n-\\\\n-main()\\\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\\\nindex 1a425a5..455f67a 100644\\\\n--- a/src/face_rec_cam.py\\\\n+++ b/src/face_rec_cam.py\\\\n@@ -52,9 +52,10 @@ def main():\\\\n             facenet.load_model(FACENET_MODEL_PATH)\\\\n \\\\n             # Get input and output tensors\\\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\n+            graph = tf.compat.v1.get_default_graph()\\\\n+            images_placeholder = graph.get_tensor_by_name("input:0")\\\\n+            embeddings = graph.get_tensor_by_name("embeddings:0")\\\\n+            phase_train_placeholder = graph.get_tensor_by_name("phase_train:0")\\\\n             embedding_size = embeddings.get_shape()[1]\\\\n \\\\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\ndiff --git a/src/freeze_graph.py b/src/freeze_graph.py\\\\ndeleted file mode 100644\\\\nindex 3584c18..0000000\\\\n--- a/src/freeze_graph.py\\\\n+++ /dev/null\\\\n@@ -1,103 +0,0 @@\\\\n-"""Imports a model metagraph and checkpoint file, converts the variables to constants\\\\n-and exports the model as a graphdef protobuf\\\\n-"""\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-from tensorflow.python.framework import graph_util\\\\n-import tensorflow as tf\\\\n-import argparse\\\\n-import os\\\\n-import sys\\\\n-import facenet\\\\n-from six.moves import xrange  # @UnresolvedImport\\\\n-\\\\n-def main(args):\\\\n-    with tf.Graph().as_default():\\\\n-        with tf.Session() as sess:\\\\n-            # Load the model metagraph and checkpoint\\\\n-            print(\\\\\\\'Model directory: %s\\\\\\\' % args.model_dir)\\\\n-            meta_file, ckpt_file = facenet.get_model_filenames(os.path.expanduser(args.model_dir))\\\\n-            \\\\n-            print(\\\\\\\'Metagraph file: %s\\\\\\\' % meta_file)\\\\n-            print(\\\\\\\'Checkpoint file: %s\\\\\\\' % ckpt_file)\\\\n-\\\\n-            model_dir_exp = os.path.expanduser(args.model_dir)\\\\n-            saver = tf.train.import_meta_graph(os.path.join(model_dir_exp, meta_file), clear_devices=True)\\\\n-            tf.get_default_session().run(tf.global_variables_initializer())\\\\n-            tf.get_default_session().run(tf.local_variables_initializer())\\\\n-            saver.restore(tf.get_default_session(), os.path.join(model_dir_exp, ckpt_file))\\\\n-            \\\\n-            # Retrieve the protobuf graph definition and fix the batch norm nodes\\\\n-            input_graph_def = sess.graph.as_graph_def()\\\\n-            \\\\n-            # Freeze the graph def\\\\n-            output_graph_def = freeze_graph_def(sess, input_graph_def, \\\\\\\'embeddings,label_batch\\\\\\\')\\\\n-\\\\n-        # Serialize and dump the output graph to the filesystem\\\\n-        with tf.gfile.GFile(args.output_file, \\\\\\\'wb\\\\\\\') as f:\\\\n-            f.write(output_graph_def.SerializeToString())\\\\n-        print("%d ops in the final graph: %s" % (len(output_graph_def.node), args.output_file))\\\\n-        \\\\n-def freeze_graph_def(sess, input_graph_def, output_node_names):\\\\n-    for node in input_graph_def.node:\\\\n-        if node.op == \\\\\\\'RefSwitch\\\\\\\':\\\\n-            node.op = \\\\\\\'Switch\\\\\\\'\\\\n-            for index in xrange(len(node.input)):\\\\n-                if \\\\\\\'moving_\\\\\\\' in node.input[index]:\\\\n-                    node.input[index] = node.input[index] + \\\\\\\'/read\\\\\\\'\\\\n-        elif node.op == \\\\\\\'AssignSub\\\\\\\':\\\\n-            node.op = \\\\\\\'Sub\\\\\\\'\\\\n-            if \\\\\\\'use_locking\\\\\\\' in node.attr: del node.attr[\\\\\\\'use_locking\\\\\\\']\\\\n-        elif node.op == \\\\\\\'AssignAdd\\\\\\\':\\\\n-            node.op = \\\\\\\'Add\\\\\\\'\\\\n-            if \\\\\\\'use_locking\\\\\\\' in node.attr: del node.attr[\\\\\\\'use_locking\\\\\\\']\\\\n-    \\\\n-    # Get the list of important nodes\\\\n-    whitelist_names = []\\\\n-    for node in input_graph_def.node:\\\\n-        if (node.name.startswith(\\\\\\\'InceptionResnet\\\\\\\') or node.name.startswith(\\\\\\\'embeddings\\\\\\\') or \\\\n-                node.name.startswith(\\\\\\\'image_batch\\\\\\\') or node.name.startswith(\\\\\\\'label_batch\\\\\\\') or\\\\n-                node.name.startswith(\\\\\\\'phase_train\\\\\\\') or node.name.startswith(\\\\\\\'Logits\\\\\\\')):\\\\n-            whitelist_names.append(node.name)\\\\n-\\\\n-    # Replace all the variables in the graph with constants of the same values\\\\n-    output_graph_def = graph_util.convert_variables_to_constants(\\\\n-        sess, input_graph_def, output_node_names.split(","),\\\\n-        variable_names_whitelist=whitelist_names)\\\\n-    return output_graph_def\\\\n-  \\\\n-def parse_arguments(argv):\\\\n-    parser = argparse.ArgumentParser()\\\\n-    \\\\n-    parser.add_argument(\\\\\\\'model_dir\\\\\\\', type=str, \\\\n-        help=\\\\\\\'Directory containing the metagraph (.meta) file and the checkpoint (ckpt) file containing model parameters\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'output_file\\\\\\\', type=str, \\\\n-        help=\\\\\\\'Filename for the exported graphdef protobuf (.pb)\\\\\\\')\\\\n-    return parser.parse_args(argv)\\\\n-\\\\n-if __name__ == \\\\\\\'__main__\\\\\\\':\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\ndiff --git a/src/lfw.py b/src/lfw.py\\\\ndeleted file mode 100644\\\\nindex 9194433..0000000\\\\n--- a/src/lfw.py\\\\n+++ /dev/null\\\\n@@ -1,86 +0,0 @@\\\\n-"""Helper for evaluation on the Labeled Faces in the Wild dataset \\\\n-"""\\\\n-\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-import os\\\\n-import numpy as np\\\\n-import facenet\\\\n-\\\\n-def evaluate(embeddings, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\\\\n-    # Calculate evaluation metrics\\\\n-    thresholds = np.arange(0, 4, 0.01)\\\\n-    embeddings1 = embeddings[0::2]\\\\n-    embeddings2 = embeddings[1::2]\\\\n-    tpr, fpr, accuracy = facenet.calculate_roc(thresholds, embeddings1, embeddings2,\\\\n-        np.asarray(actual_issame), nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\\\\n-    thresholds = np.arange(0, 4, 0.001)\\\\n-    val, val_std, far = facenet.calculate_val(thresholds, embeddings1, embeddings2,\\\\n-        np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\\\\n-    return tpr, fpr, accuracy, val, val_std, far\\\\n-\\\\n-def get_paths(lfw_dir, pairs):\\\\n-    nrof_skipped_pairs = 0\\\\n-    path_list = []\\\\n-    issame_list = []\\\\n-    for pair in pairs:\\\\n-        if len(pair) == 3:\\\\n-            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \\\\\\\'_\\\\\\\' + \\\\\\\'%04d\\\\\\\' % int(pair[1])))\\\\n-            path1 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \\\\\\\'_\\\\\\\' + \\\\\\\'%04d\\\\\\\' % int(pair[2])))\\\\n-            issame = True\\\\n-        elif len(pair) == 4:\\\\n-            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \\\\\\\'_\\\\\\\' + \\\\\\\'%04d\\\\\\\' % int(pair[1])))\\\\n-            path1 = add_extension(os.path.join(lfw_dir, pair[2], pair[2] + \\\\\\\'_\\\\\\\' + \\\\\\\'%04d\\\\\\\' % int(pair[3])))\\\\n-            issame = False\\\\n-        if os.path.exists(path0) and os.path.exists(path1):    # Only add the pair if both paths exist\\\\n-            path_list += (path0,path1)\\\\n-            issame_list.append(issame)\\\\n-        else:\\\\n-            nrof_skipped_pairs += 1\\\\n-    if nrof_skipped_pairs>0:\\\\n-        print(\\\\\\\'Skipped %d image pairs\\\\\\\' % nrof_skipped_pairs)\\\\n-    \\\\n-    return path_list, issame_list\\\\n-  \\\\n-def add_extension(path):\\\\n-    if os.path.exists(path+\\\\\\\'.jpg\\\\\\\'):\\\\n-        return path+\\\\\\\'.jpg\\\\\\\'\\\\n-    elif os.path.exists(path+\\\\\\\'.png\\\\\\\'):\\\\n-        return path+\\\\\\\'.png\\\\\\\'\\\\n-    else:\\\\n-        raise RuntimeError(\\\\\\\'No file "%s" with extension png or jpg.\\\\\\\' % path)\\\\n-\\\\n-def read_pairs(pairs_filename):\\\\n-    pairs = []\\\\n-    with open(pairs_filename, \\\\\\\'r\\\\\\\') as f:\\\\n-        for line in f.readlines()[1:]:\\\\n-            pair = line.strip().split()\\\\n-            pairs.append(pair)\\\\n-    return np.array(pairs)\\\\n-\\\\n-\\\\n-\\\\ndiff --git a/src/train_softmax.py b/src/train_softmax.py\\\\ndeleted file mode 100644\\\\nindex 6b0b28b..0000000\\\\n--- a/src/train_softmax.py\\\\n+++ /dev/null\\\\n@@ -1,580 +0,0 @@\\\\n-"""Training a face recognizer with TensorFlow using softmax cross entropy loss\\\\n-"""\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-from datetime import datetime\\\\n-import os.path\\\\n-import time\\\\n-import sys\\\\n-import random\\\\n-import tensorflow as tf\\\\n-import numpy as np\\\\n-import importlib\\\\n-import argparse\\\\n-import facenet\\\\n-import lfw\\\\n-import h5py\\\\n-import math\\\\n-import tensorflow.contrib.slim as slim\\\\n-from tensorflow.python.ops import data_flow_ops\\\\n-from tensorflow.python.framework import ops\\\\n-from tensorflow.python.ops import array_ops\\\\n-\\\\n-def main(args):\\\\n-  \\\\n-    network = importlib.import_module(args.model_def)\\\\n-    image_size = (args.image_size, args.image_size)\\\\n-\\\\n-    subdir = datetime.strftime(datetime.now(), \\\\\\\'%Y%m%d-%H%M%S\\\\\\\')\\\\n-    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\\\\n-    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\\\\\\\'t exist\\\\n-        os.makedirs(log_dir)\\\\n-    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\\\\n-    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\\\\\\\'t exist\\\\n-        os.makedirs(model_dir)\\\\n-\\\\n-    stat_file_name = os.path.join(log_dir, \\\\\\\'stat.h5\\\\\\\')\\\\n-\\\\n-    # Write arguments to a text file\\\\n-    facenet.write_arguments_to_file(args, os.path.join(log_dir, \\\\\\\'arguments.txt\\\\\\\'))\\\\n-        \\\\n-    # Store some git revision info in a text file in the log directory\\\\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\\\\n-    facenet.store_revision_info(src_path, log_dir, \\\\\\\' \\\\\\\'.join(sys.argv))\\\\n-\\\\n-    np.random.seed(seed=args.seed)\\\\n-    random.seed(args.seed)\\\\n-    dataset = facenet.get_dataset(args.data_dir)\\\\n-    if args.filter_filename:\\\\n-        dataset = filter_dataset(dataset, os.path.expanduser(args.filter_filename), \\\\n-            args.filter_percentile, args.filter_min_nrof_images_per_class)\\\\n-        \\\\n-    if args.validation_set_split_ratio>0.0:\\\\n-        train_set, val_set = facenet.split_dataset(dataset, args.validation_set_split_ratio, args.min_nrof_val_images_per_class, \\\\\\\'SPLIT_IMAGES\\\\\\\')\\\\n-    else:\\\\n-        train_set, val_set = dataset, []\\\\n-        \\\\n-    nrof_classes = len(train_set)\\\\n-    \\\\n-    print(\\\\\\\'Model directory: %s\\\\\\\' % model_dir)\\\\n-    print(\\\\\\\'Log directory: %s\\\\\\\' % log_dir)\\\\n-    pretrained_model = None\\\\n-    if args.pretrained_model:\\\\n-        pretrained_model = os.path.expanduser(args.pretrained_model)\\\\n-        print(\\\\\\\'Pre-trained model: %s\\\\\\\' % pretrained_model)\\\\n-    \\\\n-    if args.lfw_dir:\\\\n-        print(\\\\\\\'LFW directory: %s\\\\\\\' % args.lfw_dir)\\\\n-        # Read the file containing the pairs used for testing\\\\n-        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\\\\n-        # Get the paths for the corresponding images\\\\n-        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\\\\n-    \\\\n-    with tf.Graph().as_default():\\\\n-        tf.set_random_seed(args.seed)\\\\n-        global_step = tf.Variable(0, trainable=False)\\\\n-        \\\\n-        # Get a list of image paths and their labels\\\\n-        image_list, label_list = facenet.get_image_paths_and_labels(train_set)\\\\n-        assert len(image_list)>0, \\\\\\\'The training set should not be empty\\\\\\\'\\\\n-        \\\\n-        val_image_list, val_label_list = facenet.get_image_paths_and_labels(val_set)\\\\n-\\\\n-        # Create a queue that produces indices into the image_list and label_list \\\\n-        labels = ops.convert_to_tensor(label_list, dtype=tf.int32)\\\\n-        range_size = array_ops.shape(labels)[0]\\\\n-        index_queue = tf.train.range_input_producer(range_size, num_epochs=None,\\\\n-                             shuffle=True, seed=None, capacity=32)\\\\n-        \\\\n-        index_dequeue_op = index_queue.dequeue_many(args.batch_size*args.epoch_size, \\\\\\\'index_dequeue\\\\\\\')\\\\n-        \\\\n-        learning_rate_placeholder = tf.placeholder(tf.float32, name=\\\\\\\'learning_rate\\\\\\\')\\\\n-        batch_size_placeholder = tf.placeholder(tf.int32, name=\\\\\\\'batch_size\\\\\\\')\\\\n-        phase_train_placeholder = tf.placeholder(tf.bool, name=\\\\\\\'phase_train\\\\\\\')\\\\n-        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\\\\\\\'image_paths\\\\\\\')\\\\n-        labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\\\\\\\'labels\\\\\\\')\\\\n-        control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\\\\\\\'control\\\\\\\')\\\\n-        \\\\n-        nrof_preprocess_threads = 4\\\\n-        input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\\\\n-                                    dtypes=[tf.string, tf.int32, tf.int32],\\\\n-                                    shapes=[(1,), (1,), (1,)],\\\\n-                                    shared_name=None, name=None)\\\\n-        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder, control_placeholder], name=\\\\\\\'enqueue_op\\\\\\\')\\\\n-        image_batch, label_batch = facenet.create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder)\\\\n-\\\\n-        image_batch = tf.identity(image_batch, \\\\\\\'image_batch\\\\\\\')\\\\n-        image_batch = tf.identity(image_batch, \\\\\\\'input\\\\\\\')\\\\n-        label_batch = tf.identity(label_batch, \\\\\\\'label_batch\\\\\\\')\\\\n-        \\\\n-        print(\\\\\\\'Number of classes in training set: %d\\\\\\\' % nrof_classes)\\\\n-        print(\\\\\\\'Number of examples in training set: %d\\\\\\\' % len(image_list))\\\\n-\\\\n-        print(\\\\\\\'Number of classes in validation set: %d\\\\\\\' % len(val_set))\\\\n-        print(\\\\\\\'Number of examples in validation set: %d\\\\\\\' % len(val_image_list))\\\\n-        \\\\n-        print(\\\\\\\'Building training graph\\\\\\\')\\\\n-        \\\\n-        # Build the inference graph\\\\n-        prelogits, _ = network.inference(image_batch, args.keep_probability, \\\\n-            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size, \\\\n-            weight_decay=args.weight_decay)\\\\n-        logits = slim.fully_connected(prelogits, len(train_set), activation_fn=None, \\\\n-                weights_initializer=slim.initializers.xavier_initializer(), \\\\n-                weights_regularizer=slim.l2_regularizer(args.weight_decay),\\\\n-                scope=\\\\\\\'Logits\\\\\\\', reuse=False)\\\\n-\\\\n-        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\\\\\\\'embeddings\\\\\\\')\\\\n-\\\\n-        # Norm for the prelogits\\\\n-        eps = 1e-4\\\\n-        prelogits_norm = tf.reduce_mean(tf.norm(tf.abs(prelogits)+eps, ord=args.prelogits_norm_p, axis=1))\\\\n-        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_norm * args.prelogits_norm_loss_factor)\\\\n-\\\\n-        # Add center loss\\\\n-        prelogits_center_loss, _ = facenet.center_loss(prelogits, label_batch, args.center_loss_alfa, nrof_classes)\\\\n-        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_center_loss * args.center_loss_factor)\\\\n-\\\\n-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\\\\n-            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\\\\n-        tf.summary.scalar(\\\\\\\'learning_rate\\\\\\\', learning_rate)\\\\n-\\\\n-        # Calculate the average cross entropy loss across the batch\\\\n-        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\\\\n-            labels=label_batch, logits=logits, name=\\\\\\\'cross_entropy_per_example\\\\\\\')\\\\n-        cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\\\\\\\'cross_entropy\\\\\\\')\\\\n-        tf.add_to_collection(\\\\\\\'losses\\\\\\\', cross_entropy_mean)\\\\n-        \\\\n-        correct_prediction = tf.cast(tf.equal(tf.argmax(logits, 1), tf.cast(label_batch, tf.int64)), tf.float32)\\\\n-        accuracy = tf.reduce_mean(correct_prediction)\\\\n-        \\\\n-        # Calculate the total losses\\\\n-        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\\\\n-        total_loss = tf.add_n([cross_entropy_mean] + regularization_losses, name=\\\\\\\'total_loss\\\\\\\')\\\\n-\\\\n-        # Build a Graph that trains the model with one batch of examples and updates the model parameters\\\\n-        train_op = facenet.train(total_loss, global_step, args.optimizer, \\\\n-            learning_rate, args.moving_average_decay, tf.global_variables(), args.log_histograms)\\\\n-        \\\\n-        # Create a saver\\\\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\\\\n-\\\\n-        # Build the summary operation based on the TF collection of Summaries.\\\\n-        summary_op = tf.summary.merge_all()\\\\n-\\\\n-        # Start running operations on the Graph.\\\\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\\\\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\n-        sess.run(tf.global_variables_initializer())\\\\n-        sess.run(tf.local_variables_initializer())\\\\n-        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\\\\n-        coord = tf.train.Coordinator()\\\\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\\\\n-\\\\n-        with sess.as_default():\\\\n-\\\\n-            if pretrained_model:\\\\n-                print(\\\\\\\'Restoring pretrained model: %s\\\\\\\' % pretrained_model)\\\\n-                saver.restore(sess, pretrained_model)\\\\n-\\\\n-            # Training and validation loop\\\\n-            print(\\\\\\\'Running training\\\\\\\')\\\\n-            nrof_steps = args.max_nrof_epochs*args.epoch_size\\\\n-            nrof_val_samples = int(math.ceil(args.max_nrof_epochs / args.validate_every_n_epochs))   # Validate every validate_every_n_epochs as well as in the last epoch\\\\n-            stat = {\\\\n-                \\\\\\\'loss\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\n-                \\\\\\\'center_loss\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\n-                \\\\\\\'reg_loss\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\n-                \\\\\\\'xent_loss\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\n-                \\\\\\\'prelogits_norm\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\n-                \\\\\\\'accuracy\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\n-                \\\\\\\'val_loss\\\\\\\': np.zeros((nrof_val_samples,), np.float32),\\\\n-                \\\\\\\'val_xent_loss\\\\\\\': np.zeros((nrof_val_samples,), np.float32),\\\\n-                \\\\\\\'val_accuracy\\\\\\\': np.zeros((nrof_val_samples,), np.float32),\\\\n-                \\\\\\\'lfw_accuracy\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\n-                \\\\\\\'lfw_valrate\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\n-                \\\\\\\'learning_rate\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\n-                \\\\\\\'time_train\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\n-                \\\\\\\'time_validate\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\n-                \\\\\\\'time_evaluate\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\n-                \\\\\\\'prelogits_hist\\\\\\\': np.zeros((args.max_nrof_epochs, 1000), np.float32),\\\\n-              }\\\\n-            for epoch in range(1,args.max_nrof_epochs+1):\\\\n-                step = sess.run(global_step, feed_dict=None)\\\\n-                # Train for one epoch\\\\n-                t = time.time()\\\\n-                cont = train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder,\\\\n-                    learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, global_step, \\\\n-                    total_loss, train_op, summary_op, summary_writer, regularization_losses, args.learning_rate_schedule_file,\\\\n-                    stat, cross_entropy_mean, accuracy, learning_rate,\\\\n-                    prelogits, prelogits_center_loss, args.random_rotate, args.random_crop, args.random_flip, prelogits_norm, args.prelogits_hist_max, args.use_fixed_image_standardization)\\\\n-                stat[\\\\\\\'time_train\\\\\\\'][epoch-1] = time.time() - t\\\\n-                \\\\n-                if not cont:\\\\n-                    break\\\\n-                  \\\\n-                t = time.time()\\\\n-                if len(val_image_list)>0 and ((epoch-1) % args.validate_every_n_epochs == args.validate_every_n_epochs-1 or epoch==args.max_nrof_epochs):\\\\n-                    validate(args, sess, epoch, val_image_list, val_label_list, enqueue_op, image_paths_placeholder, labels_placeholder, control_placeholder,\\\\n-                        phase_train_placeholder, batch_size_placeholder, \\\\n-                        stat, total_loss, regularization_losses, cross_entropy_mean, accuracy, args.validate_every_n_epochs, args.use_fixed_image_standardization)\\\\n-                stat[\\\\\\\'time_validate\\\\\\\'][epoch-1] = time.time() - t\\\\n-\\\\n-                # Save variables and the metagraph if it doesn\\\\\\\'t exist already\\\\n-                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, epoch)\\\\n-\\\\n-                # Evaluate on LFW\\\\n-                t = time.time()\\\\n-                if args.lfw_dir:\\\\n-                    evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, \\\\n-                        embeddings, label_batch, lfw_paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, log_dir, step, summary_writer, stat, epoch, \\\\n-                        args.lfw_distance_metric, args.lfw_subtract_mean, args.lfw_use_flipped_images, args.use_fixed_image_standardization)\\\\n-                stat[\\\\\\\'time_evaluate\\\\\\\'][epoch-1] = time.time() - t\\\\n-\\\\n-                print(\\\\\\\'Saving statistics\\\\\\\')\\\\n-                with h5py.File(stat_file_name, \\\\\\\'w\\\\\\\') as f:\\\\n-                    for key, value in stat.iteritems():\\\\n-                        f.create_dataset(key, data=value)\\\\n-    \\\\n-    return model_dir\\\\n-  \\\\n-def find_threshold(var, percentile):\\\\n-    hist, bin_edges = np.histogram(var, 100)\\\\n-    cdf = np.float32(np.cumsum(hist)) / np.sum(hist)\\\\n-    bin_centers = (bin_edges[:-1]+bin_edges[1:])/2\\\\n-    #plt.plot(bin_centers, cdf)\\\\n-    threshold = np.interp(percentile*0.01, cdf, bin_centers)\\\\n-    return threshold\\\\n-  \\\\n-def filter_dataset(dataset, data_filename, percentile, min_nrof_images_per_class):\\\\n-    with h5py.File(data_filename,\\\\\\\'r\\\\\\\') as f:\\\\n-        distance_to_center = np.array(f.get(\\\\\\\'distance_to_center\\\\\\\'))\\\\n-        label_list = np.array(f.get(\\\\\\\'label_list\\\\\\\'))\\\\n-        image_list = np.array(f.get(\\\\\\\'image_list\\\\\\\'))\\\\n-        distance_to_center_threshold = find_threshold(distance_to_center, percentile)\\\\n-        indices = np.where(distance_to_center>=distance_to_center_threshold)[0]\\\\n-        filtered_dataset = dataset\\\\n-        removelist = []\\\\n-        for i in indices:\\\\n-            label = label_list[i]\\\\n-            image = image_list[i]\\\\n-            if image in filtered_dataset[label].image_paths:\\\\n-                filtered_dataset[label].image_paths.remove(image)\\\\n-            if len(filtered_dataset[label].image_paths)<min_nrof_images_per_class:\\\\n-                removelist.append(label)\\\\n-\\\\n-        ix = sorted(list(set(removelist)), reverse=True)\\\\n-        for i in ix:\\\\n-            del(filtered_dataset[i])\\\\n-\\\\n-    return filtered_dataset\\\\n-  \\\\n-def train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder, \\\\n-      learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, step, \\\\n-      loss, train_op, summary_op, summary_writer, reg_losses, learning_rate_schedule_file, \\\\n-      stat, cross_entropy_mean, accuracy, \\\\n-      learning_rate, prelogits, prelogits_center_loss, random_rotate, random_crop, random_flip, prelogits_norm, prelogits_hist_max, use_fixed_image_standardization):\\\\n-    batch_number = 0\\\\n-    \\\\n-    if args.learning_rate>0.0:\\\\n-        lr = args.learning_rate\\\\n-    else:\\\\n-        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\\\\n-        \\\\n-    if lr<=0:\\\\n-        return False \\\\n-\\\\n-    index_epoch = sess.run(index_dequeue_op)\\\\n-    label_epoch = np.array(label_list)[index_epoch]\\\\n-    image_epoch = np.array(image_list)[index_epoch]\\\\n-    \\\\n-    # Enqueue one epoch of image paths and labels\\\\n-    labels_array = np.expand_dims(np.array(label_epoch),1)\\\\n-    image_paths_array = np.expand_dims(np.array(image_epoch),1)\\\\n-    control_value = facenet.RANDOM_ROTATE * random_rotate + facenet.RANDOM_CROP * random_crop + facenet.RANDOM_FLIP * random_flip + facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization\\\\n-    control_array = np.ones_like(labels_array) * control_value\\\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\\\n-\\\\n-    # Training loop\\\\n-    train_time = 0\\\\n-    while batch_number < args.epoch_size:\\\\n-        start_time = time.time()\\\\n-        feed_dict = {learning_rate_placeholder: lr, phase_train_placeholder:True, batch_size_placeholder:args.batch_size}\\\\n-        tensor_list = [loss, train_op, step, reg_losses, prelogits, cross_entropy_mean, learning_rate, prelogits_norm, accuracy, prelogits_center_loss]\\\\n-        if batch_number % 100 == 0:\\\\n-            loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_, summary_str = sess.run(tensor_list + [summary_op], feed_dict=feed_dict)\\\\n-            summary_writer.add_summary(summary_str, global_step=step_)\\\\n-        else:\\\\n-            loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_ = sess.run(tensor_list, feed_dict=feed_dict)\\\\n-         \\\\n-        duration = time.time() - start_time\\\\n-        stat[\\\\\\\'loss\\\\\\\'][step_-1] = loss_\\\\n-        stat[\\\\\\\'center_loss\\\\\\\'][step_-1] = center_loss_\\\\n-        stat[\\\\\\\'reg_loss\\\\\\\'][step_-1] = np.sum(reg_losses_)\\\\n-        stat[\\\\\\\'xent_loss\\\\\\\'][step_-1] = cross_entropy_mean_\\\\n-        stat[\\\\\\\'prelogits_norm\\\\\\\'][step_-1] = prelogits_norm_\\\\n-        stat[\\\\\\\'learning_rate\\\\\\\'][epoch-1] = lr_\\\\n-        stat[\\\\\\\'accuracy\\\\\\\'][step_-1] = accuracy_\\\\n-        stat[\\\\\\\'prelogits_hist\\\\\\\'][epoch-1,:] += np.histogram(np.minimum(np.abs(prelogits_), prelogits_hist_max), bins=1000, range=(0.0, prelogits_hist_max))[0]\\\\n-        \\\\n-        duration = time.time() - start_time\\\\n-        print(\\\\\\\'Epoch: [%d][%d/%d]\\\\\\\\tTime %.3f\\\\\\\\tLoss %2.3f\\\\\\\\tXent %2.3f\\\\\\\\tRegLoss %2.3f\\\\\\\\tAccuracy %2.3f\\\\\\\\tLr %2.5f\\\\\\\\tCl %2.3f\\\\\\\' %\\\\n-              (epoch, batch_number+1, args.epoch_size, duration, loss_, cross_entropy_mean_, np.sum(reg_losses_), accuracy_, lr_, center_loss_))\\\\n-        batch_number += 1\\\\n-        train_time += duration\\\\n-    # Add validation loss and accuracy to summary\\\\n-    summary = tf.Summary()\\\\n-    #pylint: disable=maybe-no-member\\\\n-    summary.value.add(tag=\\\\\\\'time/total\\\\\\\', simple_value=train_time)\\\\n-    summary_writer.add_summary(summary, global_step=step_)\\\\n-    return True\\\\n-\\\\n-def validate(args, sess, epoch, image_list, label_list, enqueue_op, image_paths_placeholder, labels_placeholder, control_placeholder,\\\\n-             phase_train_placeholder, batch_size_placeholder, \\\\n-             stat, loss, regularization_losses, cross_entropy_mean, accuracy, validate_every_n_epochs, use_fixed_image_standardization):\\\\n-  \\\\n-    print(\\\\\\\'Running forward pass on validation set\\\\\\\')\\\\n-\\\\n-    nrof_batches = len(label_list) // args.lfw_batch_size\\\\n-    nrof_images = nrof_batches * args.lfw_batch_size\\\\n-    \\\\n-    # Enqueue one epoch of image paths and labels\\\\n-    labels_array = np.expand_dims(np.array(label_list[:nrof_images]),1)\\\\n-    image_paths_array = np.expand_dims(np.array(image_list[:nrof_images]),1)\\\\n-    control_array = np.ones_like(labels_array, np.int32)*facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization\\\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\\\n-\\\\n-    loss_array = np.zeros((nrof_batches,), np.float32)\\\\n-    xent_array = np.zeros((nrof_batches,), np.float32)\\\\n-    accuracy_array = np.zeros((nrof_batches,), np.float32)\\\\n-\\\\n-    # Training loop\\\\n-    start_time = time.time()\\\\n-    for i in range(nrof_batches):\\\\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:args.lfw_batch_size}\\\\n-        loss_, cross_entropy_mean_, accuracy_ = sess.run([loss, cross_entropy_mean, accuracy], feed_dict=feed_dict)\\\\n-        loss_array[i], xent_array[i], accuracy_array[i] = (loss_, cross_entropy_mean_, accuracy_)\\\\n-        if i % 10 == 9:\\\\n-            print(\\\\\\\'.\\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n-            sys.stdout.flush()\\\\n-    print(\\\\\\\'\\\\\\\')\\\\n-\\\\n-    duration = time.time() - start_time\\\\n-\\\\n-    val_index = (epoch-1)//validate_every_n_epochs\\\\n-    stat[\\\\\\\'val_loss\\\\\\\'][val_index] = np.mean(loss_array)\\\\n-    stat[\\\\\\\'val_xent_loss\\\\\\\'][val_index] = np.mean(xent_array)\\\\n-    stat[\\\\\\\'val_accuracy\\\\\\\'][val_index] = np.mean(accuracy_array)\\\\n-\\\\n-    print(\\\\\\\'Validation Epoch: %d\\\\\\\\tTime %.3f\\\\\\\\tLoss %2.3f\\\\\\\\tXent %2.3f\\\\\\\\tAccuracy %2.3f\\\\\\\' %\\\\n-          (epoch, duration, np.mean(loss_array), np.mean(xent_array), np.mean(accuracy_array)))\\\\n-\\\\n-\\\\n-def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, \\\\n-        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, log_dir, step, summary_writer, stat, epoch, distance_metric, subtract_mean, use_flipped_images, use_fixed_image_standardization):\\\\n-    start_time = time.time()\\\\n-    # Run forward pass to calculate embeddings\\\\n-    print(\\\\\\\'Runnning forward pass on LFW images\\\\\\\')\\\\n-    \\\\n-    # Enqueue one epoch of image paths and labels\\\\n-    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\\\\n-    nrof_flips = 2 if use_flipped_images else 1\\\\n-    nrof_images = nrof_embeddings * nrof_flips\\\\n-    labels_array = np.expand_dims(np.arange(0,nrof_images),1)\\\\n-    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\\\\n-    control_array = np.zeros_like(labels_array, np.int32)\\\\n-    if use_fixed_image_standardization:\\\\n-        control_array += np.ones_like(labels_array)*facenet.FIXED_STANDARDIZATION\\\\n-    if use_flipped_images:\\\\n-        # Flip every second image\\\\n-        control_array += (labels_array % 2)*facenet.FLIP\\\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\\\n-    \\\\n-    embedding_size = int(embeddings.get_shape()[1])\\\\n-    assert nrof_images % batch_size == 0, \\\\\\\'The number of LFW images must be an integer multiple of the LFW batch size\\\\\\\'\\\\n-    nrof_batches = nrof_images // batch_size\\\\n-    emb_array = np.zeros((nrof_images, embedding_size))\\\\n-    lab_array = np.zeros((nrof_images,))\\\\n-    for i in range(nrof_batches):\\\\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}\\\\n-        emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)\\\\n-        lab_array[lab] = lab\\\\n-        emb_array[lab, :] = emb\\\\n-        if i % 10 == 9:\\\\n-            print(\\\\\\\'.\\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n-            sys.stdout.flush()\\\\n-    print(\\\\\\\'\\\\\\\')\\\\n-    embeddings = np.zeros((nrof_embeddings, embedding_size*nrof_flips))\\\\n-    if use_flipped_images:\\\\n-        # Concatenate embeddings for flipped and non flipped version of the images\\\\n-        embeddings[:,:embedding_size] = emb_array[0::2,:]\\\\n-        embeddings[:,embedding_size:] = emb_array[1::2,:]\\\\n-    else:\\\\n-        embeddings = emb_array\\\\n-\\\\n-    assert np.array_equal(lab_array, np.arange(nrof_images))==True, \\\\\\\'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline\\\\\\\'\\\\n-    _, _, accuracy, val, val_std, far = lfw.evaluate(embeddings, actual_issame, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\\\\n-    \\\\n-    print(\\\\\\\'Accuracy: %2.5f+-%2.5f\\\\\\\' % (np.mean(accuracy), np.std(accuracy)))\\\\n-    print(\\\\\\\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\\\\\\\' % (val, val_std, far))\\\\n-    lfw_time = time.time() - start_time\\\\n-    # Add validation loss and accuracy to summary\\\\n-    summary = tf.Summary()\\\\n-    #pylint: disable=maybe-no-member\\\\n-    summary.value.add(tag=\\\\\\\'lfw/accuracy\\\\\\\', simple_value=np.mean(accuracy))\\\\n-    summary.value.add(tag=\\\\\\\'lfw/val_rate\\\\\\\', simple_value=val)\\\\n-    summary.value.add(tag=\\\\\\\'time/lfw\\\\\\\', simple_value=lfw_time)\\\\n-    summary_writer.add_summary(summary, step)\\\\n-    with open(os.path.join(log_dir,\\\\\\\'lfw_result.txt\\\\\\\'),\\\\\\\'at\\\\\\\') as f:\\\\n-        f.write(\\\\\\\'%d\\\\\\\\t%.5f\\\\\\\\t%.5f\\\\\\\\n\\\\\\\' % (step, np.mean(accuracy), val))\\\\n-    stat[\\\\\\\'lfw_accuracy\\\\\\\'][epoch-1] = np.mean(accuracy)\\\\n-    stat[\\\\\\\'lfw_valrate\\\\\\\'][epoch-1] = val\\\\n-\\\\n-def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\\\\n-    # Save the model checkpoint\\\\n-    print(\\\\\\\'Saving variables\\\\\\\')\\\\n-    start_time = time.time()\\\\n-    checkpoint_path = os.path.join(model_dir, \\\\\\\'model-%s.ckpt\\\\\\\' % model_name)\\\\n-    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\\\\n-    save_time_variables = time.time() - start_time\\\\n-    print(\\\\\\\'Variables saved in %.2f seconds\\\\\\\' % save_time_variables)\\\\n-    metagraph_filename = os.path.join(model_dir, \\\\\\\'model-%s.meta\\\\\\\' % model_name)\\\\n-    save_time_metagraph = 0  \\\\n-    if not os.path.exists(metagraph_filename):\\\\n-        print(\\\\\\\'Saving metagraph\\\\\\\')\\\\n-        start_time = time.time()\\\\n-        saver.export_meta_graph(metagraph_filename)\\\\n-        save_time_metagraph = time.time() - start_time\\\\n-        print(\\\\\\\'Metagraph saved in %.2f seconds\\\\\\\' % save_time_metagraph)\\\\n-    summary = tf.Summary()\\\\n-    #pylint: disable=maybe-no-member\\\\n-    summary.value.add(tag=\\\\\\\'time/save_variables\\\\\\\', simple_value=save_time_variables)\\\\n-    summary.value.add(tag=\\\\\\\'time/save_metagraph\\\\\\\', simple_value=save_time_metagraph)\\\\n-    summary_writer.add_summary(summary, step)\\\\n-  \\\\n-\\\\n-def parse_arguments(argv):\\\\n-    parser = argparse.ArgumentParser()\\\\n-    \\\\n-    parser.add_argument(\\\\\\\'--logs_base_dir\\\\\\\', type=str, \\\\n-        help=\\\\\\\'Directory where to write event logs.\\\\\\\', default=\\\\\\\'~/logs/facenet\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--models_base_dir\\\\\\\', type=str,\\\\n-        help=\\\\\\\'Directory where to write trained models and checkpoints.\\\\\\\', default=\\\\\\\'~/models/facenet\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--gpu_memory_fraction\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Upper bound on the amount of GPU memory that will be used by the process.\\\\\\\', default=1.0)\\\\n-    parser.add_argument(\\\\\\\'--pretrained_model\\\\\\\', type=str,\\\\n-        help=\\\\\\\'Load a pretrained model before training starts.\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--data_dir\\\\\\\', type=str,\\\\n-        help=\\\\\\\'Path to the data directory containing aligned face patches.\\\\\\\',\\\\n-        default=\\\\\\\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--model_def\\\\\\\', type=str,\\\\n-        help=\\\\\\\'Model definition. Points to a module containing the definition of the inference graph.\\\\\\\', default=\\\\\\\'models.inception_resnet_v1\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--max_nrof_epochs\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of epochs to run.\\\\\\\', default=500)\\\\n-    parser.add_argument(\\\\\\\'--batch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=90)\\\\n-    parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=160)\\\\n-    parser.add_argument(\\\\\\\'--epoch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of batches per epoch.\\\\\\\', default=1000)\\\\n-    parser.add_argument(\\\\\\\'--embedding_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Dimensionality of the embedding.\\\\\\\', default=128)\\\\n-    parser.add_argument(\\\\\\\'--random_crop\\\\\\\', \\\\n-        help=\\\\\\\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \\\\\\\' +\\\\n-         \\\\\\\'If the size of the images in the data directory is equal to image_size no cropping is performed\\\\\\\', action=\\\\\\\'store_true\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--random_flip\\\\\\\', \\\\n-        help=\\\\\\\'Performs random horizontal flipping of training images.\\\\\\\', action=\\\\\\\'store_true\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--random_rotate\\\\\\\', \\\\n-        help=\\\\\\\'Performs random rotations of training images.\\\\\\\', action=\\\\\\\'store_true\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--use_fixed_image_standardization\\\\\\\', \\\\n-        help=\\\\\\\'Performs fixed standardization of images.\\\\\\\', action=\\\\\\\'store_true\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--keep_probability\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Keep probability of dropout for the fully connected layer(s).\\\\\\\', default=1.0)\\\\n-    parser.add_argument(\\\\\\\'--weight_decay\\\\\\\', type=float,\\\\n-        help=\\\\\\\'L2 weight regularization.\\\\\\\', default=0.0)\\\\n-    parser.add_argument(\\\\\\\'--center_loss_factor\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Center loss factor.\\\\\\\', default=0.0)\\\\n-    parser.add_argument(\\\\\\\'--center_loss_alfa\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Center update rate for center loss.\\\\\\\', default=0.95)\\\\n-    parser.add_argument(\\\\\\\'--prelogits_norm_loss_factor\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Loss based on the norm of the activations in the prelogits layer.\\\\\\\', default=0.0)\\\\n-    parser.add_argument(\\\\\\\'--prelogits_norm_p\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Norm to use for prelogits norm loss.\\\\\\\', default=1.0)\\\\n-    parser.add_argument(\\\\\\\'--prelogits_hist_max\\\\\\\', type=float,\\\\n-        help=\\\\\\\'The max value for the prelogits histogram.\\\\\\\', default=10.0)\\\\n-    parser.add_argument(\\\\\\\'--optimizer\\\\\\\', type=str, choices=[\\\\\\\'ADAGRAD\\\\\\\', \\\\\\\'ADADELTA\\\\\\\', \\\\\\\'ADAM\\\\\\\', \\\\\\\'RMSPROP\\\\\\\', \\\\\\\'MOM\\\\\\\'],\\\\n-        help=\\\\\\\'The optimization algorithm to use\\\\\\\', default=\\\\\\\'ADAGRAD\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--learning_rate\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Initial learning rate. If set to a negative value a learning rate \\\\\\\' +\\\\n-        \\\\\\\'schedule can be specified in the file "learning_rate_schedule.txt"\\\\\\\', default=0.1)\\\\n-    parser.add_argument(\\\\\\\'--learning_rate_decay_epochs\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of epochs between learning rate decay.\\\\\\\', default=100)\\\\n-    parser.add_argument(\\\\\\\'--learning_rate_decay_factor\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Learning rate decay factor.\\\\\\\', default=1.0)\\\\n-    parser.add_argument(\\\\\\\'--moving_average_decay\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Exponential decay for tracking of training parameters.\\\\\\\', default=0.9999)\\\\n-    parser.add_argument(\\\\\\\'--seed\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Random seed.\\\\\\\', default=666)\\\\n-    parser.add_argument(\\\\\\\'--nrof_preprocess_threads\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of preprocessing (data loading and augmentation) threads.\\\\\\\', default=4)\\\\n-    parser.add_argument(\\\\\\\'--log_histograms\\\\\\\', \\\\n-        help=\\\\\\\'Enables logging of weight/bias histograms in tensorboard.\\\\\\\', action=\\\\\\\'store_true\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--learning_rate_schedule_file\\\\\\\', type=str,\\\\n-        help=\\\\\\\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\\\\\\\', default=\\\\\\\'data/learning_rate_schedule.txt\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--filter_filename\\\\\\\', type=str,\\\\n-        help=\\\\\\\'File containing image data used for dataset filtering\\\\\\\', default=\\\\\\\'\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--filter_percentile\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Keep only the percentile images closed to its class center\\\\\\\', default=100.0)\\\\n-    parser.add_argument(\\\\\\\'--filter_min_nrof_images_per_class\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Keep only the classes with this number of examples or more\\\\\\\', default=0)\\\\n-    parser.add_argument(\\\\\\\'--validate_every_n_epochs\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of epoch between validation\\\\\\\', default=5)\\\\n-    parser.add_argument(\\\\\\\'--validation_set_split_ratio\\\\\\\', type=float,\\\\n-        help=\\\\\\\'The ratio of the total dataset to use for validation\\\\\\\', default=0.0)\\\\n-    parser.add_argument(\\\\\\\'--min_nrof_val_images_per_class\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Classes with fewer images will be removed from the validation set\\\\\\\', default=0)\\\\n- \\\\n-    # Parameters for validation on LFW\\\\n-    parser.add_argument(\\\\\\\'--lfw_pairs\\\\\\\', type=str,\\\\n-        help=\\\\\\\'The file containing the pairs to use for validation.\\\\\\\', default=\\\\\\\'data/pairs.txt\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--lfw_dir\\\\\\\', type=str,\\\\n-        help=\\\\\\\'Path to the data directory containing aligned face patches.\\\\\\\', default=\\\\\\\'\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--lfw_batch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images to process in a batch in the LFW test set.\\\\\\\', default=100)\\\\n-    parser.add_argument(\\\\\\\'--lfw_nrof_folds\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of folds to use for cross validation. Mainly used for testing.\\\\\\\', default=10)\\\\n-    parser.add_argument(\\\\\\\'--lfw_distance_metric\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Type of distance metric to use. 0: Euclidian, 1:Cosine similarity distance.\\\\\\\', default=0)\\\\n-    parser.add_argument(\\\\\\\'--lfw_use_flipped_images\\\\\\\', \\\\n-        help=\\\\\\\'Concatenates embeddings for the image and its horizontally flipped counterpart.\\\\\\\', action=\\\\\\\'store_true\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--lfw_subtract_mean\\\\\\\', \\\\n-        help=\\\\\\\'Subtract feature mean before calculating distance.\\\\\\\', action=\\\\\\\'store_true\\\\\\\')\\\\n-    return parser.parse_args(argv)\\\\n-  \\\\n-\\\\n-if __name__ == \\\\\\\'__main__\\\\\\\':\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\ndiff --git a/src/train_tripletloss.py b/src/train_tripletloss.py\\\\ndeleted file mode 100644\\\\nindex d6df19a..0000000\\\\n--- a/src/train_tripletloss.py\\\\n+++ /dev/null\\\\n@@ -1,486 +0,0 @@\\\\n-"""Training a face recognizer with TensorFlow based on the FaceNet paper\\\\n-FaceNet: A Unified Embedding for Face Recognition and Clustering: http://arxiv.org/abs/1503.03832\\\\n-"""\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-from datetime import datetime\\\\n-import os.path\\\\n-import time\\\\n-import sys\\\\n-import tensorflow as tf\\\\n-import numpy as np\\\\n-import importlib\\\\n-import itertools\\\\n-import argparse\\\\n-import facenet\\\\n-import lfw\\\\n-\\\\n-from tensorflow.python.ops import data_flow_ops\\\\n-\\\\n-from six.moves import xrange  # @UnresolvedImport\\\\n-\\\\n-def main(args):\\\\n-  \\\\n-    network = importlib.import_module(args.model_def)\\\\n-\\\\n-    subdir = datetime.strftime(datetime.now(), \\\\\\\'%Y%m%d-%H%M%S\\\\\\\')\\\\n-    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\\\\n-    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\\\\\\\'t exist\\\\n-        os.makedirs(log_dir)\\\\n-    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\\\\n-    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\\\\\\\'t exist\\\\n-        os.makedirs(model_dir)\\\\n-\\\\n-    # Write arguments to a text file\\\\n-    facenet.write_arguments_to_file(args, os.path.join(log_dir, \\\\\\\'arguments.txt\\\\\\\'))\\\\n-        \\\\n-    # Store some git revision info in a text file in the log directory\\\\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\\\\n-    facenet.store_revision_info(src_path, log_dir, \\\\\\\' \\\\\\\'.join(sys.argv))\\\\n-\\\\n-    np.random.seed(seed=args.seed)\\\\n-    train_set = facenet.get_dataset(args.data_dir)\\\\n-    \\\\n-    print(\\\\\\\'Model directory: %s\\\\\\\' % model_dir)\\\\n-    print(\\\\\\\'Log directory: %s\\\\\\\' % log_dir)\\\\n-    if args.pretrained_model:\\\\n-        print(\\\\\\\'Pre-trained model: %s\\\\\\\' % os.path.expanduser(args.pretrained_model))\\\\n-    \\\\n-    if args.lfw_dir:\\\\n-        print(\\\\\\\'LFW directory: %s\\\\\\\' % args.lfw_dir)\\\\n-        # Read the file containing the pairs used for testing\\\\n-        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\\\\n-        # Get the paths for the corresponding images\\\\n-        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\\\\n-        \\\\n-    \\\\n-    with tf.Graph().as_default():\\\\n-        tf.set_random_seed(args.seed)\\\\n-        global_step = tf.Variable(0, trainable=False)\\\\n-\\\\n-        # Placeholder for the learning rate\\\\n-        learning_rate_placeholder = tf.placeholder(tf.float32, name=\\\\\\\'learning_rate\\\\\\\')\\\\n-        \\\\n-        batch_size_placeholder = tf.placeholder(tf.int32, name=\\\\\\\'batch_size\\\\\\\')\\\\n-        \\\\n-        phase_train_placeholder = tf.placeholder(tf.bool, name=\\\\\\\'phase_train\\\\\\\')\\\\n-        \\\\n-        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,3), name=\\\\\\\'image_paths\\\\\\\')\\\\n-        labels_placeholder = tf.placeholder(tf.int64, shape=(None,3), name=\\\\\\\'labels\\\\\\\')\\\\n-        \\\\n-        input_queue = data_flow_ops.FIFOQueue(capacity=100000,\\\\n-                                    dtypes=[tf.string, tf.int64],\\\\n-                                    shapes=[(3,), (3,)],\\\\n-                                    shared_name=None, name=None)\\\\n-        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder])\\\\n-        \\\\n-        nrof_preprocess_threads = 4\\\\n-        images_and_labels = []\\\\n-        for _ in range(nrof_preprocess_threads):\\\\n-            filenames, label = input_queue.dequeue()\\\\n-            images = []\\\\n-            for filename in tf.unstack(filenames):\\\\n-                file_contents = tf.read_file(filename)\\\\n-                image = tf.image.decode_image(file_contents, channels=3)\\\\n-                \\\\n-                if args.random_crop:\\\\n-                    image = tf.random_crop(image, [args.image_size, args.image_size, 3])\\\\n-                else:\\\\n-                    image = tf.image.resize_image_with_crop_or_pad(image, args.image_size, args.image_size)\\\\n-                if args.random_flip:\\\\n-                    image = tf.image.random_flip_left_right(image)\\\\n-    \\\\n-                #pylint: disable=no-member\\\\n-                image.set_shape((args.image_size, args.image_size, 3))\\\\n-                images.append(tf.image.per_image_standardization(image))\\\\n-            images_and_labels.append([images, label])\\\\n-    \\\\n-        image_batch, labels_batch = tf.train.batch_join(\\\\n-            images_and_labels, batch_size=batch_size_placeholder, \\\\n-            shapes=[(args.image_size, args.image_size, 3), ()], enqueue_many=True,\\\\n-            capacity=4 * nrof_preprocess_threads * args.batch_size,\\\\n-            allow_smaller_final_batch=True)\\\\n-        image_batch = tf.identity(image_batch, \\\\\\\'image_batch\\\\\\\')\\\\n-        image_batch = tf.identity(image_batch, \\\\\\\'input\\\\\\\')\\\\n-        labels_batch = tf.identity(labels_batch, \\\\\\\'label_batch\\\\\\\')\\\\n-\\\\n-        # Build the inference graph\\\\n-        prelogits, _ = network.inference(image_batch, args.keep_probability, \\\\n-            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size,\\\\n-            weight_decay=args.weight_decay)\\\\n-        \\\\n-        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\\\\\\\'embeddings\\\\\\\')\\\\n-        # Split embeddings into anchor, positive and negative and calculate triplet loss\\\\n-        anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\\\\n-        triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\\\\n-        \\\\n-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\\\\n-            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\\\\n-        tf.summary.scalar(\\\\\\\'learning_rate\\\\\\\', learning_rate)\\\\n-\\\\n-        # Calculate the total losses\\\\n-        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\\\\n-        total_loss = tf.add_n([triplet_loss] + regularization_losses, name=\\\\\\\'total_loss\\\\\\\')\\\\n-\\\\n-        # Build a Graph that trains the model with one batch of examples and updates the model parameters\\\\n-        train_op = facenet.train(total_loss, global_step, args.optimizer, \\\\n-            learning_rate, args.moving_average_decay, tf.global_variables())\\\\n-        \\\\n-        # Create a saver\\\\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\\\\n-\\\\n-        # Build the summary operation based on the TF collection of Summaries.\\\\n-        summary_op = tf.summary.merge_all()\\\\n-\\\\n-        # Start running operations on the Graph.\\\\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\\\\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))        \\\\n-\\\\n-        # Initialize variables\\\\n-        sess.run(tf.global_variables_initializer(), feed_dict={phase_train_placeholder:True})\\\\n-        sess.run(tf.local_variables_initializer(), feed_dict={phase_train_placeholder:True})\\\\n-\\\\n-        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\\\\n-        coord = tf.train.Coordinator()\\\\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\\\\n-\\\\n-        with sess.as_default():\\\\n-\\\\n-            if args.pretrained_model:\\\\n-                print(\\\\\\\'Restoring pretrained model: %s\\\\\\\' % args.pretrained_model)\\\\n-                saver.restore(sess, os.path.expanduser(args.pretrained_model))\\\\n-\\\\n-            # Training and validation loop\\\\n-            epoch = 0\\\\n-            while epoch < args.max_nrof_epochs:\\\\n-                step = sess.run(global_step, feed_dict=None)\\\\n-                epoch = step // args.epoch_size\\\\n-                # Train for one epoch\\\\n-                train(args, sess, train_set, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\\\\n-                    batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \\\\n-                    embeddings, total_loss, train_op, summary_op, summary_writer, args.learning_rate_schedule_file,\\\\n-                    args.embedding_size, anchor, positive, negative, triplet_loss)\\\\n-\\\\n-                # Save variables and the metagraph if it doesn\\\\\\\'t exist already\\\\n-                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, step)\\\\n-\\\\n-                # Evaluate on LFW\\\\n-                if args.lfw_dir:\\\\n-                    evaluate(sess, lfw_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \\\\n-                            batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, args.batch_size, \\\\n-                            args.lfw_nrof_folds, log_dir, step, summary_writer, args.embedding_size)\\\\n-\\\\n-    return model_dir\\\\n-\\\\n-\\\\n-def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\\\\n-          batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \\\\n-          embeddings, loss, train_op, summary_op, summary_writer, learning_rate_schedule_file,\\\\n-          embedding_size, anchor, positive, negative, triplet_loss):\\\\n-    batch_number = 0\\\\n-    \\\\n-    if args.learning_rate>0.0:\\\\n-        lr = args.learning_rate\\\\n-    else:\\\\n-        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\\\\n-    while batch_number < args.epoch_size:\\\\n-        # Sample people randomly from the dataset\\\\n-        image_paths, num_per_class = sample_people(dataset, args.people_per_batch, args.images_per_person)\\\\n-        \\\\n-        print(\\\\\\\'Running forward pass on sampled images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n-        start_time = time.time()\\\\n-        nrof_examples = args.people_per_batch * args.images_per_person\\\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\n-        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n-        sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n-        emb_array = np.zeros((nrof_examples, embedding_size))\\\\n-        nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\\n-        for i in range(nrof_batches):\\\\n-            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\\\\n-            emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size, \\\\n-                learning_rate_placeholder: lr, phase_train_placeholder: True})\\\\n-            emb_array[lab,:] = emb\\\\n-        print(\\\\\\\'%.3f\\\\\\\' % (time.time()-start_time))\\\\n-\\\\n-        # Select triplets based on the embeddings\\\\n-        print(\\\\\\\'Selecting suitable triplets for training\\\\\\\')\\\\n-        triplets, nrof_random_negs, nrof_triplets = select_triplets(emb_array, num_per_class, \\\\n-            image_paths, args.people_per_batch, args.alpha)\\\\n-        selection_time = time.time() - start_time\\\\n-        print(\\\\\\\'(nrof_random_negs, nrof_triplets) = (%d, %d): time=%.3f seconds\\\\\\\' % \\\\n-            (nrof_random_negs, nrof_triplets, selection_time))\\\\n-\\\\n-        # Perform training on the selected triplets\\\\n-        nrof_batches = int(np.ceil(nrof_triplets*3/args.batch_size))\\\\n-        triplet_paths = list(itertools.chain(*triplets))\\\\n-        labels_array = np.reshape(np.arange(len(triplet_paths)),(-1,3))\\\\n-        triplet_paths_array = np.reshape(np.expand_dims(np.array(triplet_paths),1), (-1,3))\\\\n-        sess.run(enqueue_op, {image_paths_placeholder: triplet_paths_array, labels_placeholder: labels_array})\\\\n-        nrof_examples = len(triplet_paths)\\\\n-        train_time = 0\\\\n-        i = 0\\\\n-        emb_array = np.zeros((nrof_examples, embedding_size))\\\\n-        loss_array = np.zeros((nrof_triplets,))\\\\n-        summary = tf.Summary()\\\\n-        step = 0\\\\n-        while i < nrof_batches:\\\\n-            start_time = time.time()\\\\n-            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\\\\n-            feed_dict = {batch_size_placeholder: batch_size, learning_rate_placeholder: lr, phase_train_placeholder: True}\\\\n-            err, _, step, emb, lab = sess.run([loss, train_op, global_step, embeddings, labels_batch], feed_dict=feed_dict)\\\\n-            emb_array[lab,:] = emb\\\\n-            loss_array[i] = err\\\\n-            duration = time.time() - start_time\\\\n-            print(\\\\\\\'Epoch: [%d][%d/%d]\\\\\\\\tTime %.3f\\\\\\\\tLoss %2.3f\\\\\\\' %\\\\n-                  (epoch, batch_number+1, args.epoch_size, duration, err))\\\\n-            batch_number += 1\\\\n-            i += 1\\\\n-            train_time += duration\\\\n-            summary.value.add(tag=\\\\\\\'loss\\\\\\\', simple_value=err)\\\\n-            \\\\n-        # Add validation loss and accuracy to summary\\\\n-        #pylint: disable=maybe-no-member\\\\n-        summary.value.add(tag=\\\\\\\'time/selection\\\\\\\', simple_value=selection_time)\\\\n-        summary_writer.add_summary(summary, step)\\\\n-    return step\\\\n-  \\\\n-def select_triplets(embeddings, nrof_images_per_class, image_paths, people_per_batch, alpha):\\\\n-    """ Select the triplets for training\\\\n-    """\\\\n-    trip_idx = 0\\\\n-    emb_start_idx = 0\\\\n-    num_trips = 0\\\\n-    triplets = []\\\\n-    \\\\n-    # VGG Face: Choosing good triplets is crucial and should strike a balance between\\\\n-    #  selecting informative (i.e. challenging) examples and swamping training with examples that\\\\n-    #  are too hard. This is achieve by extending each pair (a, p) to a triplet (a, p, n) by sampling\\\\n-    #  the image n at random, but only between the ones that violate the triplet loss margin. The\\\\n-    #  latter is a form of hard-negative mining, but it is not as aggressive (and much cheaper) than\\\\n-    #  choosing the maximally violating example, as often done in structured output learning.\\\\n-\\\\n-    for i in xrange(people_per_batch):\\\\n-        nrof_images = int(nrof_images_per_class[i])\\\\n-        for j in xrange(1,nrof_images):\\\\n-            a_idx = emb_start_idx + j - 1\\\\n-            neg_dists_sqr = np.sum(np.square(embeddings[a_idx] - embeddings), 1)\\\\n-            for pair in xrange(j, nrof_images): # For every possible positive pair.\\\\n-                p_idx = emb_start_idx + pair\\\\n-                pos_dist_sqr = np.sum(np.square(embeddings[a_idx]-embeddings[p_idx]))\\\\n-                neg_dists_sqr[emb_start_idx:emb_start_idx+nrof_images] = np.NaN\\\\n-                #all_neg = np.where(np.logical_and(neg_dists_sqr-pos_dist_sqr<alpha, pos_dist_sqr<neg_dists_sqr))[0]  # FaceNet selection\\\\n-                all_neg = np.where(neg_dists_sqr-pos_dist_sqr<alpha)[0] # VGG Face selecction\\\\n-                nrof_random_negs = all_neg.shape[0]\\\\n-                if nrof_random_negs>0:\\\\n-                    rnd_idx = np.random.randint(nrof_random_negs)\\\\n-                    n_idx = all_neg[rnd_idx]\\\\n-                    triplets.append((image_paths[a_idx], image_paths[p_idx], image_paths[n_idx]))\\\\n-                    #print(\\\\\\\'Triplet %d: (%d, %d, %d), pos_dist=%2.6f, neg_dist=%2.6f (%d, %d, %d, %d, %d)\\\\\\\' % \\\\n-                    #    (trip_idx, a_idx, p_idx, n_idx, pos_dist_sqr, neg_dists_sqr[n_idx], nrof_random_negs, rnd_idx, i, j, emb_start_idx))\\\\n-                    trip_idx += 1\\\\n-\\\\n-                num_trips += 1\\\\n-\\\\n-        emb_start_idx += nrof_images\\\\n-\\\\n-    np.random.shuffle(triplets)\\\\n-    return triplets, num_trips, len(triplets)\\\\n-\\\\n-def sample_people(dataset, people_per_batch, images_per_person):\\\\n-    nrof_images = people_per_batch * images_per_person\\\\n-  \\\\n-    # Sample classes from the dataset\\\\n-    nrof_classes = len(dataset)\\\\n-    class_indices = np.arange(nrof_classes)\\\\n-    np.random.shuffle(class_indices)\\\\n-    \\\\n-    i = 0\\\\n-    image_paths = []\\\\n-    num_per_class = []\\\\n-    sampled_class_indices = []\\\\n-    # Sample images from these classes until we have enough\\\\n-    while len(image_paths)<nrof_images:\\\\n-        class_index = class_indices[i]\\\\n-        nrof_images_in_class = len(dataset[class_index])\\\\n-        image_indices = np.arange(nrof_images_in_class)\\\\n-        np.random.shuffle(image_indices)\\\\n-        nrof_images_from_class = min(nrof_images_in_class, images_per_person, nrof_images-len(image_paths))\\\\n-        idx = image_indices[0:nrof_images_from_class]\\\\n-        image_paths_for_class = [dataset[class_index].image_paths[j] for j in idx]\\\\n-        sampled_class_indices += [class_index]*nrof_images_from_class\\\\n-        image_paths += image_paths_for_class\\\\n-        num_per_class.append(nrof_images_from_class)\\\\n-        i+=1\\\\n-  \\\\n-    return image_paths, num_per_class\\\\n-\\\\n-def evaluate(sess, image_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \\\\n-        batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, batch_size, \\\\n-        nrof_folds, log_dir, step, summary_writer, embedding_size):\\\\n-    start_time = time.time()\\\\n-    # Run forward pass to calculate embeddings\\\\n-    print(\\\\\\\'Running forward pass on LFW images: \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n-    \\\\n-    nrof_images = len(actual_issame)*2\\\\n-    assert(len(image_paths)==nrof_images)\\\\n-    labels_array = np.reshape(np.arange(nrof_images),(-1,3))\\\\n-    image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\n-    emb_array = np.zeros((nrof_images, embedding_size))\\\\n-    nrof_batches = int(np.ceil(nrof_images / batch_size))\\\\n-    label_check_array = np.zeros((nrof_images,))\\\\n-    for i in xrange(nrof_batches):\\\\n-        batch_size = min(nrof_images-i*batch_size, batch_size)\\\\n-        emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size,\\\\n-            learning_rate_placeholder: 0.0, phase_train_placeholder: False})\\\\n-        emb_array[lab,:] = emb\\\\n-        label_check_array[lab] = 1\\\\n-    print(\\\\\\\'%.3f\\\\\\\' % (time.time()-start_time))\\\\n-    \\\\n-    assert(np.all(label_check_array==1))\\\\n-    \\\\n-    _, _, accuracy, val, val_std, far = lfw.evaluate(emb_array, actual_issame, nrof_folds=nrof_folds)\\\\n-    \\\\n-    print(\\\\\\\'Accuracy: %1.3f+-%1.3f\\\\\\\' % (np.mean(accuracy), np.std(accuracy)))\\\\n-    print(\\\\\\\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\\\\\\\' % (val, val_std, far))\\\\n-    lfw_time = time.time() - start_time\\\\n-    # Add validation loss and accuracy to summary\\\\n-    summary = tf.Summary()\\\\n-    #pylint: disable=maybe-no-member\\\\n-    summary.value.add(tag=\\\\\\\'lfw/accuracy\\\\\\\', simple_value=np.mean(accuracy))\\\\n-    summary.value.add(tag=\\\\\\\'lfw/val_rate\\\\\\\', simple_value=val)\\\\n-    summary.value.add(tag=\\\\\\\'time/lfw\\\\\\\', simple_value=lfw_time)\\\\n-    summary_writer.add_summary(summary, step)\\\\n-    with open(os.path.join(log_dir,\\\\\\\'lfw_result.txt\\\\\\\'),\\\\\\\'at\\\\\\\') as f:\\\\n-        f.write(\\\\\\\'%d\\\\\\\\t%.5f\\\\\\\\t%.5f\\\\\\\\n\\\\\\\' % (step, np.mean(accuracy), val))\\\\n-\\\\n-def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\\\\n-    # Save the model checkpoint\\\\n-    print(\\\\\\\'Saving variables\\\\\\\')\\\\n-    start_time = time.time()\\\\n-    checkpoint_path = os.path.join(model_dir, \\\\\\\'model-%s.ckpt\\\\\\\' % model_name)\\\\n-    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\\\\n-    save_time_variables = time.time() - start_time\\\\n-    print(\\\\\\\'Variables saved in %.2f seconds\\\\\\\' % save_time_variables)\\\\n-    metagraph_filename = os.path.join(model_dir, \\\\\\\'model-%s.meta\\\\\\\' % model_name)\\\\n-    save_time_metagraph = 0  \\\\n-    if not os.path.exists(metagraph_filename):\\\\n-        print(\\\\\\\'Saving metagraph\\\\\\\')\\\\n-        start_time = time.time()\\\\n-        saver.export_meta_graph(metagraph_filename)\\\\n-        save_time_metagraph = time.time() - start_time\\\\n-        print(\\\\\\\'Metagraph saved in %.2f seconds\\\\\\\' % save_time_metagraph)\\\\n-    summary = tf.Summary()\\\\n-    #pylint: disable=maybe-no-member\\\\n-    summary.value.add(tag=\\\\\\\'time/save_variables\\\\\\\', simple_value=save_time_variables)\\\\n-    summary.value.add(tag=\\\\\\\'time/save_metagraph\\\\\\\', simple_value=save_time_metagraph)\\\\n-    summary_writer.add_summary(summary, step)\\\\n-  \\\\n-  \\\\n-def get_learning_rate_from_file(filename, epoch):\\\\n-    with open(filename, \\\\\\\'r\\\\\\\') as f:\\\\n-        for line in f.readlines():\\\\n-            line = line.split(\\\\\\\'#\\\\\\\', 1)[0]\\\\n-            if line:\\\\n-                par = line.strip().split(\\\\\\\':\\\\\\\')\\\\n-                e = int(par[0])\\\\n-                lr = float(par[1])\\\\n-                if e <= epoch:\\\\n-                    learning_rate = lr\\\\n-                else:\\\\n-                    return learning_rate\\\\n-    \\\\n-\\\\n-def parse_arguments(argv):\\\\n-    parser = argparse.ArgumentParser()\\\\n-    \\\\n-    parser.add_argument(\\\\\\\'--logs_base_dir\\\\\\\', type=str, \\\\n-        help=\\\\\\\'Directory where to write event logs.\\\\\\\', default=\\\\\\\'~/logs/facenet\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--models_base_dir\\\\\\\', type=str,\\\\n-        help=\\\\\\\'Directory where to write trained models and checkpoints.\\\\\\\', default=\\\\\\\'~/models/facenet\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--gpu_memory_fraction\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Upper bound on the amount of GPU memory that will be used by the process.\\\\\\\', default=1.0)\\\\n-    parser.add_argument(\\\\\\\'--pretrained_model\\\\\\\', type=str,\\\\n-        help=\\\\\\\'Load a pretrained model before training starts.\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--data_dir\\\\\\\', type=str,\\\\n-        help=\\\\\\\'Path to the data directory containing aligned face patches.\\\\\\\',\\\\n-        default=\\\\\\\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--model_def\\\\\\\', type=str,\\\\n-        help=\\\\\\\'Model definition. Points to a module containing the definition of the inference graph.\\\\\\\', default=\\\\\\\'models.inception_resnet_v1\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--max_nrof_epochs\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of epochs to run.\\\\\\\', default=500)\\\\n-    parser.add_argument(\\\\\\\'--batch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images to process in a batch.\\\\\\\', default=90)\\\\n-    parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=160)\\\\n-    parser.add_argument(\\\\\\\'--people_per_batch\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of people per batch.\\\\\\\', default=45)\\\\n-    parser.add_argument(\\\\\\\'--images_per_person\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images per person.\\\\\\\', default=40)\\\\n-    parser.add_argument(\\\\\\\'--epoch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of batches per epoch.\\\\\\\', default=1000)\\\\n-    parser.add_argument(\\\\\\\'--alpha\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Positive to negative triplet distance margin.\\\\\\\', default=0.2)\\\\n-    parser.add_argument(\\\\\\\'--embedding_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Dimensionality of the embedding.\\\\\\\', default=128)\\\\n-    parser.add_argument(\\\\\\\'--random_crop\\\\\\\', \\\\n-        help=\\\\\\\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \\\\\\\' +\\\\n-         \\\\\\\'If the size of the images in the data directory is equal to image_size no cropping is performed\\\\\\\', action=\\\\\\\'store_true\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--random_flip\\\\\\\', \\\\n-        help=\\\\\\\'Performs random horizontal flipping of training images.\\\\\\\', action=\\\\\\\'store_true\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--keep_probability\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Keep probability of dropout for the fully connected layer(s).\\\\\\\', default=1.0)\\\\n-    parser.add_argument(\\\\\\\'--weight_decay\\\\\\\', type=float,\\\\n-        help=\\\\\\\'L2 weight regularization.\\\\\\\', default=0.0)\\\\n-    parser.add_argument(\\\\\\\'--optimizer\\\\\\\', type=str, choices=[\\\\\\\'ADAGRAD\\\\\\\', \\\\\\\'ADADELTA\\\\\\\', \\\\\\\'ADAM\\\\\\\', \\\\\\\'RMSPROP\\\\\\\', \\\\\\\'MOM\\\\\\\'],\\\\n-        help=\\\\\\\'The optimization algorithm to use\\\\\\\', default=\\\\\\\'ADAGRAD\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--learning_rate\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Initial learning rate. If set to a negative value a learning rate \\\\\\\' +\\\\n-        \\\\\\\'schedule can be specified in the file "learning_rate_schedule.txt"\\\\\\\', default=0.1)\\\\n-    parser.add_argument(\\\\\\\'--learning_rate_decay_epochs\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of epochs between learning rate decay.\\\\\\\', default=100)\\\\n-    parser.add_argument(\\\\\\\'--learning_rate_decay_factor\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Learning rate decay factor.\\\\\\\', default=1.0)\\\\n-    parser.add_argument(\\\\\\\'--moving_average_decay\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Exponential decay for tracking of training parameters.\\\\\\\', default=0.9999)\\\\n-    parser.add_argument(\\\\\\\'--seed\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Random seed.\\\\\\\', default=666)\\\\n-    parser.add_argument(\\\\\\\'--learning_rate_schedule_file\\\\\\\', type=str,\\\\n-        help=\\\\\\\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\\\\\\\', default=\\\\\\\'data/learning_rate_schedule.txt\\\\\\\')\\\\n-\\\\n-    # Parameters for validation on LFW\\\\n-    parser.add_argument(\\\\\\\'--lfw_pairs\\\\\\\', type=str,\\\\n-        help=\\\\\\\'The file containing the pairs to use for validation.\\\\\\\', default=\\\\\\\'data/pairs.txt\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--lfw_dir\\\\\\\', type=str,\\\\n-        help=\\\\\\\'Path to the data directory containing aligned face patches.\\\\\\\', default=\\\\\\\'\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--lfw_nrof_folds\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of folds to use for cross validation. Mainly used for testing.\\\\\\\', default=10)\\\\n-    return parser.parse_args(argv)\\\\n-  \\\\n-\\\\n-if __name__ == \\\\\\\'__main__\\\\\\\':\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\ndiff --git a/src/validate_on_lfw.py b/src/validate_on_lfw.py\\\\ndeleted file mode 100644\\\\nindex ac456c5..0000000\\\\n--- a/src/validate_on_lfw.py\\\\n+++ /dev/null\\\\n@@ -1,164 +0,0 @@\\\\n-"""Validate a face recognizer on the "Labeled Faces in the Wild" dataset (http://vis-www.cs.umass.edu/lfw/).\\\\n-Embeddings are calculated using the pairs from http://vis-www.cs.umass.edu/lfw/pairs.txt and the ROC curve\\\\n-is calculated and plotted. Both the model metagraph and the model parameters need to exist\\\\n-in the same directory, and the metagraph should have the extension \\\\\\\'.meta\\\\\\\'.\\\\n-"""\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-import tensorflow as tf\\\\n-import numpy as np\\\\n-import argparse\\\\n-import facenet\\\\n-import lfw\\\\n-import os\\\\n-import sys\\\\n-from tensorflow.python.ops import data_flow_ops\\\\n-from sklearn import metrics\\\\n-from scipy.optimize import brentq\\\\n-from scipy import interpolate\\\\n-\\\\n-def main(args):\\\\n-  \\\\n-    with tf.Graph().as_default():\\\\n-      \\\\n-        with tf.Session() as sess:\\\\n-            \\\\n-            # Read the file containing the pairs used for testing\\\\n-            pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\\\\n-\\\\n-            # Get the paths for the corresponding images\\\\n-            paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\\\\n-            \\\\n-            image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\\\\\\\'image_paths\\\\\\\')\\\\n-            labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\\\\\\\'labels\\\\\\\')\\\\n-            batch_size_placeholder = tf.placeholder(tf.int32, name=\\\\\\\'batch_size\\\\\\\')\\\\n-            control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\\\\\\\'control\\\\\\\')\\\\n-            phase_train_placeholder = tf.placeholder(tf.bool, name=\\\\\\\'phase_train\\\\\\\')\\\\n- \\\\n-            nrof_preprocess_threads = 4\\\\n-            image_size = (args.image_size, args.image_size)\\\\n-            eval_input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\\\\n-                                        dtypes=[tf.string, tf.int32, tf.int32],\\\\n-                                        shapes=[(1,), (1,), (1,)],\\\\n-                                        shared_name=None, name=None)\\\\n-            eval_enqueue_op = eval_input_queue.enqueue_many([image_paths_placeholder, labels_placeholder, control_placeholder], name=\\\\\\\'eval_enqueue_op\\\\\\\')\\\\n-            image_batch, label_batch = facenet.create_input_pipeline(eval_input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder)\\\\n-     \\\\n-            # Load the model\\\\n-            input_map = {\\\\\\\'image_batch\\\\\\\': image_batch, \\\\\\\'label_batch\\\\\\\': label_batch, \\\\\\\'phase_train\\\\\\\': phase_train_placeholder}\\\\n-            facenet.load_model(args.model, input_map=input_map)\\\\n-\\\\n-            # Get output tensor\\\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\n-#              \\\\n-            coord = tf.train.Coordinator()\\\\n-            tf.train.start_queue_runners(coord=coord, sess=sess)\\\\n-\\\\n-            evaluate(sess, eval_enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\\\\n-                embeddings, label_batch, paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, args.distance_metric, args.subtract_mean,\\\\n-                args.use_flipped_images, args.use_fixed_image_standardization)\\\\n-\\\\n-              \\\\n-def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\\\\n-        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, distance_metric, subtract_mean, use_flipped_images, use_fixed_image_standardization):\\\\n-    # Run forward pass to calculate embeddings\\\\n-    print(\\\\\\\'Runnning forward pass on LFW images\\\\\\\')\\\\n-    \\\\n-    # Enqueue one epoch of image paths and labels\\\\n-    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\\\\n-    nrof_flips = 2 if use_flipped_images else 1\\\\n-    nrof_images = nrof_embeddings * nrof_flips\\\\n-    labels_array = np.expand_dims(np.arange(0,nrof_images),1)\\\\n-    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\\\\n-    control_array = np.zeros_like(labels_array, np.int32)\\\\n-    if use_fixed_image_standardization:\\\\n-        control_array += np.ones_like(labels_array)*facenet.FIXED_STANDARDIZATION\\\\n-    if use_flipped_images:\\\\n-        # Flip every second image\\\\n-        control_array += (labels_array % 2)*facenet.FLIP\\\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\\\n-    \\\\n-    embedding_size = int(embeddings.get_shape()[1])\\\\n-    assert nrof_images % batch_size == 0, \\\\\\\'The number of LFW images must be an integer multiple of the LFW batch size\\\\\\\'\\\\n-    nrof_batches = nrof_images // batch_size\\\\n-    emb_array = np.zeros((nrof_images, embedding_size))\\\\n-    lab_array = np.zeros((nrof_images,))\\\\n-    for i in range(nrof_batches):\\\\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}\\\\n-        emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)\\\\n-        lab_array[lab] = lab\\\\n-        emb_array[lab, :] = emb\\\\n-        if i % 10 == 9:\\\\n-            print(\\\\\\\'.\\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n-            sys.stdout.flush()\\\\n-    print(\\\\\\\'\\\\\\\')\\\\n-    embeddings = np.zeros((nrof_embeddings, embedding_size*nrof_flips))\\\\n-    if use_flipped_images:\\\\n-        # Concatenate embeddings for flipped and non flipped version of the images\\\\n-        embeddings[:,:embedding_size] = emb_array[0::2,:]\\\\n-        embeddings[:,embedding_size:] = emb_array[1::2,:]\\\\n-    else:\\\\n-        embeddings = emb_array\\\\n-\\\\n-    assert np.array_equal(lab_array, np.arange(nrof_images))==True, \\\\\\\'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline\\\\\\\'\\\\n-    tpr, fpr, accuracy, val, val_std, far = lfw.evaluate(embeddings, actual_issame, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\\\\n-    \\\\n-    print(\\\\\\\'Accuracy: %2.5f+-%2.5f\\\\\\\' % (np.mean(accuracy), np.std(accuracy)))\\\\n-    print(\\\\\\\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\\\\\\\' % (val, val_std, far))\\\\n-    \\\\n-    auc = metrics.auc(fpr, tpr)\\\\n-    print(\\\\\\\'Area Under Curve (AUC): %1.3f\\\\\\\' % auc)\\\\n-    eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\\\\n-    print(\\\\\\\'Equal Error Rate (EER): %1.3f\\\\\\\' % eer)\\\\n-    \\\\n-def parse_arguments(argv):\\\\n-    parser = argparse.ArgumentParser()\\\\n-    \\\\n-    parser.add_argument(\\\\\\\'lfw_dir\\\\\\\', type=str,\\\\n-        help=\\\\\\\'Path to the data directory containing aligned LFW face patches.\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--lfw_batch_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of images to process in a batch in the LFW test set.\\\\\\\', default=100)\\\\n-    parser.add_argument(\\\\\\\'model\\\\\\\', type=str, \\\\n-        help=\\\\\\\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=160)\\\\n-    parser.add_argument(\\\\\\\'--lfw_pairs\\\\\\\', type=str,\\\\n-        help=\\\\\\\'The file containing the pairs to use for validation.\\\\\\\', default=\\\\\\\'data/pairs.txt\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--lfw_nrof_folds\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Number of folds to use for cross validation. Mainly used for testing.\\\\\\\', default=10)\\\\n-    parser.add_argument(\\\\\\\'--distance_metric\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Distance metric  0:euclidian, 1:cosine similarity.\\\\\\\', default=0)\\\\n-    parser.add_argument(\\\\\\\'--use_flipped_images\\\\\\\', \\\\n-        help=\\\\\\\'Concatenates embeddings for the image and its horizontally flipped counterpart.\\\\\\\', action=\\\\\\\'store_true\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--subtract_mean\\\\\\\', \\\\n-        help=\\\\\\\'Subtract feature mean before calculating distance.\\\\\\\', action=\\\\\\\'store_true\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--use_fixed_image_standardization\\\\\\\', \\\\n-        help=\\\\\\\'Performs fixed standardization of images.\\\\\\\', action=\\\\\\\'store_true\\\\\\\')\\\\n-    return parser.parse_args(argv)\\\\n-\\\\n-if __name__ == \\\\\\\'__main__\\\\\\\':\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\ndiff --git a/test/a b/test/a\\\\ndeleted file mode 100644\\\\nindex 8b13789..0000000\\\\n--- a/test/a\\\\n+++ /dev/null\\\\n@@ -1 +0,0 @@\\\\n-\\\\ndiff --git a/test/batch_norm_test.py b/test/batch_norm_test.py\\\\ndeleted file mode 100644\\\\nindex 48cfd55..0000000\\\\n--- a/test/batch_norm_test.py\\\\n+++ /dev/null\\\\n@@ -1,66 +0,0 @@\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-import unittest\\\\n-import tensorflow as tf\\\\n-import models\\\\n-import numpy as np\\\\n-import numpy.testing as testing\\\\n-\\\\n-class BatchNormTest(unittest.TestCase):\\\\n-\\\\n-\\\\n-    @unittest.skip("Skip batch norm test case")\\\\n-    def testBatchNorm(self):\\\\n-      \\\\n-        tf.set_random_seed(123)\\\\n-  \\\\n-        x = tf.placeholder(tf.float32, [None, 20, 20, 10], name=\\\\\\\'input\\\\\\\')\\\\n-        phase_train = tf.placeholder(tf.bool, name=\\\\\\\'phase_train\\\\\\\')\\\\n-        \\\\n-        # generate random noise to pass into batch norm\\\\n-        #x_gen = tf.random_normal([50,20,20,10])\\\\n-        \\\\n-        bn = models.network.batch_norm(x, phase_train)\\\\n-        \\\\n-        init = tf.global_variables_initializer()\\\\n-        sess = tf.Session(config=tf.ConfigProto())\\\\n-        sess.run(init)\\\\n-  \\\\n-        with sess.as_default():\\\\n-        \\\\n-            #generate a constant variable to pass into batch norm\\\\n-            y = np.random.normal(0, 1, size=(50,20,20,10))\\\\n-            \\\\n-            feed_dict = {x: y, phase_train: True}\\\\n-            sess.run(bn, feed_dict=feed_dict)\\\\n-            \\\\n-            feed_dict = {x: y, phase_train: False}\\\\n-            y1 = sess.run(bn, feed_dict=feed_dict)\\\\n-            y2 = sess.run(bn, feed_dict=feed_dict)\\\\n-            \\\\n-            testing.assert_almost_equal(y1, y2, 10, \\\\\\\'Output from two forward passes with phase_train==false should be equal\\\\\\\')\\\\n-\\\\n-\\\\n-if __name__ == "__main__":\\\\n-    unittest.main()\\\\n-    \\\\n\\\\\\\\ No newline at end of file\\\\ndiff --git a/test/center_loss_test.py b/test/center_loss_test.py\\\\ndeleted file mode 100644\\\\nindex 196cd11..0000000\\\\n--- a/test/center_loss_test.py\\\\n+++ /dev/null\\\\n@@ -1,87 +0,0 @@\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-import unittest\\\\n-import tensorflow as tf\\\\n-import numpy as np\\\\n-import facenet\\\\n-\\\\n-class CenterLossTest(unittest.TestCase):\\\\n-  \\\\n-\\\\n-\\\\n-    def testCenterLoss(self):\\\\n-        batch_size = 16\\\\n-        nrof_features = 2\\\\n-        nrof_classes = 16\\\\n-        alfa = 0.5\\\\n-        \\\\n-        with tf.Graph().as_default():\\\\n-        \\\\n-            features = tf.placeholder(tf.float32, shape=(batch_size, nrof_features), name=\\\\\\\'features\\\\\\\')\\\\n-            labels = tf.placeholder(tf.int32, shape=(batch_size,), name=\\\\\\\'labels\\\\\\\')\\\\n-\\\\n-            # Define center loss\\\\n-            center_loss, centers = facenet.center_loss(features, labels, alfa, nrof_classes)\\\\n-            \\\\n-            label_to_center = np.array( [ \\\\n-                 [-3,-3],  [-3,-1],  [-3,1],  [-3,3],\\\\n-                 [-1,-3],  [-1,-1],  [-1,1],  [-1,3],\\\\n-                 [ 1,-3],  [ 1,-1],  [ 1,1],  [ 1,3],\\\\n-                 [ 3,-3],  [ 3,-1],  [ 3,1],  [ 3,3] \\\\n-                 ])\\\\n-                \\\\n-            sess = tf.Session()\\\\n-            with sess.as_default():\\\\n-                sess.run(tf.global_variables_initializer())\\\\n-                np.random.seed(seed=666)\\\\n-                \\\\n-                for _ in range(0,100):\\\\n-                    # Create array of random labels\\\\n-                    lbls = np.random.randint(low=0, high=nrof_classes, size=(batch_size,))\\\\n-                    feats = create_features(label_to_center, batch_size, nrof_features, lbls)\\\\n-\\\\n-                    center_loss_, centers_ = sess.run([center_loss, centers], feed_dict={features:feats, labels:lbls})\\\\n-                    \\\\n-                # After a large number of updates the estimated centers should be close to the true ones\\\\n-                np.testing.assert_almost_equal(centers_, label_to_center, decimal=5, err_msg=\\\\\\\'Incorrect estimated centers\\\\\\\')\\\\n-                np.testing.assert_almost_equal(center_loss_, 0.0, decimal=5, err_msg=\\\\\\\'Incorrect center loss\\\\\\\')\\\\n-                \\\\n-\\\\n-def create_features(label_to_center, batch_size, nrof_features, labels):\\\\n-    # Map label to center\\\\n-#     label_to_center_dict = { \\\\n-#          0:(-3,-3),  1:(-3,-1),  2:(-3,1),  3:(-3,3),\\\\n-#          4:(-1,-3),  5:(-1,-1),  6:(-1,1),  7:(-1,3),\\\\n-#          8:( 1,-3),  9:( 1,-1), 10:( 1,1), 11:( 1,3),\\\\n-#         12:( 3,-3), 13:( 3,-1), 14:( 3,1), 15:( 3,3),\\\\n-#         }\\\\n-    # Create array of features corresponding to the labels\\\\n-    feats = np.zeros((batch_size, nrof_features))\\\\n-    for i in range(batch_size):\\\\n-        cntr =  label_to_center[labels[i]]\\\\n-        for j in range(nrof_features):\\\\n-            feats[i,j] = cntr[j]\\\\n-    return feats\\\\n-                      \\\\n-if __name__ == "__main__":\\\\n-    unittest.main()\\\\ndiff --git a/test/restore_test.py b/test/restore_test.py\\\\ndeleted file mode 100644\\\\nindex befb04d..0000000\\\\n--- a/test/restore_test.py\\\\n+++ /dev/null\\\\n@@ -1,181 +0,0 @@\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-import unittest\\\\n-import tempfile\\\\n-import os\\\\n-import shutil\\\\n-import tensorflow as tf\\\\n-import numpy as np\\\\n-\\\\n-class TrainTest(unittest.TestCase):\\\\n-  \\\\n-    @classmethod\\\\n-    def setUpClass(self):\\\\n-        self.tmp_dir = tempfile.mkdtemp()\\\\n-        \\\\n-    @classmethod\\\\n-    def tearDownClass(self):\\\\n-        # Recursively remove the temporary directory\\\\n-        shutil.rmtree(self.tmp_dir)\\\\n-\\\\n-    def test_restore_noema(self):\\\\n-        \\\\n-        # Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\\\\n-        x_data = np.random.rand(100).astype(np.float32)\\\\n-        y_data = x_data * 0.1 + 0.3\\\\n-        \\\\n-        # Try to find values for W and b that compute y_data = W * x_data + b\\\\n-        # (We know that W should be 0.1 and b 0.3, but TensorFlow will\\\\n-        # figure that out for us.)\\\\n-        W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name=\\\\\\\'W\\\\\\\')\\\\n-        b = tf.Variable(tf.zeros([1]), name=\\\\\\\'b\\\\\\\')\\\\n-        y = W * x_data + b\\\\n-        \\\\n-        # Minimize the mean squared errors.\\\\n-        loss = tf.reduce_mean(tf.square(y - y_data))\\\\n-        optimizer = tf.train.GradientDescentOptimizer(0.5)\\\\n-        train = optimizer.minimize(loss)\\\\n-        \\\\n-        # Before starting, initialize the variables.  We will \\\\\\\'run\\\\\\\' this first.\\\\n-        init = tf.global_variables_initializer()\\\\n-\\\\n-        saver = tf.train.Saver(tf.trainable_variables())\\\\n-        \\\\n-        # Launch the graph.\\\\n-        sess = tf.Session()\\\\n-        sess.run(init)\\\\n-        \\\\n-        # Fit the line.\\\\n-        for _ in range(201):\\\\n-            sess.run(train)\\\\n-        \\\\n-        w_reference = sess.run(\\\\\\\'W:0\\\\\\\')\\\\n-        b_reference = sess.run(\\\\\\\'b:0\\\\\\\')\\\\n-        \\\\n-        saver.save(sess, os.path.join(self.tmp_dir, "model_ex1"))\\\\n-        \\\\n-        tf.reset_default_graph()\\\\n-\\\\n-        saver = tf.train.import_meta_graph(os.path.join(self.tmp_dir, "model_ex1.meta"))\\\\n-        sess = tf.Session()\\\\n-        saver.restore(sess, os.path.join(self.tmp_dir, "model_ex1"))\\\\n-        \\\\n-        w_restored = sess.run(\\\\\\\'W:0\\\\\\\')\\\\n-        b_restored = sess.run(\\\\\\\'b:0\\\\\\\')\\\\n-        \\\\n-        self.assertAlmostEqual(w_reference, w_restored, \\\\\\\'Restored model use different weight than the original model\\\\\\\')\\\\n-        self.assertAlmostEqual(b_reference, b_restored, \\\\\\\'Restored model use different weight than the original model\\\\\\\')\\\\n-\\\\n-\\\\n-    @unittest.skip("Skip restore EMA test case for now")\\\\n-    def test_restore_ema(self):\\\\n-        \\\\n-        # Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\\\\n-        x_data = np.random.rand(100).astype(np.float32)\\\\n-        y_data = x_data * 0.1 + 0.3\\\\n-        \\\\n-        # Try to find values for W and b that compute y_data = W * x_data + b\\\\n-        # (We know that W should be 0.1 and b 0.3, but TensorFlow will\\\\n-        # figure that out for us.)\\\\n-        W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name=\\\\\\\'W\\\\\\\')\\\\n-        b = tf.Variable(tf.zeros([1]), name=\\\\\\\'b\\\\\\\')\\\\n-        y = W * x_data + b\\\\n-        \\\\n-        # Minimize the mean squared errors.\\\\n-        loss = tf.reduce_mean(tf.square(y - y_data))\\\\n-        optimizer = tf.train.GradientDescentOptimizer(0.5)\\\\n-        opt_op = optimizer.minimize(loss)\\\\n-\\\\n-        # Track the moving averages of all trainable variables.\\\\n-        ema = tf.train.ExponentialMovingAverage(decay=0.9999)\\\\n-        averages_op = ema.apply(tf.trainable_variables())\\\\n-        with tf.control_dependencies([opt_op]):\\\\n-            train_op = tf.group(averages_op)\\\\n-  \\\\n-        # Before starting, initialize the variables.  We will \\\\\\\'run\\\\\\\' this first.\\\\n-        init = tf.global_variables_initializer()\\\\n-\\\\n-        saver = tf.train.Saver(tf.trainable_variables())\\\\n-        \\\\n-        # Launch the graph.\\\\n-        sess = tf.Session()\\\\n-        sess.run(init)\\\\n-        \\\\n-        # Fit the line.\\\\n-        for _ in range(201):\\\\n-            sess.run(train_op)\\\\n-        \\\\n-        w_reference = sess.run(\\\\\\\'W/ExponentialMovingAverage:0\\\\\\\')\\\\n-        b_reference = sess.run(\\\\\\\'b/ExponentialMovingAverage:0\\\\\\\')\\\\n-        \\\\n-        saver.save(sess, os.path.join(self.tmp_dir, "model_ex1"))\\\\n-                \\\\n-        tf.reset_default_graph()\\\\n-\\\\n-        tf.train.import_meta_graph(os.path.join(self.tmp_dir, "model_ex1.meta"))\\\\n-        sess = tf.Session()\\\\n-        \\\\n-        print(\\\\\\\'------------------------------------------------------\\\\\\\')\\\\n-        for var in tf.global_variables():\\\\n-            print(\\\\\\\'all variables: \\\\\\\' + var.op.name)\\\\n-        for var in tf.trainable_variables():\\\\n-            print(\\\\\\\'normal variable: \\\\\\\' + var.op.name)\\\\n-        for var in tf.moving_average_variables():\\\\n-            print(\\\\\\\'ema variable: \\\\\\\' + var.op.name)\\\\n-        print(\\\\\\\'------------------------------------------------------\\\\\\\')\\\\n-\\\\n-        mode = 1\\\\n-        restore_vars = {}\\\\n-        if mode == 0:\\\\n-            ema = tf.train.ExponentialMovingAverage(1.0)\\\\n-            for var in tf.trainable_variables():\\\\n-                print(\\\\\\\'%s: %s\\\\\\\' % (ema.average_name(var), var.op.name))\\\\n-                restore_vars[ema.average_name(var)] = var\\\\n-        elif mode == 1:\\\\n-            for var in tf.trainable_variables():\\\\n-                ema_name = var.op.name + \\\\\\\'/ExponentialMovingAverage\\\\\\\'\\\\n-                print(\\\\\\\'%s: %s\\\\\\\' % (ema_name, var.op.name))\\\\n-                restore_vars[ema_name] = var\\\\n-            \\\\n-        saver = tf.train.Saver(restore_vars, name=\\\\\\\'ema_restore\\\\\\\')\\\\n-        \\\\n-        saver.restore(sess, os.path.join(self.tmp_dir, "model_ex1"))\\\\n-        \\\\n-        w_restored = sess.run(\\\\\\\'W:0\\\\\\\')\\\\n-        b_restored = sess.run(\\\\\\\'b:0\\\\\\\')\\\\n-        \\\\n-        self.assertAlmostEqual(w_reference, w_restored, \\\\\\\'Restored model modes not use the EMA filtered weight\\\\\\\')\\\\n-        self.assertAlmostEqual(b_reference, b_restored, \\\\\\\'Restored model modes not use the EMA filtered bias\\\\\\\')\\\\n-\\\\n-        \\\\n-# Create a checkpoint file pointing to the model\\\\n-def create_checkpoint_file(model_dir, model_file):\\\\n-    checkpoint_filename = os.path.join(model_dir, \\\\\\\'checkpoint\\\\\\\')\\\\n-    full_model_filename = os.path.join(model_dir, model_file)\\\\n-    with open(checkpoint_filename, \\\\\\\'w\\\\\\\') as f:\\\\n-        f.write(\\\\\\\'model_checkpoint_path: "%s"\\\\\\\\n\\\\\\\' % full_model_filename)\\\\n-        f.write(\\\\\\\'all_model_checkpoint_paths: "%s"\\\\\\\\n\\\\\\\' % full_model_filename)\\\\n-        \\\\n-if __name__ == "__main__":\\\\n-    unittest.main()\\\\n-    \\\\n\\\\\\\\ No newline at end of file\\\\ndiff --git a/test/train_test.py b/test/train_test.py\\\\ndeleted file mode 100644\\\\nindex 12cd663..0000000\\\\n--- a/test/train_test.py\\\\n+++ /dev/null\\\\n@@ -1,246 +0,0 @@\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-import unittest\\\\n-import tempfile\\\\n-import numpy as np\\\\n-import cv2\\\\n-import os\\\\n-import shutil\\\\n-import download_and_extract  # @UnresolvedImport\\\\n-import subprocess\\\\n-\\\\n-def memory_usage_psutil():\\\\n-    # return the memory usage in MB\\\\n-    import psutil\\\\n-    process = psutil.Process(os.getpid())\\\\n-    mem = process.memory_info()[0] / float(2 ** 20)\\\\n-    return mem\\\\n-\\\\n-def align_dataset_if_needed(self):\\\\n-    if not os.path.exists(\\\\\\\'data/lfw_aligned\\\\\\\'):\\\\n-        argv = [\\\\\\\'python\\\\\\\',\\\\n-                \\\\\\\'src/align/align_dataset_mtcnn.py\\\\\\\',\\\\n-                \\\\\\\'data/lfw\\\\\\\',\\\\n-                \\\\\\\'data/lfw_aligned\\\\\\\',\\\\n-                \\\\\\\'--image_size\\\\\\\', \\\\\\\'160\\\\\\\',\\\\n-                \\\\\\\'--margin\\\\\\\', \\\\\\\'32\\\\\\\' ]\\\\n-        subprocess.call(argv)\\\\n-        \\\\n-        \\\\n-class TrainTest(unittest.TestCase):\\\\n-  \\\\n-    @classmethod\\\\n-    def setUpClass(self):\\\\n-        self.tmp_dir = tempfile.mkdtemp()\\\\n-        self.dataset_dir = os.path.join(self.tmp_dir, \\\\\\\'dataset\\\\\\\')\\\\n-        create_mock_dataset(self.dataset_dir, 160)\\\\n-        self.lfw_pairs_file = create_mock_lfw_pairs(self.tmp_dir)\\\\n-        print(self.lfw_pairs_file)\\\\n-        self.pretrained_model_name = \\\\\\\'20180402-114759\\\\\\\'\\\\n-        download_and_extract.download_and_extract_file(self.pretrained_model_name, \\\\\\\'data/\\\\\\\')\\\\n-        download_and_extract.download_and_extract_file(\\\\\\\'lfw-subset\\\\\\\', \\\\\\\'data/\\\\\\\')\\\\n-        self.model_file = os.path.join(\\\\\\\'data\\\\\\\', self.pretrained_model_name, \\\\\\\'model-%s.ckpt-275\\\\\\\' % self.pretrained_model_name)\\\\n-        self.pretrained_model = os.path.join(\\\\\\\'data\\\\\\\', self.pretrained_model_name)\\\\n-        self.frozen_graph_filename = os.path.join(\\\\\\\'data\\\\\\\', self.pretrained_model_name+\\\\\\\'.pb\\\\\\\')\\\\n-        print(\\\\\\\'Memory utilization (SetUpClass): %.3f MB\\\\\\\' % memory_usage_psutil())\\\\n-\\\\n-    @classmethod\\\\n-    def tearDownClass(self):\\\\n-        # Recursively remove the temporary directory\\\\n-        shutil.rmtree(self.tmp_dir)\\\\n-\\\\n-    def tearDown(self):\\\\n-        print(\\\\\\\'Memory utilization (TearDown): %.3f MB\\\\\\\' % memory_usage_psutil())\\\\n-\\\\n-    def test_training_classifier_inception_resnet_v1(self):\\\\n-        print(\\\\\\\'test_training_classifier_inception_resnet_v1\\\\\\\')\\\\n-        argv = [\\\\\\\'python\\\\\\\',\\\\n-                \\\\\\\'src/train_softmax.py\\\\\\\',\\\\n-                \\\\\\\'--logs_base_dir\\\\\\\', self.tmp_dir,\\\\n-                \\\\\\\'--models_base_dir\\\\\\\', self.tmp_dir,\\\\n-                \\\\\\\'--data_dir\\\\\\\', self.dataset_dir,\\\\n-                \\\\\\\'--model_def\\\\\\\', \\\\\\\'models.inception_resnet_v1\\\\\\\',\\\\n-                \\\\\\\'--epoch_size\\\\\\\', \\\\\\\'1\\\\\\\',\\\\n-                \\\\\\\'--max_nrof_epochs\\\\\\\', \\\\\\\'1\\\\\\\',\\\\n-                \\\\\\\'--batch_size\\\\\\\', \\\\\\\'1\\\\\\\',\\\\n-                \\\\\\\'--lfw_pairs\\\\\\\', self.lfw_pairs_file,\\\\n-                \\\\\\\'--lfw_dir\\\\\\\', self.dataset_dir,\\\\n-                \\\\\\\'--lfw_nrof_folds\\\\\\\', \\\\\\\'2\\\\\\\',\\\\n-                \\\\\\\'--lfw_batch_size\\\\\\\', \\\\\\\'1\\\\\\\',\\\\n-                \\\\\\\'--nrof_preprocess_threads\\\\\\\', \\\\\\\'1\\\\\\\' ]\\\\n-        subprocess.call(argv)\\\\n-\\\\n-    def test_training_classifier_inception_resnet_v2(self):\\\\n-        print(\\\\\\\'test_training_classifier_inception_resnet_v2\\\\\\\')\\\\n-        argv = [\\\\\\\'python\\\\\\\',\\\\n-                \\\\\\\'src/train_softmax.py\\\\\\\',\\\\n-                \\\\\\\'--logs_base_dir\\\\\\\', self.tmp_dir,\\\\n-                \\\\\\\'--models_base_dir\\\\\\\', self.tmp_dir,\\\\n-                \\\\\\\'--data_dir\\\\\\\', self.dataset_dir,\\\\n-                \\\\\\\'--model_def\\\\\\\', \\\\\\\'models.inception_resnet_v2\\\\\\\',\\\\n-                \\\\\\\'--epoch_size\\\\\\\', \\\\\\\'1\\\\\\\',\\\\n-                \\\\\\\'--max_nrof_epochs\\\\\\\', \\\\\\\'1\\\\\\\',\\\\n-                \\\\\\\'--batch_size\\\\\\\', \\\\\\\'1\\\\\\\',\\\\n-                \\\\\\\'--lfw_pairs\\\\\\\', self.lfw_pairs_file,\\\\n-                \\\\\\\'--lfw_dir\\\\\\\', self.dataset_dir,\\\\n-                \\\\\\\'--lfw_nrof_folds\\\\\\\', \\\\\\\'2\\\\\\\',\\\\n-                \\\\\\\'--lfw_batch_size\\\\\\\', \\\\\\\'1\\\\\\\' ]\\\\n-        subprocess.call(argv)\\\\n-  \\\\n-    def test_training_classifier_squeezenet(self):\\\\n-        print(\\\\\\\'test_training_classifier_squeezenet\\\\\\\')\\\\n-        argv = [\\\\\\\'python\\\\\\\',\\\\n-                \\\\\\\'src/train_softmax.py\\\\\\\',\\\\n-                \\\\\\\'--logs_base_dir\\\\\\\', self.tmp_dir,\\\\n-                \\\\\\\'--models_base_dir\\\\\\\', self.tmp_dir,\\\\n-                \\\\\\\'--data_dir\\\\\\\', self.dataset_dir,\\\\n-                \\\\\\\'--model_def\\\\\\\', \\\\\\\'models.squeezenet\\\\\\\',\\\\n-                \\\\\\\'--epoch_size\\\\\\\', \\\\\\\'1\\\\\\\',\\\\n-                \\\\\\\'--max_nrof_epochs\\\\\\\', \\\\\\\'1\\\\\\\',\\\\n-                \\\\\\\'--batch_size\\\\\\\', \\\\\\\'1\\\\\\\',\\\\n-                \\\\\\\'--lfw_pairs\\\\\\\', self.lfw_pairs_file,\\\\n-                \\\\\\\'--lfw_dir\\\\\\\', self.dataset_dir,\\\\n-                \\\\\\\'--lfw_nrof_folds\\\\\\\', \\\\\\\'2\\\\\\\',\\\\n-                \\\\\\\'--lfw_batch_size\\\\\\\', \\\\\\\'1\\\\\\\',\\\\n-                \\\\\\\'--nrof_preprocess_threads\\\\\\\', \\\\\\\'1\\\\\\\' ]\\\\n-        subprocess.call(argv)\\\\n- \\\\n-    def test_train_tripletloss_inception_resnet_v1(self):\\\\n-        print(\\\\\\\'test_train_tripletloss_inception_resnet_v1\\\\\\\')\\\\n-        argv = [\\\\\\\'python\\\\\\\',\\\\n-                \\\\\\\'src/train_tripletloss.py\\\\\\\',\\\\n-                \\\\\\\'--logs_base_dir\\\\\\\', self.tmp_dir,\\\\n-                \\\\\\\'--models_base_dir\\\\\\\', self.tmp_dir,\\\\n-                \\\\\\\'--data_dir\\\\\\\', self.dataset_dir,\\\\n-                \\\\\\\'--model_def\\\\\\\', \\\\\\\'models.inception_resnet_v1\\\\\\\',\\\\n-                \\\\\\\'--epoch_size\\\\\\\', \\\\\\\'1\\\\\\\',\\\\n-                \\\\\\\'--max_nrof_epochs\\\\\\\', \\\\\\\'1\\\\\\\',\\\\n-                \\\\\\\'--batch_size\\\\\\\', \\\\\\\'6\\\\\\\',\\\\n-                \\\\\\\'--people_per_batch\\\\\\\', \\\\\\\'2\\\\\\\',\\\\n-                \\\\\\\'--images_per_person\\\\\\\', \\\\\\\'3\\\\\\\',\\\\n-                \\\\\\\'--lfw_pairs\\\\\\\', self.lfw_pairs_file,\\\\n-                \\\\\\\'--lfw_dir\\\\\\\', self.dataset_dir,\\\\n-                \\\\\\\'--lfw_nrof_folds\\\\\\\', \\\\\\\'2\\\\\\\' ]\\\\n-        subprocess.call(argv)\\\\n-  \\\\n-    def test_finetune_tripletloss_inception_resnet_v1(self):\\\\n-        print(\\\\\\\'test_finetune_tripletloss_inception_resnet_v1\\\\\\\')\\\\n-        argv = [\\\\\\\'python\\\\\\\',\\\\n-                \\\\\\\'src/train_tripletloss.py\\\\\\\',\\\\n-                \\\\\\\'--logs_base_dir\\\\\\\', self.tmp_dir,\\\\n-                \\\\\\\'--models_base_dir\\\\\\\', self.tmp_dir,\\\\n-                \\\\\\\'--data_dir\\\\\\\', self.dataset_dir,\\\\n-                \\\\\\\'--model_def\\\\\\\', \\\\\\\'models.inception_resnet_v1\\\\\\\',\\\\n-                \\\\\\\'--pretrained_model\\\\\\\', self.model_file,\\\\n-                \\\\\\\'--embedding_size\\\\\\\', \\\\\\\'512\\\\\\\',\\\\n-                \\\\\\\'--epoch_size\\\\\\\', \\\\\\\'1\\\\\\\',\\\\n-                \\\\\\\'--max_nrof_epochs\\\\\\\', \\\\\\\'1\\\\\\\',\\\\n-                \\\\\\\'--batch_size\\\\\\\', \\\\\\\'6\\\\\\\',\\\\n-                \\\\\\\'--people_per_batch\\\\\\\', \\\\\\\'2\\\\\\\',\\\\n-                \\\\\\\'--images_per_person\\\\\\\', \\\\\\\'3\\\\\\\',\\\\n-                \\\\\\\'--lfw_pairs\\\\\\\', self.lfw_pairs_file,\\\\n-                \\\\\\\'--lfw_dir\\\\\\\', self.dataset_dir,\\\\n-                \\\\\\\'--lfw_nrof_folds\\\\\\\', \\\\\\\'2\\\\\\\' ]\\\\n-        subprocess.call(argv)\\\\n-  \\\\n-    def test_compare(self):\\\\n-        print(\\\\\\\'test_compare\\\\\\\')\\\\n-        argv = [\\\\\\\'python\\\\\\\',\\\\n-                \\\\\\\'src/compare.py\\\\\\\',\\\\n-                os.path.join(\\\\\\\'data/\\\\\\\', self.pretrained_model_name),\\\\n-                \\\\\\\'data/images/Anthony_Hopkins_0001.jpg\\\\\\\',\\\\n-                \\\\\\\'data/images/Anthony_Hopkins_0002.jpg\\\\\\\' ]\\\\n-        subprocess.call(argv)\\\\n-         \\\\n-    def test_validate_on_lfw(self):\\\\n-        print(\\\\\\\'test_validate_on_lfw\\\\\\\')\\\\n-        align_dataset_if_needed(self)\\\\n-        argv = [\\\\\\\'python\\\\\\\',\\\\n-                \\\\\\\'src/validate_on_lfw.py\\\\\\\', \\\\n-                \\\\\\\'data/lfw_aligned\\\\\\\',\\\\n-                self.pretrained_model,\\\\n-                \\\\\\\'--lfw_pairs\\\\\\\', \\\\\\\'data/lfw/pairs_small.txt\\\\\\\',\\\\n-                \\\\\\\'--lfw_nrof_folds\\\\\\\', \\\\\\\'2\\\\\\\',\\\\n-                \\\\\\\'--lfw_batch_size\\\\\\\', \\\\\\\'6\\\\\\\']\\\\n-        subprocess.call(argv)\\\\n- \\\\n-    def test_validate_on_lfw_frozen_graph(self):\\\\n-        print(\\\\\\\'test_validate_on_lfw_frozen_graph\\\\\\\')\\\\n-        self.pretrained_model = os.path.join(\\\\\\\'data\\\\\\\', self.pretrained_model_name)\\\\n-        frozen_model = os.path.join(self.pretrained_model, self.pretrained_model_name+\\\\\\\'.pb\\\\\\\')\\\\n-        argv = [\\\\\\\'python\\\\\\\',\\\\n-                \\\\\\\'src/validate_on_lfw.py\\\\\\\',\\\\n-                self.dataset_dir,\\\\n-                frozen_model,\\\\n-                \\\\\\\'--lfw_pairs\\\\\\\', self.lfw_pairs_file,\\\\n-                \\\\\\\'--lfw_nrof_folds\\\\\\\', \\\\\\\'2\\\\\\\',\\\\n-                \\\\\\\'--lfw_batch_size\\\\\\\', \\\\\\\'6\\\\\\\']\\\\n-        subprocess.call(argv)\\\\n- \\\\n-    def test_freeze_graph(self):\\\\n-        print(\\\\\\\'test_freeze_graph\\\\\\\')\\\\n-        argv = [\\\\\\\'python\\\\\\\',\\\\n-                \\\\\\\'src/freeze_graph.py\\\\\\\',\\\\n-                self.pretrained_model,\\\\n-                self.frozen_graph_filename ]\\\\n-        subprocess.call(argv)\\\\n-\\\\n-# Create a mock dataset with random pixel images\\\\n-def create_mock_dataset(dataset_dir, image_size):\\\\n-   \\\\n-    nrof_persons = 3\\\\n-    nrof_images_per_person = 2\\\\n-    np.random.seed(seed=666)\\\\n-    os.mkdir(dataset_dir)\\\\n-    for i in range(nrof_persons):\\\\n-        class_name = \\\\\\\'%04d\\\\\\\' % (i+1)\\\\n-        class_dir = os.path.join(dataset_dir, class_name)\\\\n-        os.mkdir(class_dir)\\\\n-        for j in range(nrof_images_per_person):\\\\n-            img_name = \\\\\\\'%04d\\\\\\\' % (j+1)\\\\n-            img_path = os.path.join(class_dir, class_name+\\\\\\\'_\\\\\\\'+img_name + \\\\\\\'.png\\\\\\\')\\\\n-            img = np.random.uniform(low=0.0, high=255.0, size=(image_size,image_size,3))\\\\n-            cv2.imwrite(img_path, img) #@UndefinedVariable\\\\n-\\\\n-# Create a mock LFW pairs file\\\\n-def create_mock_lfw_pairs(tmp_dir):\\\\n-    pairs_filename = os.path.join(tmp_dir, \\\\\\\'pairs_mock.txt\\\\\\\')\\\\n-    with open(pairs_filename, \\\\\\\'w\\\\\\\') as f:\\\\n-        f.write(\\\\\\\'10 300\\\\\\\\n\\\\\\\')\\\\n-        f.write(\\\\\\\'0001 1 2\\\\\\\\n\\\\\\\')\\\\n-        f.write(\\\\\\\'0001 1 0002 1\\\\\\\\n\\\\\\\')\\\\n-        f.write(\\\\\\\'0002 1 0003 1\\\\\\\\n\\\\\\\')\\\\n-        f.write(\\\\\\\'0001 1 0003 1\\\\\\\\n\\\\\\\')\\\\n-        f.write(\\\\\\\'0002 1 2\\\\\\\\n\\\\\\\')\\\\n-        f.write(\\\\\\\'0001 2 0002 2\\\\\\\\n\\\\\\\')\\\\n-        f.write(\\\\\\\'0002 2 0003 2\\\\\\\\n\\\\\\\')\\\\n-        f.write(\\\\\\\'0001 2 0003 2\\\\\\\\n\\\\\\\')\\\\n-        f.write(\\\\\\\'0003 1 2\\\\\\\\n\\\\\\\')\\\\n-        f.write(\\\\\\\'0001 1 0002 2\\\\\\\\n\\\\\\\')\\\\n-        f.write(\\\\\\\'0002 1 0003 2\\\\\\\\n\\\\\\\')\\\\n-        f.write(\\\\\\\'0001 1 0003 2\\\\\\\\n\\\\\\\')\\\\n-    return pairs_filename\\\\n-\\\\n-if __name__ == "__main__":\\\\n-    unittest.main()\\\\n-    \\\\n\\\\\\\\ No newline at end of file\\\\ndiff --git a/test/triplet_loss_test.py b/test/triplet_loss_test.py\\\\ndeleted file mode 100644\\\\nindex 2648b30..0000000\\\\n--- a/test/triplet_loss_test.py\\\\n+++ /dev/null\\\\n@@ -1,54 +0,0 @@\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-import unittest\\\\n-import tensorflow as tf\\\\n-import numpy as np\\\\n-import facenet\\\\n-\\\\n-class DemuxEmbeddingsTest(unittest.TestCase):\\\\n-  \\\\n-    def testDemuxEmbeddings(self):\\\\n-        batch_size = 3*12\\\\n-        embedding_size = 16\\\\n-        alpha = 0.2\\\\n-        \\\\n-        with tf.Graph().as_default():\\\\n-        \\\\n-            embeddings = tf.placeholder(tf.float64, shape=(batch_size, embedding_size), name=\\\\\\\'embeddings\\\\\\\')\\\\n-            anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,embedding_size]), 3, 1)\\\\n-            triplet_loss = facenet.triplet_loss(anchor, positive, negative, alpha)\\\\n-                \\\\n-            sess = tf.Session()\\\\n-            with sess.as_default():\\\\n-                np.random.seed(seed=666)\\\\n-                emb = np.random.uniform(size=(batch_size, embedding_size))\\\\n-                tf_triplet_loss = sess.run(triplet_loss, feed_dict={embeddings:emb})\\\\n-\\\\n-                pos_dist_sqr = np.sum(np.square(emb[0::3,:]-emb[1::3,:]),1)\\\\n-                neg_dist_sqr = np.sum(np.square(emb[0::3,:]-emb[2::3,:]),1)\\\\n-                np_triplet_loss = np.mean(np.maximum(0.0, pos_dist_sqr - neg_dist_sqr + alpha))\\\\n-                \\\\n-                np.testing.assert_almost_equal(tf_triplet_loss, np_triplet_loss, decimal=5, err_msg=\\\\\\\'Triplet loss is incorrect\\\\\\\')\\\\n-                      \\\\n-if __name__ == "__main__":\\\\n-    unittest.main()\\\\ndiff --git a/video/a b/video/a\\\\ndeleted file mode 100644\\\\nindex 8b13789..0000000\\\\n--- a/video/a\\\\n+++ /dev/null\\\\n@@ -1 +0,0 @@\\\\n-\\\\ndiff --git a/video/camtest.mp4 b/video/camtest.mp4\\\\ndeleted file mode 100644\\\\nindex a503c89..0000000\\\\nBinary files a/video/camtest.mp4 and /dev/null differ\\\'\\n\\\\ No newline at end of file\\n+b\\\'diff --git a/.gitattributes b/.gitattributes\\\\ndeleted file mode 100644\\\\nindex 64e2803..0000000\\\\n--- a/.gitattributes\\\\n+++ /dev/null\\\\n@@ -1,3 +0,0 @@\\\\n-*.zip filter=lfs diff=lfs merge=lfs -text\\\\n-*.ckpt* filter=lfs diff=lfs merge=lfs -text\\\\n-*.pb filter=lfs diff=lfs merge=lfs -text\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3) copy.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3) copy.png\\\\ndeleted file mode 100644\\\\nindex 91be536..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3) copy.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3).png\\\\ndeleted file mode 100644\\\\nindex 91be536..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro.png\\\\ndeleted file mode 100644\\\\nindex 0540b28..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex e9ef9f0..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro.png\\\\ndeleted file mode 100644\\\\nindex 04ff255..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex 0c37c23..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (3).png\\\\ndeleted file mode 100644\\\\nindex 8e7842e..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (3).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro.png\\\\ndeleted file mode 100644\\\\nindex 91b18ff..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex 881520b..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (3).png\\\\ndeleted file mode 100644\\\\nindex 03ab7d3..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (3).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro.png\\\\ndeleted file mode 100644\\\\nindex fa1f42a..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex d4d9e24..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro.png\\\\ndeleted file mode 100644\\\\nindex 7467151..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_36_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_36_Pro.png\\\\ndeleted file mode 100644\\\\nindex 4336cd9..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_36_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex c3131df..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro.png\\\\ndeleted file mode 100644\\\\nindex e010c70..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex 2a5ed54..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (3).png\\\\ndeleted file mode 100644\\\\nindex 0d89ab7..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (3).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro.png\\\\ndeleted file mode 100644\\\\nindex 669d82a..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex 38b750a..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (3).png\\\\ndeleted file mode 100644\\\\nindex c35fb76..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (3).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro.png\\\\ndeleted file mode 100644\\\\nindex b1eb45c..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex b331c99..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (3).png\\\\ndeleted file mode 100644\\\\nindex 90c13a1..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (3).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro.png\\\\ndeleted file mode 100644\\\\nindex 49b56f8..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex 10d6da8..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro.png\\\\ndeleted file mode 100644\\\\nindex 731c604..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex ce6f384..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (3).png\\\\ndeleted file mode 100644\\\\nindex 93075b3..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (3).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro.png\\\\ndeleted file mode 100644\\\\nindex 2df0742..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex c609125..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (3).png\\\\ndeleted file mode 100644\\\\nindex 4fb46a0..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (3).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro.png\\\\ndeleted file mode 100644\\\\nindex ccacd60..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex 79dc7da..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro.png\\\\ndeleted file mode 100644\\\\nindex 77e1025..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex 81a3125..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (3).png\\\\ndeleted file mode 100644\\\\nindex 70fab4b..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (3).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro.png\\\\ndeleted file mode 100644\\\\nindex 628e2ce..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex df9dc1f..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro.png\\\\ndeleted file mode 100644\\\\nindex ec12b5d..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_22_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_22_Pro.png\\\\ndeleted file mode 100644\\\\nindex 93da5e6..0000000\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_22_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_54_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_54_Pro.png\\\\ndeleted file mode 100644\\\\nindex 5197b4f..0000000\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_54_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_55_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_55_Pro.png\\\\ndeleted file mode 100644\\\\nindex da2d35a..0000000\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_55_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro (2).png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex b458418..0000000\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro.png\\\\ndeleted file mode 100644\\\\nindex c9ded27..0000000\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro (2).png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex 12980b8..0000000\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro.png\\\\ndeleted file mode 100644\\\\nindex 36eeeb0..0000000\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro (2).png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex 3ca9822..0000000\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro.png\\\\ndeleted file mode 100644\\\\nindex 7a34d00..0000000\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_59_Pro (2).png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_59_Pro (2).png\\\\ndeleted file mode 100644\\\\nindex 5bfc7ed..0000000\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_59_Pro (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png b/Dataset/FaceData/processed/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png\\\\ndeleted file mode 100644\\\\nindex 8257ab5..0000000\\\\nBinary files a/Dataset/FaceData/processed/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png b/Dataset/FaceData/processed/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png\\\\ndeleted file mode 100644\\\\nindex 45870ff..0000000\\\\nBinary files a/Dataset/FaceData/processed/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png b/Dataset/FaceData/processed/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png\\\\ndeleted file mode 100644\\\\nindex f4593ba..0000000\\\\nBinary files a/Dataset/FaceData/processed/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/processed/bounding_boxes_75121.txt b/Dataset/FaceData/processed/bounding_boxes_75121.txt\\\\ndeleted file mode 100644\\\\nindex c490bf8..0000000\\\\n--- a/Dataset/FaceData/processed/bounding_boxes_75121.txt\\\\n+++ /dev/null\\\\n@@ -1,9 +0,0 @@\\\\n-Dataset/FaceData/processed\\\\\\\\Quang\\\\\\\\WIN_20250411_21_48_55_Pro.png 573 373 798 664\\\\n-Dataset/FaceData/processed\\\\\\\\Quang\\\\\\\\WIN_20250411_21_48_56_Pro (2).png 572 373 797 658\\\\n-Dataset/FaceData/processed\\\\\\\\Quang\\\\\\\\WIN_20250411_21_48_57_Pro (2).png 571 370 804 659\\\\n-Dataset/FaceData/processed\\\\\\\\Quang\\\\\\\\WIN_20250411_21_48_56_Pro.png 570 376 805 663\\\\n-Dataset/FaceData/processed\\\\\\\\Quang\\\\\\\\WIN_20250411_21_48_58_Pro.png 574 376 802 659\\\\n-Dataset/FaceData/processed\\\\\\\\Quang\\\\\\\\WIN_20250411_21_48_57_Pro.png 569 368 805 662\\\\n-Dataset/FaceData/processed\\\\\\\\Quang\\\\\\\\WIN_20250411_21_48_59_Pro (2).png 568 370 806 663\\\\n-Dataset/FaceData/processed\\\\\\\\Quang\\\\\\\\WIN_20250411_21_48_58_Pro (2).png 569 366 808 663\\\\n-Dataset/FaceData/processed\\\\\\\\Quang\\\\\\\\WIN_20250411_21_48_54_Pro.png 574 380 800 659\\\\ndiff --git a/Dataset/FaceData/processed/bounding_boxes_79144.txt b/Dataset/FaceData/processed/bounding_boxes_79144.txt\\\\ndeleted file mode 100644\\\\nindex ae1c6a8..0000000\\\\n--- a/Dataset/FaceData/processed/bounding_boxes_79144.txt\\\\n+++ /dev/null\\\\n@@ -1,46 +0,0 @@\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_21_Pro.png 506 335 795 706\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_18_Pro (2).png 516 299 808 645\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_18_Pro (3).png 517 303 794 608\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_07_34_Pro.png 569 297 864 704\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_15_Pro.png 493 335 781 698\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_20_Pro.png 521 329 813 711\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_07_33_Pro.png 498 331 803 720\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_15_Pro (2).png 490 340 775 691\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_20_Pro (3).png 518 353 824 720\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_18_Pro.png 521 310 822 683\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_14_Pro (3).png 506 334 807 720\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_13_Pro.png 527 330 824 707\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_15_Pro (3).png 501 325 799 702\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_07_35_Pro.png 491 307 794 681\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_19_Pro (2).png 523 313 803 651\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_07_32_Pro.png 474 344 776 720\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_16_Pro.png 515 323 812 708\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_07_36_Pro.png 482 331 798 720\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_07_32_Pro (2).png 495 343 788 720\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_12_Pro (2).png 529 332 824 708\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_17_Pro (2).png 581 334 858 709\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_16_Pro (2).png 534 325 821 709\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_07_34_Pro (3).png 526 302 825 681\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_07_34_Pro (2).png 563 299 860 702\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_21_Pro (2).png 496 333 771 680\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_19_Pro.png 521 316 793 615\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_22_Pro.png 540 299 826 682\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_13_Pro (3).png 519 341 815 715\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_20_Pro (2).png 526 369 812 720\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_07_33_Pro (2).png 518 321 825 720\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_14_Pro.png 522 338 806 720\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_14_Pro (2).png 517 343 817 720\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_17_Pro.png 574 326 856 720\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_07_35_Pro (2).png 481 331 800 720\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_12_Pro.png 518 322 811 699\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_07_31_Pro.png 521 336 819 720\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_13_Pro (2).png 523 332 818 710\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_07_33_Pro (3).png 532 314 834 719\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_07_31_Pro (3).png 505 324 815 720\\\\n-Dataset/FaceData/processed\\\\\\\\LVQuang\\\\\\\\WIN_20250331_09_26_17_Pro (3).png 561 315 848 708\\\\n-Dataset/FaceData/processed\\\\\\\\TranDangHieu\\\\\\\\download (1).png 262 672 767 1357\\\\n-Dataset/FaceData/processed\\\\\\\\TranDangHieu\\\\\\\\download (2).png 152 762 696 1478\\\\n-Dataset/FaceData/processed\\\\\\\\TranDangHieu\\\\\\\\download.png 202 705 750 1444\\\\n-Dataset/FaceData/processed\\\\\\\\Viruss\\\\\\\\Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png 50 121 926 1264\\\\n-Dataset/FaceData/processed\\\\\\\\Viruss\\\\\\\\Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png 154 108 1417 1799\\\\n-Dataset/FaceData/processed\\\\\\\\Viruss\\\\\\\\Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png 364 442 910 1148\\\\ndiff --git a/Dataset/FaceData/processed/revision_info.txt b/Dataset/FaceData/processed/revision_info.txt\\\\nindex f0f591b..01cf5cc 100644\\\\n--- a/Dataset/FaceData/processed/revision_info.txt\\\\n+++ b/Dataset/FaceData/processed/revision_info.txt\\\\n@@ -2,6 +2,6 @@ arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/proc\\\\n --------------------\\\\n tensorflow version: 2.19.0\\\\n --------------------\\\\n-git hash: b\\\\\\\'69ff1e149c0d84a123d6516ddd82970e65392608\\\\\\\'\\\\n+git hash: b\\\\\\\'860368ceb20db792b6b943d52ff5dfb99fcbb23b\\\\\\\'\\\\n --------------------\\\\n-b\\\\\\\'diff --git a/README.md b/README.md\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 16ec29c..0000000\\\\\\\\n--- a/README.md\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,11 +0,0 @@\\\\\\\\n-# MiAI_FaceRecog_3\\\\\\\\n-Nh\\\\\\\\xe1\\\\\\\\xba\\\\\\\\xadn di\\\\\\\\xe1\\\\\\\\xbb\\\\\\\\x87n khu\\\\\\\\xc3\\\\\\\\xb4n m\\\\\\\\xe1\\\\\\\\xba\\\\\\\\xb7t kh\\\\\\\\xc3\\\\\\\\xa1 chu\\\\\\\\xe1\\\\\\\\xba\\\\\\\\xa9n x\\\\\\\\xc3\\\\\\\\xa1c b\\\\\\\\xe1\\\\\\\\xba\\\\\\\\xb1ng MTCNN v\\\\\\\\xc3\\\\\\\\xa0 Facenet!\\\\\\\\n-Ch\\\\\\\\xe1\\\\\\\\xba\\\\\\\\xa1y tr\\\\\\\\xc3\\\\\\\\xaan Tensorflow 2.x\\\\\\\\n-\\\\\\\\n-Article link: http://miai.vn/2019/09/11/face-recog-2-0-nhan-dien-khuon-mat-trong-video-bang-mtcnn-va-facenet/\\\\\\\\n-\\\\\\\\n-#M\\\\\\\\xc3\\\\\\\\xacAI \\\\\\\\n-Fanpage: http://facebook.com/miaiblog<br>\\\\\\\\n-Group trao \\\\\\\\xc4\\\\\\\\x91\\\\\\\\xe1\\\\\\\\xbb\\\\\\\\x95i, chia s\\\\\\\\xe1\\\\\\\\xba\\\\\\\\xbb: https://www.facebook.com/groups/miaigroup<br>\\\\\\\\n-Website: http://ainoodle.tech<br>\\\\\\\\n-Youtube: http://bit.ly/miaiyoutube<br>\\\\\\\\ndiff --git a/src/a b/src/a\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 8b13789..0000000\\\\\\\\n--- a/src/a\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1 +0,0 @@\\\\\\\\n-\\\\\\\\ndiff --git a/src/calculate_filtering_metrics.py b/src/calculate_filtering_metrics.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex f60b9ae..0000000\\\\\\\\n--- a/src/calculate_filtering_metrics.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,128 +0,0 @@\\\\\\\\n-"""Calculate filtering metrics for a dataset and store in a .hdf file.\\\\\\\\n-"""\\\\\\\\n-# MIT License\\\\\\\\n-# \\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\n-# \\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\n-# \\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\n-# \\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\n-# SOFTWARE.\\\\\\\\n-\\\\\\\\n-from __future__ import absolute_import\\\\\\\\n-from __future__ import division\\\\\\\\n-from __future__ import print_function\\\\\\\\n-\\\\\\\\n-import tensorflow as tf\\\\\\\\n-import numpy as np\\\\\\\\n-import argparse\\\\\\\\n-import facenet\\\\\\\\n-import os\\\\\\\\n-import sys\\\\\\\\n-import time\\\\\\\\n-import h5py\\\\\\\\n-import math\\\\\\\\n-from tensorflow.python.platform import gfile\\\\\\\\n-from six import iteritems\\\\\\\\n-\\\\\\\\n-def main(args):\\\\\\\\n-    dataset = facenet.get_dataset(args.dataset_dir)\\\\\\\\n-  \\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\n-      \\\\\\\\n-        # Get a list of image paths and their labels\\\\\\\\n-        image_list, label_list = facenet.get_image_paths_and_labels(dataset)\\\\\\\\n-        nrof_images = len(image_list)\\\\\\\\n-        image_indices = range(nrof_images)\\\\\\\\n-\\\\\\\\n-        image_batch, label_batch = facenet.read_and_augment_data(image_list,\\\\\\\\n-            image_indices, args.image_size, args.batch_size, None, \\\\\\\\n-            False, False, False, nrof_preprocess_threads=4, shuffle=False)\\\\\\\\n-        \\\\\\\\n-        model_exp = os.path.expanduser(args.model_file)\\\\\\\\n-        with gfile.FastGFile(model_exp,\\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\') as f:\\\\\\\\n-            graph_def = tf.GraphDef()\\\\\\\\n-            graph_def.ParseFromString(f.read())\\\\\\\\n-            input_map={\\\\\\\\\\\\\\\'input\\\\\\\\\\\\\\\':image_batch, \\\\\\\\\\\\\\\'phase_train\\\\\\\\\\\\\\\':False}\\\\\\\\n-            tf.import_graph_def(graph_def, input_map=input_map, name=\\\\\\\\\\\\\\\'net\\\\\\\\\\\\\\\')\\\\\\\\n-        \\\\\\\\n-        embeddings = tf.get_default_graph().get_tensor_by_name("net/embeddings:0")\\\\\\\\n-\\\\\\\\n-        with tf.Session() as sess:\\\\\\\\n-            tf.train.start_queue_runners(sess=sess)\\\\\\\\n-                \\\\\\\\n-            embedding_size = int(embeddings.get_shape()[1])\\\\\\\\n-            nrof_batches = int(math.ceil(nrof_images / args.batch_size))\\\\\\\\n-            nrof_classes = len(dataset)\\\\\\\\n-            label_array = np.array(label_list)\\\\\\\\n-            class_names = [cls.name for cls in dataset]\\\\\\\\n-            nrof_examples_per_class = [ len(cls.image_paths) for cls in dataset ]\\\\\\\\n-            class_variance = np.zeros((nrof_classes,))\\\\\\\\n-            class_center = np.zeros((nrof_classes,embedding_size))\\\\\\\\n-            distance_to_center = np.ones((len(label_list),))*np.NaN\\\\\\\\n-            emb_array = np.zeros((0,embedding_size))\\\\\\\\n-            idx_array = np.zeros((0,), dtype=np.int32)\\\\\\\\n-            lab_array = np.zeros((0,), dtype=np.int32)\\\\\\\\n-            index_arr = np.append(0, np.cumsum(nrof_examples_per_class))\\\\\\\\n-            for i in range(nrof_batches):\\\\\\\\n-                t = time.time()\\\\\\\\n-                emb, idx = sess.run([embeddings, label_batch])\\\\\\\\n-                emb_array = np.append(emb_array, emb, axis=0)\\\\\\\\n-                idx_array = np.append(idx_array, idx, axis=0)\\\\\\\\n-                lab_array = np.append(lab_array, label_array[idx], axis=0)\\\\\\\\n-                for cls in set(lab_array):\\\\\\\\n-                    cls_idx = np.where(lab_array==cls)[0]\\\\\\\\n-                    if cls_idx.shape[0]==nrof_examples_per_class[cls]:\\\\\\\\n-                        # We have calculated all the embeddings for this class\\\\\\\\n-                        i2 = np.argsort(idx_array[cls_idx])\\\\\\\\n-                        emb_class = emb_array[cls_idx,:]\\\\\\\\n-                        emb_sort = emb_class[i2,:]\\\\\\\\n-                        center = np.mean(emb_sort, axis=0)\\\\\\\\n-                        diffs = emb_sort - center\\\\\\\\n-                        dists_sqr = np.sum(np.square(diffs), axis=1)\\\\\\\\n-                        class_variance[cls] = np.mean(dists_sqr)\\\\\\\\n-                        class_center[cls,:] = center\\\\\\\\n-                        distance_to_center[index_arr[cls]:index_arr[cls+1]] = np.sqrt(dists_sqr)\\\\\\\\n-                        emb_array = np.delete(emb_array, cls_idx, axis=0)\\\\\\\\n-                        idx_array = np.delete(idx_array, cls_idx, axis=0)\\\\\\\\n-                        lab_array = np.delete(lab_array, cls_idx, axis=0)\\\\\\\\n-\\\\\\\\n-                        \\\\\\\\n-                print(\\\\\\\\\\\\\\\'Batch %d in %.3f seconds\\\\\\\\\\\\\\\' % (i, time.time()-t))\\\\\\\\n-                \\\\\\\\n-            print(\\\\\\\\\\\\\\\'Writing filtering data to %s\\\\\\\\\\\\\\\' % args.data_file_name)\\\\\\\\n-            mdict = {\\\\\\\\\\\\\\\'class_names\\\\\\\\\\\\\\\':class_names, \\\\\\\\\\\\\\\'image_list\\\\\\\\\\\\\\\':image_list, \\\\\\\\\\\\\\\'label_list\\\\\\\\\\\\\\\':label_list, \\\\\\\\\\\\\\\'distance_to_center\\\\\\\\\\\\\\\':distance_to_center }\\\\\\\\n-            with h5py.File(args.data_file_name, \\\\\\\\\\\\\\\'w\\\\\\\\\\\\\\\') as f:\\\\\\\\n-                for key, value in iteritems(mdict):\\\\\\\\n-                    f.create_dataset(key, data=value)\\\\\\\\n-                        \\\\\\\\n-def parse_arguments(argv):\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\n-    \\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'dataset_dir\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Path to the directory containing aligned dataset.\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'model_file\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'File containing the frozen model in protobuf (.pb) format to use for feature extraction.\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'data_file_name\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'The name of the file to store filtering data in.\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--image_size\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Image size.\\\\\\\\\\\\\\\', default=160)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--batch_size\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of images to process in a batch.\\\\\\\\\\\\\\\', default=90)\\\\\\\\n-    return parser.parse_args(argv)\\\\\\\\n-\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\':\\\\\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\\\\\ndiff --git a/src/decode_msceleb_dataset.py b/src/decode_msceleb_dataset.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 4556bfa..0000000\\\\\\\\n--- a/src/decode_msceleb_dataset.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,87 +0,0 @@\\\\\\\\n-"""Decode the MsCelebV1 dataset in TSV (tab separated values) format downloaded from\\\\\\\\n-https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/\\\\\\\\n-"""\\\\\\\\n-# MIT License\\\\\\\\n-# \\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\n-# \\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\n-# \\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\n-# \\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\n-# SOFTWARE.\\\\\\\\n-\\\\\\\\n-from __future__ import absolute_import\\\\\\\\n-from __future__ import division\\\\\\\\n-from __future__ import print_function\\\\\\\\n-\\\\\\\\n-from scipy import misc\\\\\\\\n-import numpy as np\\\\\\\\n-import base64\\\\\\\\n-import sys\\\\\\\\n-import os\\\\\\\\n-import cv2\\\\\\\\n-import argparse\\\\\\\\n-import facenet\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-# File format: text files, each line is an image record containing 6 columns, delimited by TAB.\\\\\\\\n-# Column1: Freebase MID\\\\\\\\n-# Column2: Query/Name\\\\\\\\n-# Column3: ImageSearchRank\\\\\\\\n-# Column4: ImageURL\\\\\\\\n-# Column5: PageURL\\\\\\\\n-# Column6: ImageData_Base64Encoded\\\\\\\\n-\\\\\\\\n-def main(args):\\\\\\\\n-    output_dir = os.path.expanduser(args.output_dir)\\\\\\\\n-  \\\\\\\\n-    if not os.path.exists(output_dir):\\\\\\\\n-        os.mkdir(output_dir)\\\\\\\\n-  \\\\\\\\n-    # Store some git revision info in a text file in the output directory\\\\\\\\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\\\\\\\\n-    facenet.store_revision_info(src_path, output_dir, \\\\\\\\\\\\\\\' \\\\\\\\\\\\\\\'.join(sys.argv))\\\\\\\\n-    \\\\\\\\n-    i = 0\\\\\\\\n-    for f in args.tsv_files:\\\\\\\\n-        for line in f:\\\\\\\\n-            fields = line.split(\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\')\\\\\\\\n-            class_dir = fields[0]\\\\\\\\n-            img_name = fields[1] + \\\\\\\\\\\\\\\'-\\\\\\\\\\\\\\\' + fields[4] + \\\\\\\\\\\\\\\'.\\\\\\\\\\\\\\\' + args.output_format\\\\\\\\n-            img_string = fields[5]\\\\\\\\n-            img_dec_string = base64.b64decode(img_string)\\\\\\\\n-            img_data = np.fromstring(img_dec_string, dtype=np.uint8)\\\\\\\\n-            img = cv2.imdecode(img_data, cv2.IMREAD_COLOR) #pylint: disable=maybe-no-member\\\\\\\\n-            if args.size:\\\\\\\\n-                img = misc.imresize(img, (args.size, args.size), interp=\\\\\\\\\\\\\\\'bilinear\\\\\\\\\\\\\\\')\\\\\\\\n-            full_class_dir = os.path.join(output_dir, class_dir)\\\\\\\\n-            if not os.path.exists(full_class_dir):\\\\\\\\n-                os.mkdir(full_class_dir)\\\\\\\\n-            full_path = os.path.join(full_class_dir, img_name.replace(\\\\\\\\\\\\\\\'/\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\'_\\\\\\\\\\\\\\\'))\\\\\\\\n-            cv2.imwrite(full_path, img) #pylint: disable=maybe-no-member\\\\\\\\n-            print(\\\\\\\\\\\\\\\'%8d: %s\\\\\\\\\\\\\\\' % (i, full_path))\\\\\\\\n-            i += 1\\\\\\\\n-  \\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\':\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\n-\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'output_dir\\\\\\\\\\\\\\\', type=str, help=\\\\\\\\\\\\\\\'Output base directory for the image dataset\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'tsv_files\\\\\\\\\\\\\\\', type=argparse.FileType(\\\\\\\\\\\\\\\'r\\\\\\\\\\\\\\\'), nargs=\\\\\\\\\\\\\\\'+\\\\\\\\\\\\\\\', help=\\\\\\\\\\\\\\\'Input TSV file name(s)\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--size\\\\\\\\\\\\\\\', type=int, help=\\\\\\\\\\\\\\\'Images are resized to the given size\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--output_format\\\\\\\\\\\\\\\', type=str, help=\\\\\\\\\\\\\\\'Format of the output images\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\'png\\\\\\\\\\\\\\\', choices=[\\\\\\\\\\\\\\\'png\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'jpg\\\\\\\\\\\\\\\'])\\\\\\\\n-\\\\\\\\n-    main(parser.parse_args())\\\\\\\\n-\\\\\\\\ndiff --git a/src/download_and_extract.py b/src/download_and_extract.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex a835ac2..0000000\\\\\\\\n--- a/src/download_and_extract.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,51 +0,0 @@\\\\\\\\n-import requests\\\\\\\\n-import zipfile\\\\\\\\n-import os\\\\\\\\n-\\\\\\\\n-model_dict = {\\\\\\\\n-    \\\\\\\\\\\\\\\'lfw-subset\\\\\\\\\\\\\\\':      \\\\\\\\\\\\\\\'1B5BQUZuJO-paxdN8UclxeHAR1WnR_Tzi\\\\\\\\\\\\\\\', \\\\\\\\n-    \\\\\\\\\\\\\\\'20170131-234652\\\\\\\\\\\\\\\': \\\\\\\\\\\\\\\'0B5MzpY9kBtDVSGM0RmVET2EwVEk\\\\\\\\\\\\\\\',\\\\\\\\n-    \\\\\\\\\\\\\\\'20170216-091149\\\\\\\\\\\\\\\': \\\\\\\\\\\\\\\'0B5MzpY9kBtDVTGZjcWkzT3pldDA\\\\\\\\\\\\\\\',\\\\\\\\n-    \\\\\\\\\\\\\\\'20170512-110547\\\\\\\\\\\\\\\': \\\\\\\\\\\\\\\'0B5MzpY9kBtDVZ2RpVDYwWmxoSUk\\\\\\\\\\\\\\\',\\\\\\\\n-    \\\\\\\\\\\\\\\'20180402-114759\\\\\\\\\\\\\\\': \\\\\\\\\\\\\\\'1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-\\\\\\\\\\\\\\\'\\\\\\\\n-    }\\\\\\\\n-\\\\\\\\n-def download_and_extract_file(model_name, data_dir):\\\\\\\\n-    file_id = model_dict[model_name]\\\\\\\\n-    destination = os.path.join(data_dir, model_name + \\\\\\\\\\\\\\\'.zip\\\\\\\\\\\\\\\')\\\\\\\\n-    if not os.path.exists(destination):\\\\\\\\n-        print(\\\\\\\\\\\\\\\'Downloading file to %s\\\\\\\\\\\\\\\' % destination)\\\\\\\\n-        download_file_from_google_drive(file_id, destination)\\\\\\\\n-        with zipfile.ZipFile(destination, \\\\\\\\\\\\\\\'r\\\\\\\\\\\\\\\') as zip_ref:\\\\\\\\n-            print(\\\\\\\\\\\\\\\'Extracting file to %s\\\\\\\\\\\\\\\' % data_dir)\\\\\\\\n-            zip_ref.extractall(data_dir)\\\\\\\\n-\\\\\\\\n-def download_file_from_google_drive(file_id, destination):\\\\\\\\n-    \\\\\\\\n-        URL = "https://drive.google.com/uc?export=download"\\\\\\\\n-    \\\\\\\\n-        session = requests.Session()\\\\\\\\n-    \\\\\\\\n-        response = session.get(URL, params = { \\\\\\\\\\\\\\\'id\\\\\\\\\\\\\\\' : file_id }, stream = True)\\\\\\\\n-        token = get_confirm_token(response)\\\\\\\\n-    \\\\\\\\n-        if token:\\\\\\\\n-            params = { \\\\\\\\\\\\\\\'id\\\\\\\\\\\\\\\' : file_id, \\\\\\\\\\\\\\\'confirm\\\\\\\\\\\\\\\' : token }\\\\\\\\n-            response = session.get(URL, params = params, stream = True)\\\\\\\\n-    \\\\\\\\n-        save_response_content(response, destination)    \\\\\\\\n-\\\\\\\\n-def get_confirm_token(response):\\\\\\\\n-    for key, value in response.cookies.items():\\\\\\\\n-        if key.startswith(\\\\\\\\\\\\\\\'download_warning\\\\\\\\\\\\\\\'):\\\\\\\\n-            return value\\\\\\\\n-\\\\\\\\n-    return None\\\\\\\\n-\\\\\\\\n-def save_response_content(response, destination):\\\\\\\\n-    CHUNK_SIZE = 32768\\\\\\\\n-\\\\\\\\n-    with open(destination, "wb") as f:\\\\\\\\n-        for chunk in response.iter_content(CHUNK_SIZE):\\\\\\\\n-            if chunk: # filter out keep-alive new chunks\\\\\\\\n-                f.write(chunk)\\\\\\\\ndiff --git a/src/face_rec.py b/src/face_rec.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex f92cccf..0000000\\\\\\\\n--- a/src/face_rec.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,135 +0,0 @@\\\\\\\\n-from __future__ import absolute_import\\\\\\\\n-from __future__ import division\\\\\\\\n-from __future__ import print_function\\\\\\\\n-\\\\\\\\n-import tensorflow as tf\\\\\\\\n-import argparse\\\\\\\\n-import facenet\\\\\\\\n-import os\\\\\\\\n-import sys\\\\\\\\n-import math\\\\\\\\n-import pickle\\\\\\\\n-import align.detect_face\\\\\\\\n-import numpy as np\\\\\\\\n-import cv2\\\\\\\\n-import collections\\\\\\\\n-from sklearn.svm import SVC\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-def main():\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--path\\\\\\\\\\\\\\\', help=\\\\\\\\\\\\\\\'Path of the video you want to test on.\\\\\\\\\\\\\\\', default=0)\\\\\\\\n-    args = parser.parse_args()\\\\\\\\n-    \\\\\\\\n-    # Cai dat cac tham so can thiet\\\\\\\\n-    MINSIZE = 20\\\\\\\\n-    THRESHOLD = [0.6, 0.7, 0.7]\\\\\\\\n-    FACTOR = 0.709\\\\\\\\n-    IMAGE_SIZE = 182\\\\\\\\n-    INPUT_IMAGE_SIZE = 160\\\\\\\\n-    CLASSIFIER_PATH = \\\\\\\\\\\\\\\'Models/facemodel.pkl\\\\\\\\\\\\\\\'\\\\\\\\n-    VIDEO_PATH = args.path\\\\\\\\n-    FACENET_MODEL_PATH = \\\\\\\\\\\\\\\'Models/20180402-114759.pb\\\\\\\\\\\\\\\'\\\\\\\\n-\\\\\\\\n-    # Load model da train de nhan dien khuon mat - thuc chat la classifier\\\\\\\\n-    with open(CLASSIFIER_PATH, \\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\') as file:\\\\\\\\n-        model, class_names = pickle.load(file)\\\\\\\\n-    print("Custom Classifier, Successfully loaded")\\\\\\\\n-\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\n-\\\\\\\\n-        # Cai dat GPU neu co\\\\\\\\n-        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\\\\\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\\\\\n-\\\\\\\\n-        with sess.as_default():\\\\\\\\n-\\\\\\\\n-            # Load model MTCNN phat hien khuon mat\\\\\\\\n-            print(\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\')\\\\\\\\n-            facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\n-\\\\\\\\n-            # Lay tensor input va output\\\\\\\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\n-            embedding_size = embeddings.get_shape()[1]\\\\\\\\n-\\\\\\\\n-            # Cai dat cac mang con\\\\\\\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\\\\\n-\\\\\\\\n-            people_detected = set()\\\\\\\\n-            person_detected = collections.Counter()\\\\\\\\n-\\\\\\\\n-            # Lay hinh anh tu file video\\\\\\\\n-            cap = cv2.VideoCapture(VIDEO_PATH)\\\\\\\\n-\\\\\\\\n-            while (cap.isOpened()):\\\\\\\\n-                # Doc tung frame\\\\\\\\n-                ret, frame = cap.read()\\\\\\\\n-\\\\\\\\n-                # Phat hien khuon mat, tra ve vi tri trong bounding_boxes\\\\\\\\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\n-\\\\\\\\n-                faces_found = bounding_boxes.shape[0]\\\\\\\\n-                try:\\\\\\\\n-                    # Neu co it nhat 1 khuon mat trong frame\\\\\\\\n-                    if faces_found > 0:\\\\\\\\n-                        det = bounding_boxes[:, 0:4]\\\\\\\\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\n-                        for i in range(faces_found):\\\\\\\\n-                            bb[i][0] = det[i][0]\\\\\\\\n-                            bb[i][1] = det[i][1]\\\\\\\\n-                            bb[i][2] = det[i][2]\\\\\\\\n-                            bb[i][3] = det[i][3]\\\\\\\\n-\\\\\\\\n-                            # Cat phan khuon mat tim duoc\\\\\\\\n-                            cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\n-                            scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\\\\\n-                                                interpolation=cv2.INTER_CUBIC)\\\\\\\\n-                            scaled = facenet.prewhiten(scaled)\\\\\\\\n-                            scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\n-                            feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\n-                            emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\n-                            \\\\\\\\n-                            # Dua vao model de classifier\\\\\\\\n-                            predictions = model.predict_proba(emb_array)\\\\\\\\n-                            best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\n-                            best_class_probabilities = predictions[\\\\\\\\n-                                np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\n-                            \\\\\\\\n-                            # Lay ra ten va ty le % cua class co ty le cao nhat\\\\\\\\n-                            best_name = class_names[best_class_indices[0]]\\\\\\\\n-                            print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\\\\\n-\\\\\\\\n-                            # Ve khung mau xanh quanh khuon mat\\\\\\\\n-                            cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\\\\\\\n-                            text_x = bb[i][0]\\\\\\\\n-                            text_y = bb[i][3] + 20\\\\\\\\n-\\\\\\\\n-                            # Neu ty le nhan dang > 0.5 thi hien thi ten\\\\\\\\n-                            if best_class_probabilities > 0.5:\\\\\\\\n-                                name = class_names[best_class_indices[0]]\\\\\\\\n-                            else:\\\\\\\\n-                                # Con neu <=0.5 thi hien thi Unknow\\\\\\\\n-                                name = "Unknown"\\\\\\\\n-                                \\\\\\\\n-                            # Viet text len tren frame    \\\\\\\\n-                            cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\n-                                        1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\n-                            cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\\\\\\\\n-                                        cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\n-                                        1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\n-                            person_detected[best_name] += 1\\\\\\\\n-                except:\\\\\\\\n-                    pass\\\\\\\\n-\\\\\\\\n-                # Hien thi frame len man hinh\\\\\\\\n-                cv2.imshow(\\\\\\\\\\\\\\\'Face Recognition\\\\\\\\\\\\\\\', frame)\\\\\\\\n-                if cv2.waitKey(1) & 0xFF == ord(\\\\\\\\\\\\\\\'q\\\\\\\\\\\\\\\'):\\\\\\\\n-                    break\\\\\\\\n-\\\\\\\\n-            cap.release()\\\\\\\\n-            cv2.destroyAllWindows()\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-main()\\\\\\\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\\\\\\\nindex 1a425a5..455f67a 100644\\\\\\\\n--- a/src/face_rec_cam.py\\\\\\\\n+++ b/src/face_rec_cam.py\\\\\\\\n@@ -52,9 +52,10 @@ def main():\\\\\\\\n             facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\n \\\\\\\\n             # Get input and output tensors\\\\\\\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\n+            graph = tf.compat.v1.get_default_graph()\\\\\\\\n+            images_placeholder = graph.get_tensor_by_name("input:0")\\\\\\\\n+            embeddings = graph.get_tensor_by_name("embeddings:0")\\\\\\\\n+            phase_train_placeholder = graph.get_tensor_by_name("phase_train:0")\\\\\\\\n             embedding_size = embeddings.get_shape()[1]\\\\\\\\n \\\\\\\\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\\\\\ndiff --git a/src/freeze_graph.py b/src/freeze_graph.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 3584c18..0000000\\\\\\\\n--- a/src/freeze_graph.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,103 +0,0 @@\\\\\\\\n-"""Imports a model metagraph and checkpoint file, converts the variables to constants\\\\\\\\n-and exports the model as a graphdef protobuf\\\\\\\\n-"""\\\\\\\\n-# MIT License\\\\\\\\n-# \\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\n-# \\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\n-# \\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\n-# \\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\n-# SOFTWARE.\\\\\\\\n-\\\\\\\\n-from __future__ import absolute_import\\\\\\\\n-from __future__ import division\\\\\\\\n-from __future__ import print_function\\\\\\\\n-\\\\\\\\n-from tensorflow.python.framework import graph_util\\\\\\\\n-import tensorflow as tf\\\\\\\\n-import argparse\\\\\\\\n-import os\\\\\\\\n-import sys\\\\\\\\n-import facenet\\\\\\\\n-from six.moves import xrange  # @UnresolvedImport\\\\\\\\n-\\\\\\\\n-def main(args):\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\n-        with tf.Session() as sess:\\\\\\\\n-            # Load the model metagraph and checkpoint\\\\\\\\n-            print(\\\\\\\\\\\\\\\'Model directory: %s\\\\\\\\\\\\\\\' % args.model_dir)\\\\\\\\n-            meta_file, ckpt_file = facenet.get_model_filenames(os.path.expanduser(args.model_dir))\\\\\\\\n-            \\\\\\\\n-            print(\\\\\\\\\\\\\\\'Metagraph file: %s\\\\\\\\\\\\\\\' % meta_file)\\\\\\\\n-            print(\\\\\\\\\\\\\\\'Checkpoint file: %s\\\\\\\\\\\\\\\' % ckpt_file)\\\\\\\\n-\\\\\\\\n-            model_dir_exp = os.path.expanduser(args.model_dir)\\\\\\\\n-            saver = tf.train.import_meta_graph(os.path.join(model_dir_exp, meta_file), clear_devices=True)\\\\\\\\n-            tf.get_default_session().run(tf.global_variables_initializer())\\\\\\\\n-            tf.get_default_session().run(tf.local_variables_initializer())\\\\\\\\n-            saver.restore(tf.get_default_session(), os.path.join(model_dir_exp, ckpt_file))\\\\\\\\n-            \\\\\\\\n-            # Retrieve the protobuf graph definition and fix the batch norm nodes\\\\\\\\n-            input_graph_def = sess.graph.as_graph_def()\\\\\\\\n-            \\\\\\\\n-            # Freeze the graph def\\\\\\\\n-            output_graph_def = freeze_graph_def(sess, input_graph_def, \\\\\\\\\\\\\\\'embeddings,label_batch\\\\\\\\\\\\\\\')\\\\\\\\n-\\\\\\\\n-        # Serialize and dump the output graph to the filesystem\\\\\\\\n-        with tf.gfile.GFile(args.output_file, \\\\\\\\\\\\\\\'wb\\\\\\\\\\\\\\\') as f:\\\\\\\\n-            f.write(output_graph_def.SerializeToString())\\\\\\\\n-        print("%d ops in the final graph: %s" % (len(output_graph_def.node), args.output_file))\\\\\\\\n-        \\\\\\\\n-def freeze_graph_def(sess, input_graph_def, output_node_names):\\\\\\\\n-    for node in input_graph_def.node:\\\\\\\\n-        if node.op == \\\\\\\\\\\\\\\'RefSwitch\\\\\\\\\\\\\\\':\\\\\\\\n-            node.op = \\\\\\\\\\\\\\\'Switch\\\\\\\\\\\\\\\'\\\\\\\\n-            for index in xrange(len(node.input)):\\\\\\\\n-                if \\\\\\\\\\\\\\\'moving_\\\\\\\\\\\\\\\' in node.input[index]:\\\\\\\\n-                    node.input[index] = node.input[index] + \\\\\\\\\\\\\\\'/read\\\\\\\\\\\\\\\'\\\\\\\\n-        elif node.op == \\\\\\\\\\\\\\\'AssignSub\\\\\\\\\\\\\\\':\\\\\\\\n-            node.op = \\\\\\\\\\\\\\\'Sub\\\\\\\\\\\\\\\'\\\\\\\\n-            if \\\\\\\\\\\\\\\'use_locking\\\\\\\\\\\\\\\' in node.attr: del node.attr[\\\\\\\\\\\\\\\'use_locking\\\\\\\\\\\\\\\']\\\\\\\\n-        elif node.op == \\\\\\\\\\\\\\\'AssignAdd\\\\\\\\\\\\\\\':\\\\\\\\n-            node.op = \\\\\\\\\\\\\\\'Add\\\\\\\\\\\\\\\'\\\\\\\\n-            if \\\\\\\\\\\\\\\'use_locking\\\\\\\\\\\\\\\' in node.attr: del node.attr[\\\\\\\\\\\\\\\'use_locking\\\\\\\\\\\\\\\']\\\\\\\\n-    \\\\\\\\n-    # Get the list of important nodes\\\\\\\\n-    whitelist_names = []\\\\\\\\n-    for node in input_graph_def.node:\\\\\\\\n-        if (node.name.startswith(\\\\\\\\\\\\\\\'InceptionResnet\\\\\\\\\\\\\\\') or node.name.startswith(\\\\\\\\\\\\\\\'embeddings\\\\\\\\\\\\\\\') or \\\\\\\\n-                node.name.startswith(\\\\\\\\\\\\\\\'image_batch\\\\\\\\\\\\\\\') or node.name.startswith(\\\\\\\\\\\\\\\'label_batch\\\\\\\\\\\\\\\') or\\\\\\\\n-                node.name.startswith(\\\\\\\\\\\\\\\'phase_train\\\\\\\\\\\\\\\') or node.name.startswith(\\\\\\\\\\\\\\\'Logits\\\\\\\\\\\\\\\')):\\\\\\\\n-            whitelist_names.append(node.name)\\\\\\\\n-\\\\\\\\n-    # Replace all the variables in the graph with constants of the same values\\\\\\\\n-    output_graph_def = graph_util.convert_variables_to_constants(\\\\\\\\n-        sess, input_graph_def, output_node_names.split(","),\\\\\\\\n-        variable_names_whitelist=whitelist_names)\\\\\\\\n-    return output_graph_def\\\\\\\\n-  \\\\\\\\n-def parse_arguments(argv):\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\n-    \\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'model_dir\\\\\\\\\\\\\\\', type=str, \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Directory containing the metagraph (.meta) file and the checkpoint (ckpt) file containing model parameters\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'output_file\\\\\\\\\\\\\\\', type=str, \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Filename for the exported graphdef protobuf (.pb)\\\\\\\\\\\\\\\')\\\\\\\\n-    return parser.parse_args(argv)\\\\\\\\n-\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\':\\\\\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\\\\\ndiff --git a/src/lfw.py b/src/lfw.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 9194433..0000000\\\\\\\\n--- a/src/lfw.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,86 +0,0 @@\\\\\\\\n-"""Helper for evaluation on the Labeled Faces in the Wild dataset \\\\\\\\n-"""\\\\\\\\n-\\\\\\\\n-# MIT License\\\\\\\\n-# \\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\n-# \\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\n-# \\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\n-# \\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\n-# SOFTWARE.\\\\\\\\n-\\\\\\\\n-from __future__ import absolute_import\\\\\\\\n-from __future__ import division\\\\\\\\n-from __future__ import print_function\\\\\\\\n-\\\\\\\\n-import os\\\\\\\\n-import numpy as np\\\\\\\\n-import facenet\\\\\\\\n-\\\\\\\\n-def evaluate(embeddings, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\\\\\\\\n-    # Calculate evaluation metrics\\\\\\\\n-    thresholds = np.arange(0, 4, 0.01)\\\\\\\\n-    embeddings1 = embeddings[0::2]\\\\\\\\n-    embeddings2 = embeddings[1::2]\\\\\\\\n-    tpr, fpr, accuracy = facenet.calculate_roc(thresholds, embeddings1, embeddings2,\\\\\\\\n-        np.asarray(actual_issame), nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\\\\\\\\n-    thresholds = np.arange(0, 4, 0.001)\\\\\\\\n-    val, val_std, far = facenet.calculate_val(thresholds, embeddings1, embeddings2,\\\\\\\\n-        np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\\\\\\\\n-    return tpr, fpr, accuracy, val, val_std, far\\\\\\\\n-\\\\\\\\n-def get_paths(lfw_dir, pairs):\\\\\\\\n-    nrof_skipped_pairs = 0\\\\\\\\n-    path_list = []\\\\\\\\n-    issame_list = []\\\\\\\\n-    for pair in pairs:\\\\\\\\n-        if len(pair) == 3:\\\\\\\\n-            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \\\\\\\\\\\\\\\'_\\\\\\\\\\\\\\\' + \\\\\\\\\\\\\\\'%04d\\\\\\\\\\\\\\\' % int(pair[1])))\\\\\\\\n-            path1 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \\\\\\\\\\\\\\\'_\\\\\\\\\\\\\\\' + \\\\\\\\\\\\\\\'%04d\\\\\\\\\\\\\\\' % int(pair[2])))\\\\\\\\n-            issame = True\\\\\\\\n-        elif len(pair) == 4:\\\\\\\\n-            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \\\\\\\\\\\\\\\'_\\\\\\\\\\\\\\\' + \\\\\\\\\\\\\\\'%04d\\\\\\\\\\\\\\\' % int(pair[1])))\\\\\\\\n-            path1 = add_extension(os.path.join(lfw_dir, pair[2], pair[2] + \\\\\\\\\\\\\\\'_\\\\\\\\\\\\\\\' + \\\\\\\\\\\\\\\'%04d\\\\\\\\\\\\\\\' % int(pair[3])))\\\\\\\\n-            issame = False\\\\\\\\n-        if os.path.exists(path0) and os.path.exists(path1):    # Only add the pair if both paths exist\\\\\\\\n-            path_list += (path0,path1)\\\\\\\\n-            issame_list.append(issame)\\\\\\\\n-        else:\\\\\\\\n-            nrof_skipped_pairs += 1\\\\\\\\n-    if nrof_skipped_pairs>0:\\\\\\\\n-        print(\\\\\\\\\\\\\\\'Skipped %d image pairs\\\\\\\\\\\\\\\' % nrof_skipped_pairs)\\\\\\\\n-    \\\\\\\\n-    return path_list, issame_list\\\\\\\\n-  \\\\\\\\n-def add_extension(path):\\\\\\\\n-    if os.path.exists(path+\\\\\\\\\\\\\\\'.jpg\\\\\\\\\\\\\\\'):\\\\\\\\n-        return path+\\\\\\\\\\\\\\\'.jpg\\\\\\\\\\\\\\\'\\\\\\\\n-    elif os.path.exists(path+\\\\\\\\\\\\\\\'.png\\\\\\\\\\\\\\\'):\\\\\\\\n-        return path+\\\\\\\\\\\\\\\'.png\\\\\\\\\\\\\\\'\\\\\\\\n-    else:\\\\\\\\n-        raise RuntimeError(\\\\\\\\\\\\\\\'No file "%s" with extension png or jpg.\\\\\\\\\\\\\\\' % path)\\\\\\\\n-\\\\\\\\n-def read_pairs(pairs_filename):\\\\\\\\n-    pairs = []\\\\\\\\n-    with open(pairs_filename, \\\\\\\\\\\\\\\'r\\\\\\\\\\\\\\\') as f:\\\\\\\\n-        for line in f.readlines()[1:]:\\\\\\\\n-            pair = line.strip().split()\\\\\\\\n-            pairs.append(pair)\\\\\\\\n-    return np.array(pairs)\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-\\\\\\\\ndiff --git a/src/train_softmax.py b/src/train_softmax.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 6b0b28b..0000000\\\\\\\\n--- a/src/train_softmax.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,580 +0,0 @@\\\\\\\\n-"""Training a face recognizer with TensorFlow using softmax cross entropy loss\\\\\\\\n-"""\\\\\\\\n-# MIT License\\\\\\\\n-# \\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\n-# \\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\n-# \\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\n-# \\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\n-# SOFTWARE.\\\\\\\\n-\\\\\\\\n-from __future__ import absolute_import\\\\\\\\n-from __future__ import division\\\\\\\\n-from __future__ import print_function\\\\\\\\n-\\\\\\\\n-from datetime import datetime\\\\\\\\n-import os.path\\\\\\\\n-import time\\\\\\\\n-import sys\\\\\\\\n-import random\\\\\\\\n-import tensorflow as tf\\\\\\\\n-import numpy as np\\\\\\\\n-import importlib\\\\\\\\n-import argparse\\\\\\\\n-import facenet\\\\\\\\n-import lfw\\\\\\\\n-import h5py\\\\\\\\n-import math\\\\\\\\n-import tensorflow.contrib.slim as slim\\\\\\\\n-from tensorflow.python.ops import data_flow_ops\\\\\\\\n-from tensorflow.python.framework import ops\\\\\\\\n-from tensorflow.python.ops import array_ops\\\\\\\\n-\\\\\\\\n-def main(args):\\\\\\\\n-  \\\\\\\\n-    network = importlib.import_module(args.model_def)\\\\\\\\n-    image_size = (args.image_size, args.image_size)\\\\\\\\n-\\\\\\\\n-    subdir = datetime.strftime(datetime.now(), \\\\\\\\\\\\\\\'%Y%m%d-%H%M%S\\\\\\\\\\\\\\\')\\\\\\\\n-    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\\\\\\\\n-    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\\\\\\\\\\\\\\\'t exist\\\\\\\\n-        os.makedirs(log_dir)\\\\\\\\n-    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\\\\\\\\n-    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\\\\\\\\\\\\\\\'t exist\\\\\\\\n-        os.makedirs(model_dir)\\\\\\\\n-\\\\\\\\n-    stat_file_name = os.path.join(log_dir, \\\\\\\\\\\\\\\'stat.h5\\\\\\\\\\\\\\\')\\\\\\\\n-\\\\\\\\n-    # Write arguments to a text file\\\\\\\\n-    facenet.write_arguments_to_file(args, os.path.join(log_dir, \\\\\\\\\\\\\\\'arguments.txt\\\\\\\\\\\\\\\'))\\\\\\\\n-        \\\\\\\\n-    # Store some git revision info in a text file in the log directory\\\\\\\\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\\\\\\\\n-    facenet.store_revision_info(src_path, log_dir, \\\\\\\\\\\\\\\' \\\\\\\\\\\\\\\'.join(sys.argv))\\\\\\\\n-\\\\\\\\n-    np.random.seed(seed=args.seed)\\\\\\\\n-    random.seed(args.seed)\\\\\\\\n-    dataset = facenet.get_dataset(args.data_dir)\\\\\\\\n-    if args.filter_filename:\\\\\\\\n-        dataset = filter_dataset(dataset, os.path.expanduser(args.filter_filename), \\\\\\\\n-            args.filter_percentile, args.filter_min_nrof_images_per_class)\\\\\\\\n-        \\\\\\\\n-    if args.validation_set_split_ratio>0.0:\\\\\\\\n-        train_set, val_set = facenet.split_dataset(dataset, args.validation_set_split_ratio, args.min_nrof_val_images_per_class, \\\\\\\\\\\\\\\'SPLIT_IMAGES\\\\\\\\\\\\\\\')\\\\\\\\n-    else:\\\\\\\\n-        train_set, val_set = dataset, []\\\\\\\\n-        \\\\\\\\n-    nrof_classes = len(train_set)\\\\\\\\n-    \\\\\\\\n-    print(\\\\\\\\\\\\\\\'Model directory: %s\\\\\\\\\\\\\\\' % model_dir)\\\\\\\\n-    print(\\\\\\\\\\\\\\\'Log directory: %s\\\\\\\\\\\\\\\' % log_dir)\\\\\\\\n-    pretrained_model = None\\\\\\\\n-    if args.pretrained_model:\\\\\\\\n-        pretrained_model = os.path.expanduser(args.pretrained_model)\\\\\\\\n-        print(\\\\\\\\\\\\\\\'Pre-trained model: %s\\\\\\\\\\\\\\\' % pretrained_model)\\\\\\\\n-    \\\\\\\\n-    if args.lfw_dir:\\\\\\\\n-        print(\\\\\\\\\\\\\\\'LFW directory: %s\\\\\\\\\\\\\\\' % args.lfw_dir)\\\\\\\\n-        # Read the file containing the pairs used for testing\\\\\\\\n-        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\\\\\\\\n-        # Get the paths for the corresponding images\\\\\\\\n-        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\\\\\\\\n-    \\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\n-        tf.set_random_seed(args.seed)\\\\\\\\n-        global_step = tf.Variable(0, trainable=False)\\\\\\\\n-        \\\\\\\\n-        # Get a list of image paths and their labels\\\\\\\\n-        image_list, label_list = facenet.get_image_paths_and_labels(train_set)\\\\\\\\n-        assert len(image_list)>0, \\\\\\\\\\\\\\\'The training set should not be empty\\\\\\\\\\\\\\\'\\\\\\\\n-        \\\\\\\\n-        val_image_list, val_label_list = facenet.get_image_paths_and_labels(val_set)\\\\\\\\n-\\\\\\\\n-        # Create a queue that produces indices into the image_list and label_list \\\\\\\\n-        labels = ops.convert_to_tensor(label_list, dtype=tf.int32)\\\\\\\\n-        range_size = array_ops.shape(labels)[0]\\\\\\\\n-        index_queue = tf.train.range_input_producer(range_size, num_epochs=None,\\\\\\\\n-                             shuffle=True, seed=None, capacity=32)\\\\\\\\n-        \\\\\\\\n-        index_dequeue_op = index_queue.dequeue_many(args.batch_size*args.epoch_size, \\\\\\\\\\\\\\\'index_dequeue\\\\\\\\\\\\\\\')\\\\\\\\n-        \\\\\\\\n-        learning_rate_placeholder = tf.placeholder(tf.float32, name=\\\\\\\\\\\\\\\'learning_rate\\\\\\\\\\\\\\\')\\\\\\\\n-        batch_size_placeholder = tf.placeholder(tf.int32, name=\\\\\\\\\\\\\\\'batch_size\\\\\\\\\\\\\\\')\\\\\\\\n-        phase_train_placeholder = tf.placeholder(tf.bool, name=\\\\\\\\\\\\\\\'phase_train\\\\\\\\\\\\\\\')\\\\\\\\n-        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\\\\\\\\\\\\\\\'image_paths\\\\\\\\\\\\\\\')\\\\\\\\n-        labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\\\\\\\\\\\\\\\'labels\\\\\\\\\\\\\\\')\\\\\\\\n-        control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\\\\\\\\\\\\\\\'control\\\\\\\\\\\\\\\')\\\\\\\\n-        \\\\\\\\n-        nrof_preprocess_threads = 4\\\\\\\\n-        input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\\\\\\\\n-                                    dtypes=[tf.string, tf.int32, tf.int32],\\\\\\\\n-                                    shapes=[(1,), (1,), (1,)],\\\\\\\\n-                                    shared_name=None, name=None)\\\\\\\\n-        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder, control_placeholder], name=\\\\\\\\\\\\\\\'enqueue_op\\\\\\\\\\\\\\\')\\\\\\\\n-        image_batch, label_batch = facenet.create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder)\\\\\\\\n-\\\\\\\\n-        image_batch = tf.identity(image_batch, \\\\\\\\\\\\\\\'image_batch\\\\\\\\\\\\\\\')\\\\\\\\n-        image_batch = tf.identity(image_batch, \\\\\\\\\\\\\\\'input\\\\\\\\\\\\\\\')\\\\\\\\n-        label_batch = tf.identity(label_batch, \\\\\\\\\\\\\\\'label_batch\\\\\\\\\\\\\\\')\\\\\\\\n-        \\\\\\\\n-        print(\\\\\\\\\\\\\\\'Number of classes in training set: %d\\\\\\\\\\\\\\\' % nrof_classes)\\\\\\\\n-        print(\\\\\\\\\\\\\\\'Number of examples in training set: %d\\\\\\\\\\\\\\\' % len(image_list))\\\\\\\\n-\\\\\\\\n-        print(\\\\\\\\\\\\\\\'Number of classes in validation set: %d\\\\\\\\\\\\\\\' % len(val_set))\\\\\\\\n-        print(\\\\\\\\\\\\\\\'Number of examples in validation set: %d\\\\\\\\\\\\\\\' % len(val_image_list))\\\\\\\\n-        \\\\\\\\n-        print(\\\\\\\\\\\\\\\'Building training graph\\\\\\\\\\\\\\\')\\\\\\\\n-        \\\\\\\\n-        # Build the inference graph\\\\\\\\n-        prelogits, _ = network.inference(image_batch, args.keep_probability, \\\\\\\\n-            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size, \\\\\\\\n-            weight_decay=args.weight_decay)\\\\\\\\n-        logits = slim.fully_connected(prelogits, len(train_set), activation_fn=None, \\\\\\\\n-                weights_initializer=slim.initializers.xavier_initializer(), \\\\\\\\n-                weights_regularizer=slim.l2_regularizer(args.weight_decay),\\\\\\\\n-                scope=\\\\\\\\\\\\\\\'Logits\\\\\\\\\\\\\\\', reuse=False)\\\\\\\\n-\\\\\\\\n-        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\\\\\\\\\\\\\\\'embeddings\\\\\\\\\\\\\\\')\\\\\\\\n-\\\\\\\\n-        # Norm for the prelogits\\\\\\\\n-        eps = 1e-4\\\\\\\\n-        prelogits_norm = tf.reduce_mean(tf.norm(tf.abs(prelogits)+eps, ord=args.prelogits_norm_p, axis=1))\\\\\\\\n-        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_norm * args.prelogits_norm_loss_factor)\\\\\\\\n-\\\\\\\\n-        # Add center loss\\\\\\\\n-        prelogits_center_loss, _ = facenet.center_loss(prelogits, label_batch, args.center_loss_alfa, nrof_classes)\\\\\\\\n-        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_center_loss * args.center_loss_factor)\\\\\\\\n-\\\\\\\\n-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\\\\\\\\n-            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\\\\\\\\n-        tf.summary.scalar(\\\\\\\\\\\\\\\'learning_rate\\\\\\\\\\\\\\\', learning_rate)\\\\\\\\n-\\\\\\\\n-        # Calculate the average cross entropy loss across the batch\\\\\\\\n-        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\\\\\\\\n-            labels=label_batch, logits=logits, name=\\\\\\\\\\\\\\\'cross_entropy_per_example\\\\\\\\\\\\\\\')\\\\\\\\n-        cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\\\\\\\\\\\\\\\'cross_entropy\\\\\\\\\\\\\\\')\\\\\\\\n-        tf.add_to_collection(\\\\\\\\\\\\\\\'losses\\\\\\\\\\\\\\\', cross_entropy_mean)\\\\\\\\n-        \\\\\\\\n-        correct_prediction = tf.cast(tf.equal(tf.argmax(logits, 1), tf.cast(label_batch, tf.int64)), tf.float32)\\\\\\\\n-        accuracy = tf.reduce_mean(correct_prediction)\\\\\\\\n-        \\\\\\\\n-        # Calculate the total losses\\\\\\\\n-        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\\\\\\\\n-        total_loss = tf.add_n([cross_entropy_mean] + regularization_losses, name=\\\\\\\\\\\\\\\'total_loss\\\\\\\\\\\\\\\')\\\\\\\\n-\\\\\\\\n-        # Build a Graph that trains the model with one batch of examples and updates the model parameters\\\\\\\\n-        train_op = facenet.train(total_loss, global_step, args.optimizer, \\\\\\\\n-            learning_rate, args.moving_average_decay, tf.global_variables(), args.log_histograms)\\\\\\\\n-        \\\\\\\\n-        # Create a saver\\\\\\\\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\\\\\\\\n-\\\\\\\\n-        # Build the summary operation based on the TF collection of Summaries.\\\\\\\\n-        summary_op = tf.summary.merge_all()\\\\\\\\n-\\\\\\\\n-        # Start running operations on the Graph.\\\\\\\\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\\\\\\\\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\\\\\n-        sess.run(tf.global_variables_initializer())\\\\\\\\n-        sess.run(tf.local_variables_initializer())\\\\\\\\n-        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\\\\\\\\n-        coord = tf.train.Coordinator()\\\\\\\\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\\\\\\\\n-\\\\\\\\n-        with sess.as_default():\\\\\\\\n-\\\\\\\\n-            if pretrained_model:\\\\\\\\n-                print(\\\\\\\\\\\\\\\'Restoring pretrained model: %s\\\\\\\\\\\\\\\' % pretrained_model)\\\\\\\\n-                saver.restore(sess, pretrained_model)\\\\\\\\n-\\\\\\\\n-            # Training and validation loop\\\\\\\\n-            print(\\\\\\\\\\\\\\\'Running training\\\\\\\\\\\\\\\')\\\\\\\\n-            nrof_steps = args.max_nrof_epochs*args.epoch_size\\\\\\\\n-            nrof_val_samples = int(math.ceil(args.max_nrof_epochs / args.validate_every_n_epochs))   # Validate every validate_every_n_epochs as well as in the last epoch\\\\\\\\n-            stat = {\\\\\\\\n-                \\\\\\\\\\\\\\\'loss\\\\\\\\\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\\\\\n-                \\\\\\\\\\\\\\\'center_loss\\\\\\\\\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\\\\\n-                \\\\\\\\\\\\\\\'reg_loss\\\\\\\\\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\\\\\n-                \\\\\\\\\\\\\\\'xent_loss\\\\\\\\\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\\\\\n-                \\\\\\\\\\\\\\\'prelogits_norm\\\\\\\\\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\\\\\n-                \\\\\\\\\\\\\\\'accuracy\\\\\\\\\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\\\\\n-                \\\\\\\\\\\\\\\'val_loss\\\\\\\\\\\\\\\': np.zeros((nrof_val_samples,), np.float32),\\\\\\\\n-                \\\\\\\\\\\\\\\'val_xent_loss\\\\\\\\\\\\\\\': np.zeros((nrof_val_samples,), np.float32),\\\\\\\\n-                \\\\\\\\\\\\\\\'val_accuracy\\\\\\\\\\\\\\\': np.zeros((nrof_val_samples,), np.float32),\\\\\\\\n-                \\\\\\\\\\\\\\\'lfw_accuracy\\\\\\\\\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\\\\\n-                \\\\\\\\\\\\\\\'lfw_valrate\\\\\\\\\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\\\\\n-                \\\\\\\\\\\\\\\'learning_rate\\\\\\\\\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\\\\\n-                \\\\\\\\\\\\\\\'time_train\\\\\\\\\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\\\\\n-                \\\\\\\\\\\\\\\'time_validate\\\\\\\\\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\\\\\n-                \\\\\\\\\\\\\\\'time_evaluate\\\\\\\\\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\\\\\n-                \\\\\\\\\\\\\\\'prelogits_hist\\\\\\\\\\\\\\\': np.zeros((args.max_nrof_epochs, 1000), np.float32),\\\\\\\\n-              }\\\\\\\\n-            for epoch in range(1,args.max_nrof_epochs+1):\\\\\\\\n-                step = sess.run(global_step, feed_dict=None)\\\\\\\\n-                # Train for one epoch\\\\\\\\n-                t = time.time()\\\\\\\\n-                cont = train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder,\\\\\\\\n-                    learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, global_step, \\\\\\\\n-                    total_loss, train_op, summary_op, summary_writer, regularization_losses, args.learning_rate_schedule_file,\\\\\\\\n-                    stat, cross_entropy_mean, accuracy, learning_rate,\\\\\\\\n-                    prelogits, prelogits_center_loss, args.random_rotate, args.random_crop, args.random_flip, prelogits_norm, args.prelogits_hist_max, args.use_fixed_image_standardization)\\\\\\\\n-                stat[\\\\\\\\\\\\\\\'time_train\\\\\\\\\\\\\\\'][epoch-1] = time.time() - t\\\\\\\\n-                \\\\\\\\n-                if not cont:\\\\\\\\n-                    break\\\\\\\\n-                  \\\\\\\\n-                t = time.time()\\\\\\\\n-                if len(val_image_list)>0 and ((epoch-1) % args.validate_every_n_epochs == args.validate_every_n_epochs-1 or epoch==args.max_nrof_epochs):\\\\\\\\n-                    validate(args, sess, epoch, val_image_list, val_label_list, enqueue_op, image_paths_placeholder, labels_placeholder, control_placeholder,\\\\\\\\n-                        phase_train_placeholder, batch_size_placeholder, \\\\\\\\n-                        stat, total_loss, regularization_losses, cross_entropy_mean, accuracy, args.validate_every_n_epochs, args.use_fixed_image_standardization)\\\\\\\\n-                stat[\\\\\\\\\\\\\\\'time_validate\\\\\\\\\\\\\\\'][epoch-1] = time.time() - t\\\\\\\\n-\\\\\\\\n-                # Save variables and the metagraph if it doesn\\\\\\\\\\\\\\\'t exist already\\\\\\\\n-                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, epoch)\\\\\\\\n-\\\\\\\\n-                # Evaluate on LFW\\\\\\\\n-                t = time.time()\\\\\\\\n-                if args.lfw_dir:\\\\\\\\n-                    evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, \\\\\\\\n-                        embeddings, label_batch, lfw_paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, log_dir, step, summary_writer, stat, epoch, \\\\\\\\n-                        args.lfw_distance_metric, args.lfw_subtract_mean, args.lfw_use_flipped_images, args.use_fixed_image_standardization)\\\\\\\\n-                stat[\\\\\\\\\\\\\\\'time_evaluate\\\\\\\\\\\\\\\'][epoch-1] = time.time() - t\\\\\\\\n-\\\\\\\\n-                print(\\\\\\\\\\\\\\\'Saving statistics\\\\\\\\\\\\\\\')\\\\\\\\n-                with h5py.File(stat_file_name, \\\\\\\\\\\\\\\'w\\\\\\\\\\\\\\\') as f:\\\\\\\\n-                    for key, value in stat.iteritems():\\\\\\\\n-                        f.create_dataset(key, data=value)\\\\\\\\n-    \\\\\\\\n-    return model_dir\\\\\\\\n-  \\\\\\\\n-def find_threshold(var, percentile):\\\\\\\\n-    hist, bin_edges = np.histogram(var, 100)\\\\\\\\n-    cdf = np.float32(np.cumsum(hist)) / np.sum(hist)\\\\\\\\n-    bin_centers = (bin_edges[:-1]+bin_edges[1:])/2\\\\\\\\n-    #plt.plot(bin_centers, cdf)\\\\\\\\n-    threshold = np.interp(percentile*0.01, cdf, bin_centers)\\\\\\\\n-    return threshold\\\\\\\\n-  \\\\\\\\n-def filter_dataset(dataset, data_filename, percentile, min_nrof_images_per_class):\\\\\\\\n-    with h5py.File(data_filename,\\\\\\\\\\\\\\\'r\\\\\\\\\\\\\\\') as f:\\\\\\\\n-        distance_to_center = np.array(f.get(\\\\\\\\\\\\\\\'distance_to_center\\\\\\\\\\\\\\\'))\\\\\\\\n-        label_list = np.array(f.get(\\\\\\\\\\\\\\\'label_list\\\\\\\\\\\\\\\'))\\\\\\\\n-        image_list = np.array(f.get(\\\\\\\\\\\\\\\'image_list\\\\\\\\\\\\\\\'))\\\\\\\\n-        distance_to_center_threshold = find_threshold(distance_to_center, percentile)\\\\\\\\n-        indices = np.where(distance_to_center>=distance_to_center_threshold)[0]\\\\\\\\n-        filtered_dataset = dataset\\\\\\\\n-        removelist = []\\\\\\\\n-        for i in indices:\\\\\\\\n-            label = label_list[i]\\\\\\\\n-            image = image_list[i]\\\\\\\\n-            if image in filtered_dataset[label].image_paths:\\\\\\\\n-                filtered_dataset[label].image_paths.remove(image)\\\\\\\\n-            if len(filtered_dataset[label].image_paths)<min_nrof_images_per_class:\\\\\\\\n-                removelist.append(label)\\\\\\\\n-\\\\\\\\n-        ix = sorted(list(set(removelist)), reverse=True)\\\\\\\\n-        for i in ix:\\\\\\\\n-            del(filtered_dataset[i])\\\\\\\\n-\\\\\\\\n-    return filtered_dataset\\\\\\\\n-  \\\\\\\\n-def train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder, \\\\\\\\n-      learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, step, \\\\\\\\n-      loss, train_op, summary_op, summary_writer, reg_losses, learning_rate_schedule_file, \\\\\\\\n-      stat, cross_entropy_mean, accuracy, \\\\\\\\n-      learning_rate, prelogits, prelogits_center_loss, random_rotate, random_crop, random_flip, prelogits_norm, prelogits_hist_max, use_fixed_image_standardization):\\\\\\\\n-    batch_number = 0\\\\\\\\n-    \\\\\\\\n-    if args.learning_rate>0.0:\\\\\\\\n-        lr = args.learning_rate\\\\\\\\n-    else:\\\\\\\\n-        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\\\\\\\\n-        \\\\\\\\n-    if lr<=0:\\\\\\\\n-        return False \\\\\\\\n-\\\\\\\\n-    index_epoch = sess.run(index_dequeue_op)\\\\\\\\n-    label_epoch = np.array(label_list)[index_epoch]\\\\\\\\n-    image_epoch = np.array(image_list)[index_epoch]\\\\\\\\n-    \\\\\\\\n-    # Enqueue one epoch of image paths and labels\\\\\\\\n-    labels_array = np.expand_dims(np.array(label_epoch),1)\\\\\\\\n-    image_paths_array = np.expand_dims(np.array(image_epoch),1)\\\\\\\\n-    control_value = facenet.RANDOM_ROTATE * random_rotate + facenet.RANDOM_CROP * random_crop + facenet.RANDOM_FLIP * random_flip + facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization\\\\\\\\n-    control_array = np.ones_like(labels_array) * control_value\\\\\\\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\\\\\\\n-\\\\\\\\n-    # Training loop\\\\\\\\n-    train_time = 0\\\\\\\\n-    while batch_number < args.epoch_size:\\\\\\\\n-        start_time = time.time()\\\\\\\\n-        feed_dict = {learning_rate_placeholder: lr, phase_train_placeholder:True, batch_size_placeholder:args.batch_size}\\\\\\\\n-        tensor_list = [loss, train_op, step, reg_losses, prelogits, cross_entropy_mean, learning_rate, prelogits_norm, accuracy, prelogits_center_loss]\\\\\\\\n-        if batch_number % 100 == 0:\\\\\\\\n-            loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_, summary_str = sess.run(tensor_list + [summary_op], feed_dict=feed_dict)\\\\\\\\n-            summary_writer.add_summary(summary_str, global_step=step_)\\\\\\\\n-        else:\\\\\\\\n-            loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_ = sess.run(tensor_list, feed_dict=feed_dict)\\\\\\\\n-         \\\\\\\\n-        duration = time.time() - start_time\\\\\\\\n-        stat[\\\\\\\\\\\\\\\'loss\\\\\\\\\\\\\\\'][step_-1] = loss_\\\\\\\\n-        stat[\\\\\\\\\\\\\\\'center_loss\\\\\\\\\\\\\\\'][step_-1] = center_loss_\\\\\\\\n-        stat[\\\\\\\\\\\\\\\'reg_loss\\\\\\\\\\\\\\\'][step_-1] = np.sum(reg_losses_)\\\\\\\\n-        stat[\\\\\\\\\\\\\\\'xent_loss\\\\\\\\\\\\\\\'][step_-1] = cross_entropy_mean_\\\\\\\\n-        stat[\\\\\\\\\\\\\\\'prelogits_norm\\\\\\\\\\\\\\\'][step_-1] = prelogits_norm_\\\\\\\\n-        stat[\\\\\\\\\\\\\\\'learning_rate\\\\\\\\\\\\\\\'][epoch-1] = lr_\\\\\\\\n-        stat[\\\\\\\\\\\\\\\'accuracy\\\\\\\\\\\\\\\'][step_-1] = accuracy_\\\\\\\\n-        stat[\\\\\\\\\\\\\\\'prelogits_hist\\\\\\\\\\\\\\\'][epoch-1,:] += np.histogram(np.minimum(np.abs(prelogits_), prelogits_hist_max), bins=1000, range=(0.0, prelogits_hist_max))[0]\\\\\\\\n-        \\\\\\\\n-        duration = time.time() - start_time\\\\\\\\n-        print(\\\\\\\\\\\\\\\'Epoch: [%d][%d/%d]\\\\\\\\\\\\\\\\tTime %.3f\\\\\\\\\\\\\\\\tLoss %2.3f\\\\\\\\\\\\\\\\tXent %2.3f\\\\\\\\\\\\\\\\tRegLoss %2.3f\\\\\\\\\\\\\\\\tAccuracy %2.3f\\\\\\\\\\\\\\\\tLr %2.5f\\\\\\\\\\\\\\\\tCl %2.3f\\\\\\\\\\\\\\\' %\\\\\\\\n-              (epoch, batch_number+1, args.epoch_size, duration, loss_, cross_entropy_mean_, np.sum(reg_losses_), accuracy_, lr_, center_loss_))\\\\\\\\n-        batch_number += 1\\\\\\\\n-        train_time += duration\\\\\\\\n-    # Add validation loss and accuracy to summary\\\\\\\\n-    summary = tf.Summary()\\\\\\\\n-    #pylint: disable=maybe-no-member\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\'time/total\\\\\\\\\\\\\\\', simple_value=train_time)\\\\\\\\n-    summary_writer.add_summary(summary, global_step=step_)\\\\\\\\n-    return True\\\\\\\\n-\\\\\\\\n-def validate(args, sess, epoch, image_list, label_list, enqueue_op, image_paths_placeholder, labels_placeholder, control_placeholder,\\\\\\\\n-             phase_train_placeholder, batch_size_placeholder, \\\\\\\\n-             stat, loss, regularization_losses, cross_entropy_mean, accuracy, validate_every_n_epochs, use_fixed_image_standardization):\\\\\\\\n-  \\\\\\\\n-    print(\\\\\\\\\\\\\\\'Running forward pass on validation set\\\\\\\\\\\\\\\')\\\\\\\\n-\\\\\\\\n-    nrof_batches = len(label_list) // args.lfw_batch_size\\\\\\\\n-    nrof_images = nrof_batches * args.lfw_batch_size\\\\\\\\n-    \\\\\\\\n-    # Enqueue one epoch of image paths and labels\\\\\\\\n-    labels_array = np.expand_dims(np.array(label_list[:nrof_images]),1)\\\\\\\\n-    image_paths_array = np.expand_dims(np.array(image_list[:nrof_images]),1)\\\\\\\\n-    control_array = np.ones_like(labels_array, np.int32)*facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization\\\\\\\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\\\\\\\n-\\\\\\\\n-    loss_array = np.zeros((nrof_batches,), np.float32)\\\\\\\\n-    xent_array = np.zeros((nrof_batches,), np.float32)\\\\\\\\n-    accuracy_array = np.zeros((nrof_batches,), np.float32)\\\\\\\\n-\\\\\\\\n-    # Training loop\\\\\\\\n-    start_time = time.time()\\\\\\\\n-    for i in range(nrof_batches):\\\\\\\\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:args.lfw_batch_size}\\\\\\\\n-        loss_, cross_entropy_mean_, accuracy_ = sess.run([loss, cross_entropy_mean, accuracy], feed_dict=feed_dict)\\\\\\\\n-        loss_array[i], xent_array[i], accuracy_array[i] = (loss_, cross_entropy_mean_, accuracy_)\\\\\\\\n-        if i % 10 == 9:\\\\\\\\n-            print(\\\\\\\\\\\\\\\'.\\\\\\\\\\\\\\\', end=\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-            sys.stdout.flush()\\\\\\\\n-    print(\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-\\\\\\\\n-    duration = time.time() - start_time\\\\\\\\n-\\\\\\\\n-    val_index = (epoch-1)//validate_every_n_epochs\\\\\\\\n-    stat[\\\\\\\\\\\\\\\'val_loss\\\\\\\\\\\\\\\'][val_index] = np.mean(loss_array)\\\\\\\\n-    stat[\\\\\\\\\\\\\\\'val_xent_loss\\\\\\\\\\\\\\\'][val_index] = np.mean(xent_array)\\\\\\\\n-    stat[\\\\\\\\\\\\\\\'val_accuracy\\\\\\\\\\\\\\\'][val_index] = np.mean(accuracy_array)\\\\\\\\n-\\\\\\\\n-    print(\\\\\\\\\\\\\\\'Validation Epoch: %d\\\\\\\\\\\\\\\\tTime %.3f\\\\\\\\\\\\\\\\tLoss %2.3f\\\\\\\\\\\\\\\\tXent %2.3f\\\\\\\\\\\\\\\\tAccuracy %2.3f\\\\\\\\\\\\\\\' %\\\\\\\\n-          (epoch, duration, np.mean(loss_array), np.mean(xent_array), np.mean(accuracy_array)))\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, \\\\\\\\n-        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, log_dir, step, summary_writer, stat, epoch, distance_metric, subtract_mean, use_flipped_images, use_fixed_image_standardization):\\\\\\\\n-    start_time = time.time()\\\\\\\\n-    # Run forward pass to calculate embeddings\\\\\\\\n-    print(\\\\\\\\\\\\\\\'Runnning forward pass on LFW images\\\\\\\\\\\\\\\')\\\\\\\\n-    \\\\\\\\n-    # Enqueue one epoch of image paths and labels\\\\\\\\n-    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\\\\\\\\n-    nrof_flips = 2 if use_flipped_images else 1\\\\\\\\n-    nrof_images = nrof_embeddings * nrof_flips\\\\\\\\n-    labels_array = np.expand_dims(np.arange(0,nrof_images),1)\\\\\\\\n-    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\\\\\\\\n-    control_array = np.zeros_like(labels_array, np.int32)\\\\\\\\n-    if use_fixed_image_standardization:\\\\\\\\n-        control_array += np.ones_like(labels_array)*facenet.FIXED_STANDARDIZATION\\\\\\\\n-    if use_flipped_images:\\\\\\\\n-        # Flip every second image\\\\\\\\n-        control_array += (labels_array % 2)*facenet.FLIP\\\\\\\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\\\\\\\n-    \\\\\\\\n-    embedding_size = int(embeddings.get_shape()[1])\\\\\\\\n-    assert nrof_images % batch_size == 0, \\\\\\\\\\\\\\\'The number of LFW images must be an integer multiple of the LFW batch size\\\\\\\\\\\\\\\'\\\\\\\\n-    nrof_batches = nrof_images // batch_size\\\\\\\\n-    emb_array = np.zeros((nrof_images, embedding_size))\\\\\\\\n-    lab_array = np.zeros((nrof_images,))\\\\\\\\n-    for i in range(nrof_batches):\\\\\\\\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}\\\\\\\\n-        emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)\\\\\\\\n-        lab_array[lab] = lab\\\\\\\\n-        emb_array[lab, :] = emb\\\\\\\\n-        if i % 10 == 9:\\\\\\\\n-            print(\\\\\\\\\\\\\\\'.\\\\\\\\\\\\\\\', end=\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-            sys.stdout.flush()\\\\\\\\n-    print(\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-    embeddings = np.zeros((nrof_embeddings, embedding_size*nrof_flips))\\\\\\\\n-    if use_flipped_images:\\\\\\\\n-        # Concatenate embeddings for flipped and non flipped version of the images\\\\\\\\n-        embeddings[:,:embedding_size] = emb_array[0::2,:]\\\\\\\\n-        embeddings[:,embedding_size:] = emb_array[1::2,:]\\\\\\\\n-    else:\\\\\\\\n-        embeddings = emb_array\\\\\\\\n-\\\\\\\\n-    assert np.array_equal(lab_array, np.arange(nrof_images))==True, \\\\\\\\\\\\\\\'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline\\\\\\\\\\\\\\\'\\\\\\\\n-    _, _, accuracy, val, val_std, far = lfw.evaluate(embeddings, actual_issame, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\\\\\\\\n-    \\\\\\\\n-    print(\\\\\\\\\\\\\\\'Accuracy: %2.5f+-%2.5f\\\\\\\\\\\\\\\' % (np.mean(accuracy), np.std(accuracy)))\\\\\\\\n-    print(\\\\\\\\\\\\\\\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\\\\\\\\\\\\\\\' % (val, val_std, far))\\\\\\\\n-    lfw_time = time.time() - start_time\\\\\\\\n-    # Add validation loss and accuracy to summary\\\\\\\\n-    summary = tf.Summary()\\\\\\\\n-    #pylint: disable=maybe-no-member\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\'lfw/accuracy\\\\\\\\\\\\\\\', simple_value=np.mean(accuracy))\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\'lfw/val_rate\\\\\\\\\\\\\\\', simple_value=val)\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\'time/lfw\\\\\\\\\\\\\\\', simple_value=lfw_time)\\\\\\\\n-    summary_writer.add_summary(summary, step)\\\\\\\\n-    with open(os.path.join(log_dir,\\\\\\\\\\\\\\\'lfw_result.txt\\\\\\\\\\\\\\\'),\\\\\\\\\\\\\\\'at\\\\\\\\\\\\\\\') as f:\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\'%d\\\\\\\\\\\\\\\\t%.5f\\\\\\\\\\\\\\\\t%.5f\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\' % (step, np.mean(accuracy), val))\\\\\\\\n-    stat[\\\\\\\\\\\\\\\'lfw_accuracy\\\\\\\\\\\\\\\'][epoch-1] = np.mean(accuracy)\\\\\\\\n-    stat[\\\\\\\\\\\\\\\'lfw_valrate\\\\\\\\\\\\\\\'][epoch-1] = val\\\\\\\\n-\\\\\\\\n-def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\\\\\\\\n-    # Save the model checkpoint\\\\\\\\n-    print(\\\\\\\\\\\\\\\'Saving variables\\\\\\\\\\\\\\\')\\\\\\\\n-    start_time = time.time()\\\\\\\\n-    checkpoint_path = os.path.join(model_dir, \\\\\\\\\\\\\\\'model-%s.ckpt\\\\\\\\\\\\\\\' % model_name)\\\\\\\\n-    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\\\\\\\\n-    save_time_variables = time.time() - start_time\\\\\\\\n-    print(\\\\\\\\\\\\\\\'Variables saved in %.2f seconds\\\\\\\\\\\\\\\' % save_time_variables)\\\\\\\\n-    metagraph_filename = os.path.join(model_dir, \\\\\\\\\\\\\\\'model-%s.meta\\\\\\\\\\\\\\\' % model_name)\\\\\\\\n-    save_time_metagraph = 0  \\\\\\\\n-    if not os.path.exists(metagraph_filename):\\\\\\\\n-        print(\\\\\\\\\\\\\\\'Saving metagraph\\\\\\\\\\\\\\\')\\\\\\\\n-        start_time = time.time()\\\\\\\\n-        saver.export_meta_graph(metagraph_filename)\\\\\\\\n-        save_time_metagraph = time.time() - start_time\\\\\\\\n-        print(\\\\\\\\\\\\\\\'Metagraph saved in %.2f seconds\\\\\\\\\\\\\\\' % save_time_metagraph)\\\\\\\\n-    summary = tf.Summary()\\\\\\\\n-    #pylint: disable=maybe-no-member\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\'time/save_variables\\\\\\\\\\\\\\\', simple_value=save_time_variables)\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\'time/save_metagraph\\\\\\\\\\\\\\\', simple_value=save_time_metagraph)\\\\\\\\n-    summary_writer.add_summary(summary, step)\\\\\\\\n-  \\\\\\\\n-\\\\\\\\n-def parse_arguments(argv):\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\n-    \\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--logs_base_dir\\\\\\\\\\\\\\\', type=str, \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Directory where to write event logs.\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\'~/logs/facenet\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--models_base_dir\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Directory where to write trained models and checkpoints.\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\'~/models/facenet\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--gpu_memory_fraction\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Upper bound on the amount of GPU memory that will be used by the process.\\\\\\\\\\\\\\\', default=1.0)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--pretrained_model\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Load a pretrained model before training starts.\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--data_dir\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Path to the data directory containing aligned face patches.\\\\\\\\\\\\\\\',\\\\\\\\n-        default=\\\\\\\\\\\\\\\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--model_def\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Model definition. Points to a module containing the definition of the inference graph.\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\'models.inception_resnet_v1\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--max_nrof_epochs\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of epochs to run.\\\\\\\\\\\\\\\', default=500)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--batch_size\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of images to process in a batch.\\\\\\\\\\\\\\\', default=90)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--image_size\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Image size (height, width) in pixels.\\\\\\\\\\\\\\\', default=160)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--epoch_size\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of batches per epoch.\\\\\\\\\\\\\\\', default=1000)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--embedding_size\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Dimensionality of the embedding.\\\\\\\\\\\\\\\', default=128)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--random_crop\\\\\\\\\\\\\\\', \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \\\\\\\\\\\\\\\' +\\\\\\\\n-         \\\\\\\\\\\\\\\'If the size of the images in the data directory is equal to image_size no cropping is performed\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--random_flip\\\\\\\\\\\\\\\', \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Performs random horizontal flipping of training images.\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--random_rotate\\\\\\\\\\\\\\\', \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Performs random rotations of training images.\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--use_fixed_image_standardization\\\\\\\\\\\\\\\', \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Performs fixed standardization of images.\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--keep_probability\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Keep probability of dropout for the fully connected layer(s).\\\\\\\\\\\\\\\', default=1.0)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--weight_decay\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'L2 weight regularization.\\\\\\\\\\\\\\\', default=0.0)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--center_loss_factor\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Center loss factor.\\\\\\\\\\\\\\\', default=0.0)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--center_loss_alfa\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Center update rate for center loss.\\\\\\\\\\\\\\\', default=0.95)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--prelogits_norm_loss_factor\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Loss based on the norm of the activations in the prelogits layer.\\\\\\\\\\\\\\\', default=0.0)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--prelogits_norm_p\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Norm to use for prelogits norm loss.\\\\\\\\\\\\\\\', default=1.0)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--prelogits_hist_max\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'The max value for the prelogits histogram.\\\\\\\\\\\\\\\', default=10.0)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--optimizer\\\\\\\\\\\\\\\', type=str, choices=[\\\\\\\\\\\\\\\'ADAGRAD\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'ADADELTA\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'ADAM\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'RMSPROP\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'MOM\\\\\\\\\\\\\\\'],\\\\\\\\n-        help=\\\\\\\\\\\\\\\'The optimization algorithm to use\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\'ADAGRAD\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--learning_rate\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Initial learning rate. If set to a negative value a learning rate \\\\\\\\\\\\\\\' +\\\\\\\\n-        \\\\\\\\\\\\\\\'schedule can be specified in the file "learning_rate_schedule.txt"\\\\\\\\\\\\\\\', default=0.1)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--learning_rate_decay_epochs\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of epochs between learning rate decay.\\\\\\\\\\\\\\\', default=100)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--learning_rate_decay_factor\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Learning rate decay factor.\\\\\\\\\\\\\\\', default=1.0)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--moving_average_decay\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Exponential decay for tracking of training parameters.\\\\\\\\\\\\\\\', default=0.9999)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--seed\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Random seed.\\\\\\\\\\\\\\\', default=666)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--nrof_preprocess_threads\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of preprocessing (data loading and augmentation) threads.\\\\\\\\\\\\\\\', default=4)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--log_histograms\\\\\\\\\\\\\\\', \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Enables logging of weight/bias histograms in tensorboard.\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--learning_rate_schedule_file\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\'data/learning_rate_schedule.txt\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--filter_filename\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'File containing image data used for dataset filtering\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--filter_percentile\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Keep only the percentile images closed to its class center\\\\\\\\\\\\\\\', default=100.0)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--filter_min_nrof_images_per_class\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Keep only the classes with this number of examples or more\\\\\\\\\\\\\\\', default=0)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--validate_every_n_epochs\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of epoch between validation\\\\\\\\\\\\\\\', default=5)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--validation_set_split_ratio\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'The ratio of the total dataset to use for validation\\\\\\\\\\\\\\\', default=0.0)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--min_nrof_val_images_per_class\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Classes with fewer images will be removed from the validation set\\\\\\\\\\\\\\\', default=0)\\\\\\\\n- \\\\\\\\n-    # Parameters for validation on LFW\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'The file containing the pairs to use for validation.\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\'data/pairs.txt\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--lfw_dir\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Path to the data directory containing aligned face patches.\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--lfw_batch_size\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of images to process in a batch in the LFW test set.\\\\\\\\\\\\\\\', default=100)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of folds to use for cross validation. Mainly used for testing.\\\\\\\\\\\\\\\', default=10)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--lfw_distance_metric\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Type of distance metric to use. 0: Euclidian, 1:Cosine similarity distance.\\\\\\\\\\\\\\\', default=0)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--lfw_use_flipped_images\\\\\\\\\\\\\\\', \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Concatenates embeddings for the image and its horizontally flipped counterpart.\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--lfw_subtract_mean\\\\\\\\\\\\\\\', \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Subtract feature mean before calculating distance.\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\')\\\\\\\\n-    return parser.parse_args(argv)\\\\\\\\n-  \\\\\\\\n-\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\':\\\\\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\\\\\ndiff --git a/src/train_tripletloss.py b/src/train_tripletloss.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex d6df19a..0000000\\\\\\\\n--- a/src/train_tripletloss.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,486 +0,0 @@\\\\\\\\n-"""Training a face recognizer with TensorFlow based on the FaceNet paper\\\\\\\\n-FaceNet: A Unified Embedding for Face Recognition and Clustering: http://arxiv.org/abs/1503.03832\\\\\\\\n-"""\\\\\\\\n-# MIT License\\\\\\\\n-# \\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\n-# \\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\n-# \\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\n-# \\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\n-# SOFTWARE.\\\\\\\\n-\\\\\\\\n-from __future__ import absolute_import\\\\\\\\n-from __future__ import division\\\\\\\\n-from __future__ import print_function\\\\\\\\n-\\\\\\\\n-from datetime import datetime\\\\\\\\n-import os.path\\\\\\\\n-import time\\\\\\\\n-import sys\\\\\\\\n-import tensorflow as tf\\\\\\\\n-import numpy as np\\\\\\\\n-import importlib\\\\\\\\n-import itertools\\\\\\\\n-import argparse\\\\\\\\n-import facenet\\\\\\\\n-import lfw\\\\\\\\n-\\\\\\\\n-from tensorflow.python.ops import data_flow_ops\\\\\\\\n-\\\\\\\\n-from six.moves import xrange  # @UnresolvedImport\\\\\\\\n-\\\\\\\\n-def main(args):\\\\\\\\n-  \\\\\\\\n-    network = importlib.import_module(args.model_def)\\\\\\\\n-\\\\\\\\n-    subdir = datetime.strftime(datetime.now(), \\\\\\\\\\\\\\\'%Y%m%d-%H%M%S\\\\\\\\\\\\\\\')\\\\\\\\n-    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\\\\\\\\n-    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\\\\\\\\\\\\\\\'t exist\\\\\\\\n-        os.makedirs(log_dir)\\\\\\\\n-    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\\\\\\\\n-    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\\\\\\\\\\\\\\\'t exist\\\\\\\\n-        os.makedirs(model_dir)\\\\\\\\n-\\\\\\\\n-    # Write arguments to a text file\\\\\\\\n-    facenet.write_arguments_to_file(args, os.path.join(log_dir, \\\\\\\\\\\\\\\'arguments.txt\\\\\\\\\\\\\\\'))\\\\\\\\n-        \\\\\\\\n-    # Store some git revision info in a text file in the log directory\\\\\\\\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\\\\\\\\n-    facenet.store_revision_info(src_path, log_dir, \\\\\\\\\\\\\\\' \\\\\\\\\\\\\\\'.join(sys.argv))\\\\\\\\n-\\\\\\\\n-    np.random.seed(seed=args.seed)\\\\\\\\n-    train_set = facenet.get_dataset(args.data_dir)\\\\\\\\n-    \\\\\\\\n-    print(\\\\\\\\\\\\\\\'Model directory: %s\\\\\\\\\\\\\\\' % model_dir)\\\\\\\\n-    print(\\\\\\\\\\\\\\\'Log directory: %s\\\\\\\\\\\\\\\' % log_dir)\\\\\\\\n-    if args.pretrained_model:\\\\\\\\n-        print(\\\\\\\\\\\\\\\'Pre-trained model: %s\\\\\\\\\\\\\\\' % os.path.expanduser(args.pretrained_model))\\\\\\\\n-    \\\\\\\\n-    if args.lfw_dir:\\\\\\\\n-        print(\\\\\\\\\\\\\\\'LFW directory: %s\\\\\\\\\\\\\\\' % args.lfw_dir)\\\\\\\\n-        # Read the file containing the pairs used for testing\\\\\\\\n-        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\\\\\\\\n-        # Get the paths for the corresponding images\\\\\\\\n-        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\\\\\\\\n-        \\\\\\\\n-    \\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\n-        tf.set_random_seed(args.seed)\\\\\\\\n-        global_step = tf.Variable(0, trainable=False)\\\\\\\\n-\\\\\\\\n-        # Placeholder for the learning rate\\\\\\\\n-        learning_rate_placeholder = tf.placeholder(tf.float32, name=\\\\\\\\\\\\\\\'learning_rate\\\\\\\\\\\\\\\')\\\\\\\\n-        \\\\\\\\n-        batch_size_placeholder = tf.placeholder(tf.int32, name=\\\\\\\\\\\\\\\'batch_size\\\\\\\\\\\\\\\')\\\\\\\\n-        \\\\\\\\n-        phase_train_placeholder = tf.placeholder(tf.bool, name=\\\\\\\\\\\\\\\'phase_train\\\\\\\\\\\\\\\')\\\\\\\\n-        \\\\\\\\n-        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,3), name=\\\\\\\\\\\\\\\'image_paths\\\\\\\\\\\\\\\')\\\\\\\\n-        labels_placeholder = tf.placeholder(tf.int64, shape=(None,3), name=\\\\\\\\\\\\\\\'labels\\\\\\\\\\\\\\\')\\\\\\\\n-        \\\\\\\\n-        input_queue = data_flow_ops.FIFOQueue(capacity=100000,\\\\\\\\n-                                    dtypes=[tf.string, tf.int64],\\\\\\\\n-                                    shapes=[(3,), (3,)],\\\\\\\\n-                                    shared_name=None, name=None)\\\\\\\\n-        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder])\\\\\\\\n-        \\\\\\\\n-        nrof_preprocess_threads = 4\\\\\\\\n-        images_and_labels = []\\\\\\\\n-        for _ in range(nrof_preprocess_threads):\\\\\\\\n-            filenames, label = input_queue.dequeue()\\\\\\\\n-            images = []\\\\\\\\n-            for filename in tf.unstack(filenames):\\\\\\\\n-                file_contents = tf.read_file(filename)\\\\\\\\n-                image = tf.image.decode_image(file_contents, channels=3)\\\\\\\\n-                \\\\\\\\n-                if args.random_crop:\\\\\\\\n-                    image = tf.random_crop(image, [args.image_size, args.image_size, 3])\\\\\\\\n-                else:\\\\\\\\n-                    image = tf.image.resize_image_with_crop_or_pad(image, args.image_size, args.image_size)\\\\\\\\n-                if args.random_flip:\\\\\\\\n-                    image = tf.image.random_flip_left_right(image)\\\\\\\\n-    \\\\\\\\n-                #pylint: disable=no-member\\\\\\\\n-                image.set_shape((args.image_size, args.image_size, 3))\\\\\\\\n-                images.append(tf.image.per_image_standardization(image))\\\\\\\\n-            images_and_labels.append([images, label])\\\\\\\\n-    \\\\\\\\n-        image_batch, labels_batch = tf.train.batch_join(\\\\\\\\n-            images_and_labels, batch_size=batch_size_placeholder, \\\\\\\\n-            shapes=[(args.image_size, args.image_size, 3), ()], enqueue_many=True,\\\\\\\\n-            capacity=4 * nrof_preprocess_threads * args.batch_size,\\\\\\\\n-            allow_smaller_final_batch=True)\\\\\\\\n-        image_batch = tf.identity(image_batch, \\\\\\\\\\\\\\\'image_batch\\\\\\\\\\\\\\\')\\\\\\\\n-        image_batch = tf.identity(image_batch, \\\\\\\\\\\\\\\'input\\\\\\\\\\\\\\\')\\\\\\\\n-        labels_batch = tf.identity(labels_batch, \\\\\\\\\\\\\\\'label_batch\\\\\\\\\\\\\\\')\\\\\\\\n-\\\\\\\\n-        # Build the inference graph\\\\\\\\n-        prelogits, _ = network.inference(image_batch, args.keep_probability, \\\\\\\\n-            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size,\\\\\\\\n-            weight_decay=args.weight_decay)\\\\\\\\n-        \\\\\\\\n-        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\\\\\\\\\\\\\\\'embeddings\\\\\\\\\\\\\\\')\\\\\\\\n-        # Split embeddings into anchor, positive and negative and calculate triplet loss\\\\\\\\n-        anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\\\\\\\\n-        triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\\\\\\\\n-        \\\\\\\\n-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\\\\\\\\n-            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\\\\\\\\n-        tf.summary.scalar(\\\\\\\\\\\\\\\'learning_rate\\\\\\\\\\\\\\\', learning_rate)\\\\\\\\n-\\\\\\\\n-        # Calculate the total losses\\\\\\\\n-        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\\\\\\\\n-        total_loss = tf.add_n([triplet_loss] + regularization_losses, name=\\\\\\\\\\\\\\\'total_loss\\\\\\\\\\\\\\\')\\\\\\\\n-\\\\\\\\n-        # Build a Graph that trains the model with one batch of examples and updates the model parameters\\\\\\\\n-        train_op = facenet.train(total_loss, global_step, args.optimizer, \\\\\\\\n-            learning_rate, args.moving_average_decay, tf.global_variables())\\\\\\\\n-        \\\\\\\\n-        # Create a saver\\\\\\\\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\\\\\\\\n-\\\\\\\\n-        # Build the summary operation based on the TF collection of Summaries.\\\\\\\\n-        summary_op = tf.summary.merge_all()\\\\\\\\n-\\\\\\\\n-        # Start running operations on the Graph.\\\\\\\\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\\\\\\\\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))        \\\\\\\\n-\\\\\\\\n-        # Initialize variables\\\\\\\\n-        sess.run(tf.global_variables_initializer(), feed_dict={phase_train_placeholder:True})\\\\\\\\n-        sess.run(tf.local_variables_initializer(), feed_dict={phase_train_placeholder:True})\\\\\\\\n-\\\\\\\\n-        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\\\\\\\\n-        coord = tf.train.Coordinator()\\\\\\\\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\\\\\\\\n-\\\\\\\\n-        with sess.as_default():\\\\\\\\n-\\\\\\\\n-            if args.pretrained_model:\\\\\\\\n-                print(\\\\\\\\\\\\\\\'Restoring pretrained model: %s\\\\\\\\\\\\\\\' % args.pretrained_model)\\\\\\\\n-                saver.restore(sess, os.path.expanduser(args.pretrained_model))\\\\\\\\n-\\\\\\\\n-            # Training and validation loop\\\\\\\\n-            epoch = 0\\\\\\\\n-            while epoch < args.max_nrof_epochs:\\\\\\\\n-                step = sess.run(global_step, feed_dict=None)\\\\\\\\n-                epoch = step // args.epoch_size\\\\\\\\n-                # Train for one epoch\\\\\\\\n-                train(args, sess, train_set, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\\\\\\\\n-                    batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \\\\\\\\n-                    embeddings, total_loss, train_op, summary_op, summary_writer, args.learning_rate_schedule_file,\\\\\\\\n-                    args.embedding_size, anchor, positive, negative, triplet_loss)\\\\\\\\n-\\\\\\\\n-                # Save variables and the metagraph if it doesn\\\\\\\\\\\\\\\'t exist already\\\\\\\\n-                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, step)\\\\\\\\n-\\\\\\\\n-                # Evaluate on LFW\\\\\\\\n-                if args.lfw_dir:\\\\\\\\n-                    evaluate(sess, lfw_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \\\\\\\\n-                            batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, args.batch_size, \\\\\\\\n-                            args.lfw_nrof_folds, log_dir, step, summary_writer, args.embedding_size)\\\\\\\\n-\\\\\\\\n-    return model_dir\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\\\\\\\\n-          batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \\\\\\\\n-          embeddings, loss, train_op, summary_op, summary_writer, learning_rate_schedule_file,\\\\\\\\n-          embedding_size, anchor, positive, negative, triplet_loss):\\\\\\\\n-    batch_number = 0\\\\\\\\n-    \\\\\\\\n-    if args.learning_rate>0.0:\\\\\\\\n-        lr = args.learning_rate\\\\\\\\n-    else:\\\\\\\\n-        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\\\\\\\\n-    while batch_number < args.epoch_size:\\\\\\\\n-        # Sample people randomly from the dataset\\\\\\\\n-        image_paths, num_per_class = sample_people(dataset, args.people_per_batch, args.images_per_person)\\\\\\\\n-        \\\\\\\\n-        print(\\\\\\\\\\\\\\\'Running forward pass on sampled images: \\\\\\\\\\\\\\\', end=\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-        start_time = time.time()\\\\\\\\n-        nrof_examples = args.people_per_batch * args.images_per_person\\\\\\\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\\\\\n-        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\\\\\n-        sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\\\\\n-        emb_array = np.zeros((nrof_examples, embedding_size))\\\\\\\\n-        nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\\\\\\n-        for i in range(nrof_batches):\\\\\\\\n-            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\\\\\\\\n-            emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size, \\\\\\\\n-                learning_rate_placeholder: lr, phase_train_placeholder: True})\\\\\\\\n-            emb_array[lab,:] = emb\\\\\\\\n-        print(\\\\\\\\\\\\\\\'%.3f\\\\\\\\\\\\\\\' % (time.time()-start_time))\\\\\\\\n-\\\\\\\\n-        # Select triplets based on the embeddings\\\\\\\\n-        print(\\\\\\\\\\\\\\\'Selecting suitable triplets for training\\\\\\\\\\\\\\\')\\\\\\\\n-        triplets, nrof_random_negs, nrof_triplets = select_triplets(emb_array, num_per_class, \\\\\\\\n-            image_paths, args.people_per_batch, args.alpha)\\\\\\\\n-        selection_time = time.time() - start_time\\\\\\\\n-        print(\\\\\\\\\\\\\\\'(nrof_random_negs, nrof_triplets) = (%d, %d): time=%.3f seconds\\\\\\\\\\\\\\\' % \\\\\\\\n-            (nrof_random_negs, nrof_triplets, selection_time))\\\\\\\\n-\\\\\\\\n-        # Perform training on the selected triplets\\\\\\\\n-        nrof_batches = int(np.ceil(nrof_triplets*3/args.batch_size))\\\\\\\\n-        triplet_paths = list(itertools.chain(*triplets))\\\\\\\\n-        labels_array = np.reshape(np.arange(len(triplet_paths)),(-1,3))\\\\\\\\n-        triplet_paths_array = np.reshape(np.expand_dims(np.array(triplet_paths),1), (-1,3))\\\\\\\\n-        sess.run(enqueue_op, {image_paths_placeholder: triplet_paths_array, labels_placeholder: labels_array})\\\\\\\\n-        nrof_examples = len(triplet_paths)\\\\\\\\n-        train_time = 0\\\\\\\\n-        i = 0\\\\\\\\n-        emb_array = np.zeros((nrof_examples, embedding_size))\\\\\\\\n-        loss_array = np.zeros((nrof_triplets,))\\\\\\\\n-        summary = tf.Summary()\\\\\\\\n-        step = 0\\\\\\\\n-        while i < nrof_batches:\\\\\\\\n-            start_time = time.time()\\\\\\\\n-            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\\\\\\\\n-            feed_dict = {batch_size_placeholder: batch_size, learning_rate_placeholder: lr, phase_train_placeholder: True}\\\\\\\\n-            err, _, step, emb, lab = sess.run([loss, train_op, global_step, embeddings, labels_batch], feed_dict=feed_dict)\\\\\\\\n-            emb_array[lab,:] = emb\\\\\\\\n-            loss_array[i] = err\\\\\\\\n-            duration = time.time() - start_time\\\\\\\\n-            print(\\\\\\\\\\\\\\\'Epoch: [%d][%d/%d]\\\\\\\\\\\\\\\\tTime %.3f\\\\\\\\\\\\\\\\tLoss %2.3f\\\\\\\\\\\\\\\' %\\\\\\\\n-                  (epoch, batch_number+1, args.epoch_size, duration, err))\\\\\\\\n-            batch_number += 1\\\\\\\\n-            i += 1\\\\\\\\n-            train_time += duration\\\\\\\\n-            summary.value.add(tag=\\\\\\\\\\\\\\\'loss\\\\\\\\\\\\\\\', simple_value=err)\\\\\\\\n-            \\\\\\\\n-        # Add validation loss and accuracy to summary\\\\\\\\n-        #pylint: disable=maybe-no-member\\\\\\\\n-        summary.value.add(tag=\\\\\\\\\\\\\\\'time/selection\\\\\\\\\\\\\\\', simple_value=selection_time)\\\\\\\\n-        summary_writer.add_summary(summary, step)\\\\\\\\n-    return step\\\\\\\\n-  \\\\\\\\n-def select_triplets(embeddings, nrof_images_per_class, image_paths, people_per_batch, alpha):\\\\\\\\n-    """ Select the triplets for training\\\\\\\\n-    """\\\\\\\\n-    trip_idx = 0\\\\\\\\n-    emb_start_idx = 0\\\\\\\\n-    num_trips = 0\\\\\\\\n-    triplets = []\\\\\\\\n-    \\\\\\\\n-    # VGG Face: Choosing good triplets is crucial and should strike a balance between\\\\\\\\n-    #  selecting informative (i.e. challenging) examples and swamping training with examples that\\\\\\\\n-    #  are too hard. This is achieve by extending each pair (a, p) to a triplet (a, p, n) by sampling\\\\\\\\n-    #  the image n at random, but only between the ones that violate the triplet loss margin. The\\\\\\\\n-    #  latter is a form of hard-negative mining, but it is not as aggressive (and much cheaper) than\\\\\\\\n-    #  choosing the maximally violating example, as often done in structured output learning.\\\\\\\\n-\\\\\\\\n-    for i in xrange(people_per_batch):\\\\\\\\n-        nrof_images = int(nrof_images_per_class[i])\\\\\\\\n-        for j in xrange(1,nrof_images):\\\\\\\\n-            a_idx = emb_start_idx + j - 1\\\\\\\\n-            neg_dists_sqr = np.sum(np.square(embeddings[a_idx] - embeddings), 1)\\\\\\\\n-            for pair in xrange(j, nrof_images): # For every possible positive pair.\\\\\\\\n-                p_idx = emb_start_idx + pair\\\\\\\\n-                pos_dist_sqr = np.sum(np.square(embeddings[a_idx]-embeddings[p_idx]))\\\\\\\\n-                neg_dists_sqr[emb_start_idx:emb_start_idx+nrof_images] = np.NaN\\\\\\\\n-                #all_neg = np.where(np.logical_and(neg_dists_sqr-pos_dist_sqr<alpha, pos_dist_sqr<neg_dists_sqr))[0]  # FaceNet selection\\\\\\\\n-                all_neg = np.where(neg_dists_sqr-pos_dist_sqr<alpha)[0] # VGG Face selecction\\\\\\\\n-                nrof_random_negs = all_neg.shape[0]\\\\\\\\n-                if nrof_random_negs>0:\\\\\\\\n-                    rnd_idx = np.random.randint(nrof_random_negs)\\\\\\\\n-                    n_idx = all_neg[rnd_idx]\\\\\\\\n-                    triplets.append((image_paths[a_idx], image_paths[p_idx], image_paths[n_idx]))\\\\\\\\n-                    #print(\\\\\\\\\\\\\\\'Triplet %d: (%d, %d, %d), pos_dist=%2.6f, neg_dist=%2.6f (%d, %d, %d, %d, %d)\\\\\\\\\\\\\\\' % \\\\\\\\n-                    #    (trip_idx, a_idx, p_idx, n_idx, pos_dist_sqr, neg_dists_sqr[n_idx], nrof_random_negs, rnd_idx, i, j, emb_start_idx))\\\\\\\\n-                    trip_idx += 1\\\\\\\\n-\\\\\\\\n-                num_trips += 1\\\\\\\\n-\\\\\\\\n-        emb_start_idx += nrof_images\\\\\\\\n-\\\\\\\\n-    np.random.shuffle(triplets)\\\\\\\\n-    return triplets, num_trips, len(triplets)\\\\\\\\n-\\\\\\\\n-def sample_people(dataset, people_per_batch, images_per_person):\\\\\\\\n-    nrof_images = people_per_batch * images_per_person\\\\\\\\n-  \\\\\\\\n-    # Sample classes from the dataset\\\\\\\\n-    nrof_classes = len(dataset)\\\\\\\\n-    class_indices = np.arange(nrof_classes)\\\\\\\\n-    np.random.shuffle(class_indices)\\\\\\\\n-    \\\\\\\\n-    i = 0\\\\\\\\n-    image_paths = []\\\\\\\\n-    num_per_class = []\\\\\\\\n-    sampled_class_indices = []\\\\\\\\n-    # Sample images from these classes until we have enough\\\\\\\\n-    while len(image_paths)<nrof_images:\\\\\\\\n-        class_index = class_indices[i]\\\\\\\\n-        nrof_images_in_class = len(dataset[class_index])\\\\\\\\n-        image_indices = np.arange(nrof_images_in_class)\\\\\\\\n-        np.random.shuffle(image_indices)\\\\\\\\n-        nrof_images_from_class = min(nrof_images_in_class, images_per_person, nrof_images-len(image_paths))\\\\\\\\n-        idx = image_indices[0:nrof_images_from_class]\\\\\\\\n-        image_paths_for_class = [dataset[class_index].image_paths[j] for j in idx]\\\\\\\\n-        sampled_class_indices += [class_index]*nrof_images_from_class\\\\\\\\n-        image_paths += image_paths_for_class\\\\\\\\n-        num_per_class.append(nrof_images_from_class)\\\\\\\\n-        i+=1\\\\\\\\n-  \\\\\\\\n-    return image_paths, num_per_class\\\\\\\\n-\\\\\\\\n-def evaluate(sess, image_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \\\\\\\\n-        batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, batch_size, \\\\\\\\n-        nrof_folds, log_dir, step, summary_writer, embedding_size):\\\\\\\\n-    start_time = time.time()\\\\\\\\n-    # Run forward pass to calculate embeddings\\\\\\\\n-    print(\\\\\\\\\\\\\\\'Running forward pass on LFW images: \\\\\\\\\\\\\\\', end=\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-    \\\\\\\\n-    nrof_images = len(actual_issame)*2\\\\\\\\n-    assert(len(image_paths)==nrof_images)\\\\\\\\n-    labels_array = np.reshape(np.arange(nrof_images),(-1,3))\\\\\\\\n-    image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\\\\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\\\\\n-    emb_array = np.zeros((nrof_images, embedding_size))\\\\\\\\n-    nrof_batches = int(np.ceil(nrof_images / batch_size))\\\\\\\\n-    label_check_array = np.zeros((nrof_images,))\\\\\\\\n-    for i in xrange(nrof_batches):\\\\\\\\n-        batch_size = min(nrof_images-i*batch_size, batch_size)\\\\\\\\n-        emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size,\\\\\\\\n-            learning_rate_placeholder: 0.0, phase_train_placeholder: False})\\\\\\\\n-        emb_array[lab,:] = emb\\\\\\\\n-        label_check_array[lab] = 1\\\\\\\\n-    print(\\\\\\\\\\\\\\\'%.3f\\\\\\\\\\\\\\\' % (time.time()-start_time))\\\\\\\\n-    \\\\\\\\n-    assert(np.all(label_check_array==1))\\\\\\\\n-    \\\\\\\\n-    _, _, accuracy, val, val_std, far = lfw.evaluate(emb_array, actual_issame, nrof_folds=nrof_folds)\\\\\\\\n-    \\\\\\\\n-    print(\\\\\\\\\\\\\\\'Accuracy: %1.3f+-%1.3f\\\\\\\\\\\\\\\' % (np.mean(accuracy), np.std(accuracy)))\\\\\\\\n-    print(\\\\\\\\\\\\\\\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\\\\\\\\\\\\\\\' % (val, val_std, far))\\\\\\\\n-    lfw_time = time.time() - start_time\\\\\\\\n-    # Add validation loss and accuracy to summary\\\\\\\\n-    summary = tf.Summary()\\\\\\\\n-    #pylint: disable=maybe-no-member\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\'lfw/accuracy\\\\\\\\\\\\\\\', simple_value=np.mean(accuracy))\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\'lfw/val_rate\\\\\\\\\\\\\\\', simple_value=val)\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\'time/lfw\\\\\\\\\\\\\\\', simple_value=lfw_time)\\\\\\\\n-    summary_writer.add_summary(summary, step)\\\\\\\\n-    with open(os.path.join(log_dir,\\\\\\\\\\\\\\\'lfw_result.txt\\\\\\\\\\\\\\\'),\\\\\\\\\\\\\\\'at\\\\\\\\\\\\\\\') as f:\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\'%d\\\\\\\\\\\\\\\\t%.5f\\\\\\\\\\\\\\\\t%.5f\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\' % (step, np.mean(accuracy), val))\\\\\\\\n-\\\\\\\\n-def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\\\\\\\\n-    # Save the model checkpoint\\\\\\\\n-    print(\\\\\\\\\\\\\\\'Saving variables\\\\\\\\\\\\\\\')\\\\\\\\n-    start_time = time.time()\\\\\\\\n-    checkpoint_path = os.path.join(model_dir, \\\\\\\\\\\\\\\'model-%s.ckpt\\\\\\\\\\\\\\\' % model_name)\\\\\\\\n-    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\\\\\\\\n-    save_time_variables = time.time() - start_time\\\\\\\\n-    print(\\\\\\\\\\\\\\\'Variables saved in %.2f seconds\\\\\\\\\\\\\\\' % save_time_variables)\\\\\\\\n-    metagraph_filename = os.path.join(model_dir, \\\\\\\\\\\\\\\'model-%s.meta\\\\\\\\\\\\\\\' % model_name)\\\\\\\\n-    save_time_metagraph = 0  \\\\\\\\n-    if not os.path.exists(metagraph_filename):\\\\\\\\n-        print(\\\\\\\\\\\\\\\'Saving metagraph\\\\\\\\\\\\\\\')\\\\\\\\n-        start_time = time.time()\\\\\\\\n-        saver.export_meta_graph(metagraph_filename)\\\\\\\\n-        save_time_metagraph = time.time() - start_time\\\\\\\\n-        print(\\\\\\\\\\\\\\\'Metagraph saved in %.2f seconds\\\\\\\\\\\\\\\' % save_time_metagraph)\\\\\\\\n-    summary = tf.Summary()\\\\\\\\n-    #pylint: disable=maybe-no-member\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\'time/save_variables\\\\\\\\\\\\\\\', simple_value=save_time_variables)\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\'time/save_metagraph\\\\\\\\\\\\\\\', simple_value=save_time_metagraph)\\\\\\\\n-    summary_writer.add_summary(summary, step)\\\\\\\\n-  \\\\\\\\n-  \\\\\\\\n-def get_learning_rate_from_file(filename, epoch):\\\\\\\\n-    with open(filename, \\\\\\\\\\\\\\\'r\\\\\\\\\\\\\\\') as f:\\\\\\\\n-        for line in f.readlines():\\\\\\\\n-            line = line.split(\\\\\\\\\\\\\\\'#\\\\\\\\\\\\\\\', 1)[0]\\\\\\\\n-            if line:\\\\\\\\n-                par = line.strip().split(\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\')\\\\\\\\n-                e = int(par[0])\\\\\\\\n-                lr = float(par[1])\\\\\\\\n-                if e <= epoch:\\\\\\\\n-                    learning_rate = lr\\\\\\\\n-                else:\\\\\\\\n-                    return learning_rate\\\\\\\\n-    \\\\\\\\n-\\\\\\\\n-def parse_arguments(argv):\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\n-    \\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--logs_base_dir\\\\\\\\\\\\\\\', type=str, \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Directory where to write event logs.\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\'~/logs/facenet\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--models_base_dir\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Directory where to write trained models and checkpoints.\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\'~/models/facenet\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--gpu_memory_fraction\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Upper bound on the amount of GPU memory that will be used by the process.\\\\\\\\\\\\\\\', default=1.0)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--pretrained_model\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Load a pretrained model before training starts.\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--data_dir\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Path to the data directory containing aligned face patches.\\\\\\\\\\\\\\\',\\\\\\\\n-        default=\\\\\\\\\\\\\\\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--model_def\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Model definition. Points to a module containing the definition of the inference graph.\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\'models.inception_resnet_v1\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--max_nrof_epochs\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of epochs to run.\\\\\\\\\\\\\\\', default=500)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--batch_size\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of images to process in a batch.\\\\\\\\\\\\\\\', default=90)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--image_size\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Image size (height, width) in pixels.\\\\\\\\\\\\\\\', default=160)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--people_per_batch\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of people per batch.\\\\\\\\\\\\\\\', default=45)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--images_per_person\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of images per person.\\\\\\\\\\\\\\\', default=40)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--epoch_size\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of batches per epoch.\\\\\\\\\\\\\\\', default=1000)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--alpha\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Positive to negative triplet distance margin.\\\\\\\\\\\\\\\', default=0.2)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--embedding_size\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Dimensionality of the embedding.\\\\\\\\\\\\\\\', default=128)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--random_crop\\\\\\\\\\\\\\\', \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \\\\\\\\\\\\\\\' +\\\\\\\\n-         \\\\\\\\\\\\\\\'If the size of the images in the data directory is equal to image_size no cropping is performed\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--random_flip\\\\\\\\\\\\\\\', \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Performs random horizontal flipping of training images.\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--keep_probability\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Keep probability of dropout for the fully connected layer(s).\\\\\\\\\\\\\\\', default=1.0)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--weight_decay\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'L2 weight regularization.\\\\\\\\\\\\\\\', default=0.0)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--optimizer\\\\\\\\\\\\\\\', type=str, choices=[\\\\\\\\\\\\\\\'ADAGRAD\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'ADADELTA\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'ADAM\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'RMSPROP\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'MOM\\\\\\\\\\\\\\\'],\\\\\\\\n-        help=\\\\\\\\\\\\\\\'The optimization algorithm to use\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\'ADAGRAD\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--learning_rate\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Initial learning rate. If set to a negative value a learning rate \\\\\\\\\\\\\\\' +\\\\\\\\n-        \\\\\\\\\\\\\\\'schedule can be specified in the file "learning_rate_schedule.txt"\\\\\\\\\\\\\\\', default=0.1)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--learning_rate_decay_epochs\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of epochs between learning rate decay.\\\\\\\\\\\\\\\', default=100)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--learning_rate_decay_factor\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Learning rate decay factor.\\\\\\\\\\\\\\\', default=1.0)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--moving_average_decay\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Exponential decay for tracking of training parameters.\\\\\\\\\\\\\\\', default=0.9999)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--seed\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Random seed.\\\\\\\\\\\\\\\', default=666)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--learning_rate_schedule_file\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\'data/learning_rate_schedule.txt\\\\\\\\\\\\\\\')\\\\\\\\n-\\\\\\\\n-    # Parameters for validation on LFW\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'The file containing the pairs to use for validation.\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\'data/pairs.txt\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--lfw_dir\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Path to the data directory containing aligned face patches.\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of folds to use for cross validation. Mainly used for testing.\\\\\\\\\\\\\\\', default=10)\\\\\\\\n-    return parser.parse_args(argv)\\\\\\\\n-  \\\\\\\\n-\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\':\\\\\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\\\\\ndiff --git a/src/validate_on_lfw.py b/src/validate_on_lfw.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex ac456c5..0000000\\\\\\\\n--- a/src/validate_on_lfw.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,164 +0,0 @@\\\\\\\\n-"""Validate a face recognizer on the "Labeled Faces in the Wild" dataset (http://vis-www.cs.umass.edu/lfw/).\\\\\\\\n-Embeddings are calculated using the pairs from http://vis-www.cs.umass.edu/lfw/pairs.txt and the ROC curve\\\\\\\\n-is calculated and plotted. Both the model metagraph and the model parameters need to exist\\\\\\\\n-in the same directory, and the metagraph should have the extension \\\\\\\\\\\\\\\'.meta\\\\\\\\\\\\\\\'.\\\\\\\\n-"""\\\\\\\\n-# MIT License\\\\\\\\n-# \\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\n-# \\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\n-# \\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\n-# \\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\n-# SOFTWARE.\\\\\\\\n-\\\\\\\\n-from __future__ import absolute_import\\\\\\\\n-from __future__ import division\\\\\\\\n-from __future__ import print_function\\\\\\\\n-\\\\\\\\n-import tensorflow as tf\\\\\\\\n-import numpy as np\\\\\\\\n-import argparse\\\\\\\\n-import facenet\\\\\\\\n-import lfw\\\\\\\\n-import os\\\\\\\\n-import sys\\\\\\\\n-from tensorflow.python.ops import data_flow_ops\\\\\\\\n-from sklearn import metrics\\\\\\\\n-from scipy.optimize import brentq\\\\\\\\n-from scipy import interpolate\\\\\\\\n-\\\\\\\\n-def main(args):\\\\\\\\n-  \\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\n-      \\\\\\\\n-        with tf.Session() as sess:\\\\\\\\n-            \\\\\\\\n-            # Read the file containing the pairs used for testing\\\\\\\\n-            pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\\\\\\\\n-\\\\\\\\n-            # Get the paths for the corresponding images\\\\\\\\n-            paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\\\\\\\\n-            \\\\\\\\n-            image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\\\\\\\\\\\\\\\'image_paths\\\\\\\\\\\\\\\')\\\\\\\\n-            labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\\\\\\\\\\\\\\\'labels\\\\\\\\\\\\\\\')\\\\\\\\n-            batch_size_placeholder = tf.placeholder(tf.int32, name=\\\\\\\\\\\\\\\'batch_size\\\\\\\\\\\\\\\')\\\\\\\\n-            control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\\\\\\\\\\\\\\\'control\\\\\\\\\\\\\\\')\\\\\\\\n-            phase_train_placeholder = tf.placeholder(tf.bool, name=\\\\\\\\\\\\\\\'phase_train\\\\\\\\\\\\\\\')\\\\\\\\n- \\\\\\\\n-            nrof_preprocess_threads = 4\\\\\\\\n-            image_size = (args.image_size, args.image_size)\\\\\\\\n-            eval_input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\\\\\\\\n-                                        dtypes=[tf.string, tf.int32, tf.int32],\\\\\\\\n-                                        shapes=[(1,), (1,), (1,)],\\\\\\\\n-                                        shared_name=None, name=None)\\\\\\\\n-            eval_enqueue_op = eval_input_queue.enqueue_many([image_paths_placeholder, labels_placeholder, control_placeholder], name=\\\\\\\\\\\\\\\'eval_enqueue_op\\\\\\\\\\\\\\\')\\\\\\\\n-            image_batch, label_batch = facenet.create_input_pipeline(eval_input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder)\\\\\\\\n-     \\\\\\\\n-            # Load the model\\\\\\\\n-            input_map = {\\\\\\\\\\\\\\\'image_batch\\\\\\\\\\\\\\\': image_batch, \\\\\\\\\\\\\\\'label_batch\\\\\\\\\\\\\\\': label_batch, \\\\\\\\\\\\\\\'phase_train\\\\\\\\\\\\\\\': phase_train_placeholder}\\\\\\\\n-            facenet.load_model(args.model, input_map=input_map)\\\\\\\\n-\\\\\\\\n-            # Get output tensor\\\\\\\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\n-#              \\\\\\\\n-            coord = tf.train.Coordinator()\\\\\\\\n-            tf.train.start_queue_runners(coord=coord, sess=sess)\\\\\\\\n-\\\\\\\\n-            evaluate(sess, eval_enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\\\\\\\\n-                embeddings, label_batch, paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, args.distance_metric, args.subtract_mean,\\\\\\\\n-                args.use_flipped_images, args.use_fixed_image_standardization)\\\\\\\\n-\\\\\\\\n-              \\\\\\\\n-def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\\\\\\\\n-        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, distance_metric, subtract_mean, use_flipped_images, use_fixed_image_standardization):\\\\\\\\n-    # Run forward pass to calculate embeddings\\\\\\\\n-    print(\\\\\\\\\\\\\\\'Runnning forward pass on LFW images\\\\\\\\\\\\\\\')\\\\\\\\n-    \\\\\\\\n-    # Enqueue one epoch of image paths and labels\\\\\\\\n-    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\\\\\\\\n-    nrof_flips = 2 if use_flipped_images else 1\\\\\\\\n-    nrof_images = nrof_embeddings * nrof_flips\\\\\\\\n-    labels_array = np.expand_dims(np.arange(0,nrof_images),1)\\\\\\\\n-    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\\\\\\\\n-    control_array = np.zeros_like(labels_array, np.int32)\\\\\\\\n-    if use_fixed_image_standardization:\\\\\\\\n-        control_array += np.ones_like(labels_array)*facenet.FIXED_STANDARDIZATION\\\\\\\\n-    if use_flipped_images:\\\\\\\\n-        # Flip every second image\\\\\\\\n-        control_array += (labels_array % 2)*facenet.FLIP\\\\\\\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\\\\\\\n-    \\\\\\\\n-    embedding_size = int(embeddings.get_shape()[1])\\\\\\\\n-    assert nrof_images % batch_size == 0, \\\\\\\\\\\\\\\'The number of LFW images must be an integer multiple of the LFW batch size\\\\\\\\\\\\\\\'\\\\\\\\n-    nrof_batches = nrof_images // batch_size\\\\\\\\n-    emb_array = np.zeros((nrof_images, embedding_size))\\\\\\\\n-    lab_array = np.zeros((nrof_images,))\\\\\\\\n-    for i in range(nrof_batches):\\\\\\\\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}\\\\\\\\n-        emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)\\\\\\\\n-        lab_array[lab] = lab\\\\\\\\n-        emb_array[lab, :] = emb\\\\\\\\n-        if i % 10 == 9:\\\\\\\\n-            print(\\\\\\\\\\\\\\\'.\\\\\\\\\\\\\\\', end=\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-            sys.stdout.flush()\\\\\\\\n-    print(\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-    embeddings = np.zeros((nrof_embeddings, embedding_size*nrof_flips))\\\\\\\\n-    if use_flipped_images:\\\\\\\\n-        # Concatenate embeddings for flipped and non flipped version of the images\\\\\\\\n-        embeddings[:,:embedding_size] = emb_array[0::2,:]\\\\\\\\n-        embeddings[:,embedding_size:] = emb_array[1::2,:]\\\\\\\\n-    else:\\\\\\\\n-        embeddings = emb_array\\\\\\\\n-\\\\\\\\n-    assert np.array_equal(lab_array, np.arange(nrof_images))==True, \\\\\\\\\\\\\\\'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline\\\\\\\\\\\\\\\'\\\\\\\\n-    tpr, fpr, accuracy, val, val_std, far = lfw.evaluate(embeddings, actual_issame, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\\\\\\\\n-    \\\\\\\\n-    print(\\\\\\\\\\\\\\\'Accuracy: %2.5f+-%2.5f\\\\\\\\\\\\\\\' % (np.mean(accuracy), np.std(accuracy)))\\\\\\\\n-    print(\\\\\\\\\\\\\\\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\\\\\\\\\\\\\\\' % (val, val_std, far))\\\\\\\\n-    \\\\\\\\n-    auc = metrics.auc(fpr, tpr)\\\\\\\\n-    print(\\\\\\\\\\\\\\\'Area Under Curve (AUC): %1.3f\\\\\\\\\\\\\\\' % auc)\\\\\\\\n-    eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\\\\\\\\n-    print(\\\\\\\\\\\\\\\'Equal Error Rate (EER): %1.3f\\\\\\\\\\\\\\\' % eer)\\\\\\\\n-    \\\\\\\\n-def parse_arguments(argv):\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\n-    \\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'lfw_dir\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Path to the data directory containing aligned LFW face patches.\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--lfw_batch_size\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of images to process in a batch in the LFW test set.\\\\\\\\\\\\\\\', default=100)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'model\\\\\\\\\\\\\\\', type=str, \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--image_size\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Image size (height, width) in pixels.\\\\\\\\\\\\\\\', default=160)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\', type=str,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'The file containing the pairs to use for validation.\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\'data/pairs.txt\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Number of folds to use for cross validation. Mainly used for testing.\\\\\\\\\\\\\\\', default=10)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--distance_metric\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Distance metric  0:euclidian, 1:cosine similarity.\\\\\\\\\\\\\\\', default=0)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--use_flipped_images\\\\\\\\\\\\\\\', \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Concatenates embeddings for the image and its horizontally flipped counterpart.\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--subtract_mean\\\\\\\\\\\\\\\', \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Subtract feature mean before calculating distance.\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--use_fixed_image_standardization\\\\\\\\\\\\\\\', \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Performs fixed standardization of images.\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\')\\\\\\\\n-    return parser.parse_args(argv)\\\\\\\\n-\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\':\\\\\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\\\\\ndiff --git a/test/a b/test/a\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 8b13789..0000000\\\\\\\\n--- a/test/a\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1 +0,0 @@\\\\\\\\n-\\\\\\\\ndiff --git a/test/batch_norm_test.py b/test/batch_norm_test.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 48cfd55..0000000\\\\\\\\n--- a/test/batch_norm_test.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,66 +0,0 @@\\\\\\\\n-# MIT License\\\\\\\\n-# \\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\n-# \\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\n-# \\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\n-# \\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\n-# SOFTWARE.\\\\\\\\n-\\\\\\\\n-import unittest\\\\\\\\n-import tensorflow as tf\\\\\\\\n-import models\\\\\\\\n-import numpy as np\\\\\\\\n-import numpy.testing as testing\\\\\\\\n-\\\\\\\\n-class BatchNormTest(unittest.TestCase):\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-    @unittest.skip("Skip batch norm test case")\\\\\\\\n-    def testBatchNorm(self):\\\\\\\\n-      \\\\\\\\n-        tf.set_random_seed(123)\\\\\\\\n-  \\\\\\\\n-        x = tf.placeholder(tf.float32, [None, 20, 20, 10], name=\\\\\\\\\\\\\\\'input\\\\\\\\\\\\\\\')\\\\\\\\n-        phase_train = tf.placeholder(tf.bool, name=\\\\\\\\\\\\\\\'phase_train\\\\\\\\\\\\\\\')\\\\\\\\n-        \\\\\\\\n-        # generate random noise to pass into batch norm\\\\\\\\n-        #x_gen = tf.random_normal([50,20,20,10])\\\\\\\\n-        \\\\\\\\n-        bn = models.network.batch_norm(x, phase_train)\\\\\\\\n-        \\\\\\\\n-        init = tf.global_variables_initializer()\\\\\\\\n-        sess = tf.Session(config=tf.ConfigProto())\\\\\\\\n-        sess.run(init)\\\\\\\\n-  \\\\\\\\n-        with sess.as_default():\\\\\\\\n-        \\\\\\\\n-            #generate a constant variable to pass into batch norm\\\\\\\\n-            y = np.random.normal(0, 1, size=(50,20,20,10))\\\\\\\\n-            \\\\\\\\n-            feed_dict = {x: y, phase_train: True}\\\\\\\\n-            sess.run(bn, feed_dict=feed_dict)\\\\\\\\n-            \\\\\\\\n-            feed_dict = {x: y, phase_train: False}\\\\\\\\n-            y1 = sess.run(bn, feed_dict=feed_dict)\\\\\\\\n-            y2 = sess.run(bn, feed_dict=feed_dict)\\\\\\\\n-            \\\\\\\\n-            testing.assert_almost_equal(y1, y2, 10, \\\\\\\\\\\\\\\'Output from two forward passes with phase_train==false should be equal\\\\\\\\\\\\\\\')\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-if __name__ == "__main__":\\\\\\\\n-    unittest.main()\\\\\\\\n-    \\\\\\\\n\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\ndiff --git a/test/center_loss_test.py b/test/center_loss_test.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 196cd11..0000000\\\\\\\\n--- a/test/center_loss_test.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,87 +0,0 @@\\\\\\\\n-# MIT License\\\\\\\\n-# \\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\n-# \\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\n-# \\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\n-# \\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\n-# SOFTWARE.\\\\\\\\n-\\\\\\\\n-import unittest\\\\\\\\n-import tensorflow as tf\\\\\\\\n-import numpy as np\\\\\\\\n-import facenet\\\\\\\\n-\\\\\\\\n-class CenterLossTest(unittest.TestCase):\\\\\\\\n-  \\\\\\\\n-\\\\\\\\n-\\\\\\\\n-    def testCenterLoss(self):\\\\\\\\n-        batch_size = 16\\\\\\\\n-        nrof_features = 2\\\\\\\\n-        nrof_classes = 16\\\\\\\\n-        alfa = 0.5\\\\\\\\n-        \\\\\\\\n-        with tf.Graph().as_default():\\\\\\\\n-        \\\\\\\\n-            features = tf.placeholder(tf.float32, shape=(batch_size, nrof_features), name=\\\\\\\\\\\\\\\'features\\\\\\\\\\\\\\\')\\\\\\\\n-            labels = tf.placeholder(tf.int32, shape=(batch_size,), name=\\\\\\\\\\\\\\\'labels\\\\\\\\\\\\\\\')\\\\\\\\n-\\\\\\\\n-            # Define center loss\\\\\\\\n-            center_loss, centers = facenet.center_loss(features, labels, alfa, nrof_classes)\\\\\\\\n-            \\\\\\\\n-            label_to_center = np.array( [ \\\\\\\\n-                 [-3,-3],  [-3,-1],  [-3,1],  [-3,3],\\\\\\\\n-                 [-1,-3],  [-1,-1],  [-1,1],  [-1,3],\\\\\\\\n-                 [ 1,-3],  [ 1,-1],  [ 1,1],  [ 1,3],\\\\\\\\n-                 [ 3,-3],  [ 3,-1],  [ 3,1],  [ 3,3] \\\\\\\\n-                 ])\\\\\\\\n-                \\\\\\\\n-            sess = tf.Session()\\\\\\\\n-            with sess.as_default():\\\\\\\\n-                sess.run(tf.global_variables_initializer())\\\\\\\\n-                np.random.seed(seed=666)\\\\\\\\n-                \\\\\\\\n-                for _ in range(0,100):\\\\\\\\n-                    # Create array of random labels\\\\\\\\n-                    lbls = np.random.randint(low=0, high=nrof_classes, size=(batch_size,))\\\\\\\\n-                    feats = create_features(label_to_center, batch_size, nrof_features, lbls)\\\\\\\\n-\\\\\\\\n-                    center_loss_, centers_ = sess.run([center_loss, centers], feed_dict={features:feats, labels:lbls})\\\\\\\\n-                    \\\\\\\\n-                # After a large number of updates the estimated centers should be close to the true ones\\\\\\\\n-                np.testing.assert_almost_equal(centers_, label_to_center, decimal=5, err_msg=\\\\\\\\\\\\\\\'Incorrect estimated centers\\\\\\\\\\\\\\\')\\\\\\\\n-                np.testing.assert_almost_equal(center_loss_, 0.0, decimal=5, err_msg=\\\\\\\\\\\\\\\'Incorrect center loss\\\\\\\\\\\\\\\')\\\\\\\\n-                \\\\\\\\n-\\\\\\\\n-def create_features(label_to_center, batch_size, nrof_features, labels):\\\\\\\\n-    # Map label to center\\\\\\\\n-#     label_to_center_dict = { \\\\\\\\n-#          0:(-3,-3),  1:(-3,-1),  2:(-3,1),  3:(-3,3),\\\\\\\\n-#          4:(-1,-3),  5:(-1,-1),  6:(-1,1),  7:(-1,3),\\\\\\\\n-#          8:( 1,-3),  9:( 1,-1), 10:( 1,1), 11:( 1,3),\\\\\\\\n-#         12:( 3,-3), 13:( 3,-1), 14:( 3,1), 15:( 3,3),\\\\\\\\n-#         }\\\\\\\\n-    # Create array of features corresponding to the labels\\\\\\\\n-    feats = np.zeros((batch_size, nrof_features))\\\\\\\\n-    for i in range(batch_size):\\\\\\\\n-        cntr =  label_to_center[labels[i]]\\\\\\\\n-        for j in range(nrof_features):\\\\\\\\n-            feats[i,j] = cntr[j]\\\\\\\\n-    return feats\\\\\\\\n-                      \\\\\\\\n-if __name__ == "__main__":\\\\\\\\n-    unittest.main()\\\\\\\\ndiff --git a/test/restore_test.py b/test/restore_test.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex befb04d..0000000\\\\\\\\n--- a/test/restore_test.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,181 +0,0 @@\\\\\\\\n-# MIT License\\\\\\\\n-# \\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\n-# \\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\n-# \\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\n-# \\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\n-# SOFTWARE.\\\\\\\\n-\\\\\\\\n-import unittest\\\\\\\\n-import tempfile\\\\\\\\n-import os\\\\\\\\n-import shutil\\\\\\\\n-import tensorflow as tf\\\\\\\\n-import numpy as np\\\\\\\\n-\\\\\\\\n-class TrainTest(unittest.TestCase):\\\\\\\\n-  \\\\\\\\n-    @classmethod\\\\\\\\n-    def setUpClass(self):\\\\\\\\n-        self.tmp_dir = tempfile.mkdtemp()\\\\\\\\n-        \\\\\\\\n-    @classmethod\\\\\\\\n-    def tearDownClass(self):\\\\\\\\n-        # Recursively remove the temporary directory\\\\\\\\n-        shutil.rmtree(self.tmp_dir)\\\\\\\\n-\\\\\\\\n-    def test_restore_noema(self):\\\\\\\\n-        \\\\\\\\n-        # Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\\\\\\\\n-        x_data = np.random.rand(100).astype(np.float32)\\\\\\\\n-        y_data = x_data * 0.1 + 0.3\\\\\\\\n-        \\\\\\\\n-        # Try to find values for W and b that compute y_data = W * x_data + b\\\\\\\\n-        # (We know that W should be 0.1 and b 0.3, but TensorFlow will\\\\\\\\n-        # figure that out for us.)\\\\\\\\n-        W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name=\\\\\\\\\\\\\\\'W\\\\\\\\\\\\\\\')\\\\\\\\n-        b = tf.Variable(tf.zeros([1]), name=\\\\\\\\\\\\\\\'b\\\\\\\\\\\\\\\')\\\\\\\\n-        y = W * x_data + b\\\\\\\\n-        \\\\\\\\n-        # Minimize the mean squared errors.\\\\\\\\n-        loss = tf.reduce_mean(tf.square(y - y_data))\\\\\\\\n-        optimizer = tf.train.GradientDescentOptimizer(0.5)\\\\\\\\n-        train = optimizer.minimize(loss)\\\\\\\\n-        \\\\\\\\n-        # Before starting, initialize the variables.  We will \\\\\\\\\\\\\\\'run\\\\\\\\\\\\\\\' this first.\\\\\\\\n-        init = tf.global_variables_initializer()\\\\\\\\n-\\\\\\\\n-        saver = tf.train.Saver(tf.trainable_variables())\\\\\\\\n-        \\\\\\\\n-        # Launch the graph.\\\\\\\\n-        sess = tf.Session()\\\\\\\\n-        sess.run(init)\\\\\\\\n-        \\\\\\\\n-        # Fit the line.\\\\\\\\n-        for _ in range(201):\\\\\\\\n-            sess.run(train)\\\\\\\\n-        \\\\\\\\n-        w_reference = sess.run(\\\\\\\\\\\\\\\'W:0\\\\\\\\\\\\\\\')\\\\\\\\n-        b_reference = sess.run(\\\\\\\\\\\\\\\'b:0\\\\\\\\\\\\\\\')\\\\\\\\n-        \\\\\\\\n-        saver.save(sess, os.path.join(self.tmp_dir, "model_ex1"))\\\\\\\\n-        \\\\\\\\n-        tf.reset_default_graph()\\\\\\\\n-\\\\\\\\n-        saver = tf.train.import_meta_graph(os.path.join(self.tmp_dir, "model_ex1.meta"))\\\\\\\\n-        sess = tf.Session()\\\\\\\\n-        saver.restore(sess, os.path.join(self.tmp_dir, "model_ex1"))\\\\\\\\n-        \\\\\\\\n-        w_restored = sess.run(\\\\\\\\\\\\\\\'W:0\\\\\\\\\\\\\\\')\\\\\\\\n-        b_restored = sess.run(\\\\\\\\\\\\\\\'b:0\\\\\\\\\\\\\\\')\\\\\\\\n-        \\\\\\\\n-        self.assertAlmostEqual(w_reference, w_restored, \\\\\\\\\\\\\\\'Restored model use different weight than the original model\\\\\\\\\\\\\\\')\\\\\\\\n-        self.assertAlmostEqual(b_reference, b_restored, \\\\\\\\\\\\\\\'Restored model use different weight than the original model\\\\\\\\\\\\\\\')\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-    @unittest.skip("Skip restore EMA test case for now")\\\\\\\\n-    def test_restore_ema(self):\\\\\\\\n-        \\\\\\\\n-        # Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\\\\\\\\n-        x_data = np.random.rand(100).astype(np.float32)\\\\\\\\n-        y_data = x_data * 0.1 + 0.3\\\\\\\\n-        \\\\\\\\n-        # Try to find values for W and b that compute y_data = W * x_data + b\\\\\\\\n-        # (We know that W should be 0.1 and b 0.3, but TensorFlow will\\\\\\\\n-        # figure that out for us.)\\\\\\\\n-        W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name=\\\\\\\\\\\\\\\'W\\\\\\\\\\\\\\\')\\\\\\\\n-        b = tf.Variable(tf.zeros([1]), name=\\\\\\\\\\\\\\\'b\\\\\\\\\\\\\\\')\\\\\\\\n-        y = W * x_data + b\\\\\\\\n-        \\\\\\\\n-        # Minimize the mean squared errors.\\\\\\\\n-        loss = tf.reduce_mean(tf.square(y - y_data))\\\\\\\\n-        optimizer = tf.train.GradientDescentOptimizer(0.5)\\\\\\\\n-        opt_op = optimizer.minimize(loss)\\\\\\\\n-\\\\\\\\n-        # Track the moving averages of all trainable variables.\\\\\\\\n-        ema = tf.train.ExponentialMovingAverage(decay=0.9999)\\\\\\\\n-        averages_op = ema.apply(tf.trainable_variables())\\\\\\\\n-        with tf.control_dependencies([opt_op]):\\\\\\\\n-            train_op = tf.group(averages_op)\\\\\\\\n-  \\\\\\\\n-        # Before starting, initialize the variables.  We will \\\\\\\\\\\\\\\'run\\\\\\\\\\\\\\\' this first.\\\\\\\\n-        init = tf.global_variables_initializer()\\\\\\\\n-\\\\\\\\n-        saver = tf.train.Saver(tf.trainable_variables())\\\\\\\\n-        \\\\\\\\n-        # Launch the graph.\\\\\\\\n-        sess = tf.Session()\\\\\\\\n-        sess.run(init)\\\\\\\\n-        \\\\\\\\n-        # Fit the line.\\\\\\\\n-        for _ in range(201):\\\\\\\\n-            sess.run(train_op)\\\\\\\\n-        \\\\\\\\n-        w_reference = sess.run(\\\\\\\\\\\\\\\'W/ExponentialMovingAverage:0\\\\\\\\\\\\\\\')\\\\\\\\n-        b_reference = sess.run(\\\\\\\\\\\\\\\'b/ExponentialMovingAverage:0\\\\\\\\\\\\\\\')\\\\\\\\n-        \\\\\\\\n-        saver.save(sess, os.path.join(self.tmp_dir, "model_ex1"))\\\\\\\\n-                \\\\\\\\n-        tf.reset_default_graph()\\\\\\\\n-\\\\\\\\n-        tf.train.import_meta_graph(os.path.join(self.tmp_dir, "model_ex1.meta"))\\\\\\\\n-        sess = tf.Session()\\\\\\\\n-        \\\\\\\\n-        print(\\\\\\\\\\\\\\\'------------------------------------------------------\\\\\\\\\\\\\\\')\\\\\\\\n-        for var in tf.global_variables():\\\\\\\\n-            print(\\\\\\\\\\\\\\\'all variables: \\\\\\\\\\\\\\\' + var.op.name)\\\\\\\\n-        for var in tf.trainable_variables():\\\\\\\\n-            print(\\\\\\\\\\\\\\\'normal variable: \\\\\\\\\\\\\\\' + var.op.name)\\\\\\\\n-        for var in tf.moving_average_variables():\\\\\\\\n-            print(\\\\\\\\\\\\\\\'ema variable: \\\\\\\\\\\\\\\' + var.op.name)\\\\\\\\n-        print(\\\\\\\\\\\\\\\'------------------------------------------------------\\\\\\\\\\\\\\\')\\\\\\\\n-\\\\\\\\n-        mode = 1\\\\\\\\n-        restore_vars = {}\\\\\\\\n-        if mode == 0:\\\\\\\\n-            ema = tf.train.ExponentialMovingAverage(1.0)\\\\\\\\n-            for var in tf.trainable_variables():\\\\\\\\n-                print(\\\\\\\\\\\\\\\'%s: %s\\\\\\\\\\\\\\\' % (ema.average_name(var), var.op.name))\\\\\\\\n-                restore_vars[ema.average_name(var)] = var\\\\\\\\n-        elif mode == 1:\\\\\\\\n-            for var in tf.trainable_variables():\\\\\\\\n-                ema_name = var.op.name + \\\\\\\\\\\\\\\'/ExponentialMovingAverage\\\\\\\\\\\\\\\'\\\\\\\\n-                print(\\\\\\\\\\\\\\\'%s: %s\\\\\\\\\\\\\\\' % (ema_name, var.op.name))\\\\\\\\n-                restore_vars[ema_name] = var\\\\\\\\n-            \\\\\\\\n-        saver = tf.train.Saver(restore_vars, name=\\\\\\\\\\\\\\\'ema_restore\\\\\\\\\\\\\\\')\\\\\\\\n-        \\\\\\\\n-        saver.restore(sess, os.path.join(self.tmp_dir, "model_ex1"))\\\\\\\\n-        \\\\\\\\n-        w_restored = sess.run(\\\\\\\\\\\\\\\'W:0\\\\\\\\\\\\\\\')\\\\\\\\n-        b_restored = sess.run(\\\\\\\\\\\\\\\'b:0\\\\\\\\\\\\\\\')\\\\\\\\n-        \\\\\\\\n-        self.assertAlmostEqual(w_reference, w_restored, \\\\\\\\\\\\\\\'Restored model modes not use the EMA filtered weight\\\\\\\\\\\\\\\')\\\\\\\\n-        self.assertAlmostEqual(b_reference, b_restored, \\\\\\\\\\\\\\\'Restored model modes not use the EMA filtered bias\\\\\\\\\\\\\\\')\\\\\\\\n-\\\\\\\\n-        \\\\\\\\n-# Create a checkpoint file pointing to the model\\\\\\\\n-def create_checkpoint_file(model_dir, model_file):\\\\\\\\n-    checkpoint_filename = os.path.join(model_dir, \\\\\\\\\\\\\\\'checkpoint\\\\\\\\\\\\\\\')\\\\\\\\n-    full_model_filename = os.path.join(model_dir, model_file)\\\\\\\\n-    with open(checkpoint_filename, \\\\\\\\\\\\\\\'w\\\\\\\\\\\\\\\') as f:\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\'model_checkpoint_path: "%s"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\' % full_model_filename)\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\'all_model_checkpoint_paths: "%s"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\' % full_model_filename)\\\\\\\\n-        \\\\\\\\n-if __name__ == "__main__":\\\\\\\\n-    unittest.main()\\\\\\\\n-    \\\\\\\\n\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\ndiff --git a/test/train_test.py b/test/train_test.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 12cd663..0000000\\\\\\\\n--- a/test/train_test.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,246 +0,0 @@\\\\\\\\n-# MIT License\\\\\\\\n-# \\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\n-# \\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\n-# \\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\n-# \\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\n-# SOFTWARE.\\\\\\\\n-\\\\\\\\n-import unittest\\\\\\\\n-import tempfile\\\\\\\\n-import numpy as np\\\\\\\\n-import cv2\\\\\\\\n-import os\\\\\\\\n-import shutil\\\\\\\\n-import download_and_extract  # @UnresolvedImport\\\\\\\\n-import subprocess\\\\\\\\n-\\\\\\\\n-def memory_usage_psutil():\\\\\\\\n-    # return the memory usage in MB\\\\\\\\n-    import psutil\\\\\\\\n-    process = psutil.Process(os.getpid())\\\\\\\\n-    mem = process.memory_info()[0] / float(2 ** 20)\\\\\\\\n-    return mem\\\\\\\\n-\\\\\\\\n-def align_dataset_if_needed(self):\\\\\\\\n-    if not os.path.exists(\\\\\\\\\\\\\\\'data/lfw_aligned\\\\\\\\\\\\\\\'):\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'src/align/align_dataset_mtcnn.py\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'data/lfw\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'data/lfw_aligned\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--image_size\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'160\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--margin\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'32\\\\\\\\\\\\\\\' ]\\\\\\\\n-        subprocess.call(argv)\\\\\\\\n-        \\\\\\\\n-        \\\\\\\\n-class TrainTest(unittest.TestCase):\\\\\\\\n-  \\\\\\\\n-    @classmethod\\\\\\\\n-    def setUpClass(self):\\\\\\\\n-        self.tmp_dir = tempfile.mkdtemp()\\\\\\\\n-        self.dataset_dir = os.path.join(self.tmp_dir, \\\\\\\\\\\\\\\'dataset\\\\\\\\\\\\\\\')\\\\\\\\n-        create_mock_dataset(self.dataset_dir, 160)\\\\\\\\n-        self.lfw_pairs_file = create_mock_lfw_pairs(self.tmp_dir)\\\\\\\\n-        print(self.lfw_pairs_file)\\\\\\\\n-        self.pretrained_model_name = \\\\\\\\\\\\\\\'20180402-114759\\\\\\\\\\\\\\\'\\\\\\\\n-        download_and_extract.download_and_extract_file(self.pretrained_model_name, \\\\\\\\\\\\\\\'data/\\\\\\\\\\\\\\\')\\\\\\\\n-        download_and_extract.download_and_extract_file(\\\\\\\\\\\\\\\'lfw-subset\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'data/\\\\\\\\\\\\\\\')\\\\\\\\n-        self.model_file = os.path.join(\\\\\\\\\\\\\\\'data\\\\\\\\\\\\\\\', self.pretrained_model_name, \\\\\\\\\\\\\\\'model-%s.ckpt-275\\\\\\\\\\\\\\\' % self.pretrained_model_name)\\\\\\\\n-        self.pretrained_model = os.path.join(\\\\\\\\\\\\\\\'data\\\\\\\\\\\\\\\', self.pretrained_model_name)\\\\\\\\n-        self.frozen_graph_filename = os.path.join(\\\\\\\\\\\\\\\'data\\\\\\\\\\\\\\\', self.pretrained_model_name+\\\\\\\\\\\\\\\'.pb\\\\\\\\\\\\\\\')\\\\\\\\n-        print(\\\\\\\\\\\\\\\'Memory utilization (SetUpClass): %.3f MB\\\\\\\\\\\\\\\' % memory_usage_psutil())\\\\\\\\n-\\\\\\\\n-    @classmethod\\\\\\\\n-    def tearDownClass(self):\\\\\\\\n-        # Recursively remove the temporary directory\\\\\\\\n-        shutil.rmtree(self.tmp_dir)\\\\\\\\n-\\\\\\\\n-    def tearDown(self):\\\\\\\\n-        print(\\\\\\\\\\\\\\\'Memory utilization (TearDown): %.3f MB\\\\\\\\\\\\\\\' % memory_usage_psutil())\\\\\\\\n-\\\\\\\\n-    def test_training_classifier_inception_resnet_v1(self):\\\\\\\\n-        print(\\\\\\\\\\\\\\\'test_training_classifier_inception_resnet_v1\\\\\\\\\\\\\\\')\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'src/train_softmax.py\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--logs_base_dir\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--models_base_dir\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--data_dir\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--model_def\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'models.inception_resnet_v1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--epoch_size\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--max_nrof_epochs\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--batch_size\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\', self.lfw_pairs_file,\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_dir\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_batch_size\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--nrof_preprocess_threads\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\' ]\\\\\\\\n-        subprocess.call(argv)\\\\\\\\n-\\\\\\\\n-    def test_training_classifier_inception_resnet_v2(self):\\\\\\\\n-        print(\\\\\\\\\\\\\\\'test_training_classifier_inception_resnet_v2\\\\\\\\\\\\\\\')\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'src/train_softmax.py\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--logs_base_dir\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--models_base_dir\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--data_dir\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--model_def\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'models.inception_resnet_v2\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--epoch_size\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--max_nrof_epochs\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--batch_size\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\', self.lfw_pairs_file,\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_dir\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_batch_size\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\' ]\\\\\\\\n-        subprocess.call(argv)\\\\\\\\n-  \\\\\\\\n-    def test_training_classifier_squeezenet(self):\\\\\\\\n-        print(\\\\\\\\\\\\\\\'test_training_classifier_squeezenet\\\\\\\\\\\\\\\')\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'src/train_softmax.py\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--logs_base_dir\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--models_base_dir\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--data_dir\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--model_def\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'models.squeezenet\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--epoch_size\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--max_nrof_epochs\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--batch_size\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\', self.lfw_pairs_file,\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_dir\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_batch_size\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--nrof_preprocess_threads\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\' ]\\\\\\\\n-        subprocess.call(argv)\\\\\\\\n- \\\\\\\\n-    def test_train_tripletloss_inception_resnet_v1(self):\\\\\\\\n-        print(\\\\\\\\\\\\\\\'test_train_tripletloss_inception_resnet_v1\\\\\\\\\\\\\\\')\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'src/train_tripletloss.py\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--logs_base_dir\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--models_base_dir\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--data_dir\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--model_def\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'models.inception_resnet_v1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--epoch_size\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--max_nrof_epochs\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--batch_size\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'6\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--people_per_batch\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--images_per_person\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'3\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\', self.lfw_pairs_file,\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_dir\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\' ]\\\\\\\\n-        subprocess.call(argv)\\\\\\\\n-  \\\\\\\\n-    def test_finetune_tripletloss_inception_resnet_v1(self):\\\\\\\\n-        print(\\\\\\\\\\\\\\\'test_finetune_tripletloss_inception_resnet_v1\\\\\\\\\\\\\\\')\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'src/train_tripletloss.py\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--logs_base_dir\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--models_base_dir\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--data_dir\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--model_def\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'models.inception_resnet_v1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--pretrained_model\\\\\\\\\\\\\\\', self.model_file,\\\\\\\\n-                \\\\\\\\\\\\\\\'--embedding_size\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'512\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--epoch_size\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--max_nrof_epochs\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--batch_size\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'6\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--people_per_batch\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--images_per_person\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'3\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\', self.lfw_pairs_file,\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_dir\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\' ]\\\\\\\\n-        subprocess.call(argv)\\\\\\\\n-  \\\\\\\\n-    def test_compare(self):\\\\\\\\n-        print(\\\\\\\\\\\\\\\'test_compare\\\\\\\\\\\\\\\')\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'src/compare.py\\\\\\\\\\\\\\\',\\\\\\\\n-                os.path.join(\\\\\\\\\\\\\\\'data/\\\\\\\\\\\\\\\', self.pretrained_model_name),\\\\\\\\n-                \\\\\\\\\\\\\\\'data/images/Anthony_Hopkins_0001.jpg\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'data/images/Anthony_Hopkins_0002.jpg\\\\\\\\\\\\\\\' ]\\\\\\\\n-        subprocess.call(argv)\\\\\\\\n-         \\\\\\\\n-    def test_validate_on_lfw(self):\\\\\\\\n-        print(\\\\\\\\\\\\\\\'test_validate_on_lfw\\\\\\\\\\\\\\\')\\\\\\\\n-        align_dataset_if_needed(self)\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'src/validate_on_lfw.py\\\\\\\\\\\\\\\', \\\\\\\\n-                \\\\\\\\\\\\\\\'data/lfw_aligned\\\\\\\\\\\\\\\',\\\\\\\\n-                self.pretrained_model,\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'data/lfw/pairs_small.txt\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_batch_size\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'6\\\\\\\\\\\\\\\']\\\\\\\\n-        subprocess.call(argv)\\\\\\\\n- \\\\\\\\n-    def test_validate_on_lfw_frozen_graph(self):\\\\\\\\n-        print(\\\\\\\\\\\\\\\'test_validate_on_lfw_frozen_graph\\\\\\\\\\\\\\\')\\\\\\\\n-        self.pretrained_model = os.path.join(\\\\\\\\\\\\\\\'data\\\\\\\\\\\\\\\', self.pretrained_model_name)\\\\\\\\n-        frozen_model = os.path.join(self.pretrained_model, self.pretrained_model_name+\\\\\\\\\\\\\\\'.pb\\\\\\\\\\\\\\\')\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'src/validate_on_lfw.py\\\\\\\\\\\\\\\',\\\\\\\\n-                self.dataset_dir,\\\\\\\\n-                frozen_model,\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\', self.lfw_pairs_file,\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'--lfw_batch_size\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'6\\\\\\\\\\\\\\\']\\\\\\\\n-        subprocess.call(argv)\\\\\\\\n- \\\\\\\\n-    def test_freeze_graph(self):\\\\\\\\n-        print(\\\\\\\\\\\\\\\'test_freeze_graph\\\\\\\\\\\\\\\')\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\',\\\\\\\\n-                \\\\\\\\\\\\\\\'src/freeze_graph.py\\\\\\\\\\\\\\\',\\\\\\\\n-                self.pretrained_model,\\\\\\\\n-                self.frozen_graph_filename ]\\\\\\\\n-        subprocess.call(argv)\\\\\\\\n-\\\\\\\\n-# Create a mock dataset with random pixel images\\\\\\\\n-def create_mock_dataset(dataset_dir, image_size):\\\\\\\\n-   \\\\\\\\n-    nrof_persons = 3\\\\\\\\n-    nrof_images_per_person = 2\\\\\\\\n-    np.random.seed(seed=666)\\\\\\\\n-    os.mkdir(dataset_dir)\\\\\\\\n-    for i in range(nrof_persons):\\\\\\\\n-        class_name = \\\\\\\\\\\\\\\'%04d\\\\\\\\\\\\\\\' % (i+1)\\\\\\\\n-        class_dir = os.path.join(dataset_dir, class_name)\\\\\\\\n-        os.mkdir(class_dir)\\\\\\\\n-        for j in range(nrof_images_per_person):\\\\\\\\n-            img_name = \\\\\\\\\\\\\\\'%04d\\\\\\\\\\\\\\\' % (j+1)\\\\\\\\n-            img_path = os.path.join(class_dir, class_name+\\\\\\\\\\\\\\\'_\\\\\\\\\\\\\\\'+img_name + \\\\\\\\\\\\\\\'.png\\\\\\\\\\\\\\\')\\\\\\\\n-            img = np.random.uniform(low=0.0, high=255.0, size=(image_size,image_size,3))\\\\\\\\n-            cv2.imwrite(img_path, img) #@UndefinedVariable\\\\\\\\n-\\\\\\\\n-# Create a mock LFW pairs file\\\\\\\\n-def create_mock_lfw_pairs(tmp_dir):\\\\\\\\n-    pairs_filename = os.path.join(tmp_dir, \\\\\\\\\\\\\\\'pairs_mock.txt\\\\\\\\\\\\\\\')\\\\\\\\n-    with open(pairs_filename, \\\\\\\\\\\\\\\'w\\\\\\\\\\\\\\\') as f:\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\'10 300\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\')\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\'0001 1 2\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\')\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\'0001 1 0002 1\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\')\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\'0002 1 0003 1\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\')\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\'0001 1 0003 1\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\')\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\'0002 1 2\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\')\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\'0001 2 0002 2\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\')\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\'0002 2 0003 2\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\')\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\'0001 2 0003 2\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\')\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\'0003 1 2\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\')\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\'0001 1 0002 2\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\')\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\'0002 1 0003 2\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\')\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\'0001 1 0003 2\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\')\\\\\\\\n-    return pairs_filename\\\\\\\\n-\\\\\\\\n-if __name__ == "__main__":\\\\\\\\n-    unittest.main()\\\\\\\\n-    \\\\\\\\n\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\ndiff --git a/test/triplet_loss_test.py b/test/triplet_loss_test.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 2648b30..0000000\\\\\\\\n--- a/test/triplet_loss_test.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,54 +0,0 @@\\\\\\\\n-# MIT License\\\\\\\\n-# \\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\n-# \\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\n-# \\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\n-# \\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\n-# SOFTWARE.\\\\\\\\n-\\\\\\\\n-import unittest\\\\\\\\n-import tensorflow as tf\\\\\\\\n-import numpy as np\\\\\\\\n-import facenet\\\\\\\\n-\\\\\\\\n-class DemuxEmbeddingsTest(unittest.TestCase):\\\\\\\\n-  \\\\\\\\n-    def testDemuxEmbeddings(self):\\\\\\\\n-        batch_size = 3*12\\\\\\\\n-        embedding_size = 16\\\\\\\\n-        alpha = 0.2\\\\\\\\n-        \\\\\\\\n-        with tf.Graph().as_default():\\\\\\\\n-        \\\\\\\\n-            embeddings = tf.placeholder(tf.float64, shape=(batch_size, embedding_size), name=\\\\\\\\\\\\\\\'embeddings\\\\\\\\\\\\\\\')\\\\\\\\n-            anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,embedding_size]), 3, 1)\\\\\\\\n-            triplet_loss = facenet.triplet_loss(anchor, positive, negative, alpha)\\\\\\\\n-                \\\\\\\\n-            sess = tf.Session()\\\\\\\\n-            with sess.as_default():\\\\\\\\n-                np.random.seed(seed=666)\\\\\\\\n-                emb = np.random.uniform(size=(batch_size, embedding_size))\\\\\\\\n-                tf_triplet_loss = sess.run(triplet_loss, feed_dict={embeddings:emb})\\\\\\\\n-\\\\\\\\n-                pos_dist_sqr = np.sum(np.square(emb[0::3,:]-emb[1::3,:]),1)\\\\\\\\n-                neg_dist_sqr = np.sum(np.square(emb[0::3,:]-emb[2::3,:]),1)\\\\\\\\n-                np_triplet_loss = np.mean(np.maximum(0.0, pos_dist_sqr - neg_dist_sqr + alpha))\\\\\\\\n-                \\\\\\\\n-                np.testing.assert_almost_equal(tf_triplet_loss, np_triplet_loss, decimal=5, err_msg=\\\\\\\\\\\\\\\'Triplet loss is incorrect\\\\\\\\\\\\\\\')\\\\\\\\n-                      \\\\\\\\n-if __name__ == "__main__":\\\\\\\\n-    unittest.main()\\\\\\\\ndiff --git a/video/a b/video/a\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 8b13789..0000000\\\\\\\\n--- a/video/a\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1 +0,0 @@\\\\\\\\n-\\\\\\\\ndiff --git a/video/camtest.mp4 b/video/camtest.mp4\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex a503c89..0000000\\\\\\\\nBinary files a/video/camtest.mp4 and /dev/null differ\\\\\\\'\\\\n\\\\\\\\ No newline at end of file\\\\n+b\\\\\\\'diff --git a/.gitattributes b/.gitattributes\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 64e2803..0000000\\\\\\\\n--- a/.gitattributes\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,3 +0,0 @@\\\\\\\\n-*.zip filter=lfs diff=lfs merge=lfs -text\\\\\\\\n-*.ckpt* filter=lfs diff=lfs merge=lfs -text\\\\\\\\n-*.pb filter=lfs diff=lfs merge=lfs -text\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3) copy.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3) copy.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 91be536..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3) copy.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 91be536..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro (3).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 0540b28..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_31_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex e9ef9f0..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 04ff255..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_32_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 0c37c23..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (3).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 8e7842e..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro (3).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 91b18ff..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_33_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 881520b..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (3).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 03ab7d3..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro (3).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex fa1f42a..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_34_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex d4d9e24..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 7467151..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_35_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_36_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_36_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 4336cd9..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_07_36_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex c3131df..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex e010c70..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_12_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 2a5ed54..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (3).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 0d89ab7..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro (3).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 669d82a..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_13_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 38b750a..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (3).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex c35fb76..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro (3).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex b1eb45c..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_14_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex b331c99..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (3).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 90c13a1..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro (3).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 49b56f8..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_15_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 10d6da8..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 731c604..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_16_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex ce6f384..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (3).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 93075b3..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro (3).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 2df0742..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_17_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex c609125..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (3).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 4fb46a0..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro (3).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex ccacd60..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_18_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 79dc7da..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 77e1025..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_19_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 81a3125..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (3).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (3).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 70fab4b..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro (3).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 628e2ce..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_20_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro (2).png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex df9dc1f..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex ec12b5d..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_21_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_22_Pro.png b/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_22_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 93da5e6..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/LVQuang/WIN_20250331_09_26_22_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_54_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_54_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 5197b4f..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_54_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_55_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_55_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex da2d35a..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_55_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro (2).png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex b458418..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex c9ded27..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_56_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro (2).png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 12980b8..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 36eeeb0..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_57_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro (2).png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 3ca9822..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro.png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 7a34d00..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_58_Pro.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_59_Pro (2).png b/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_59_Pro (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 5bfc7ed..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/Quang/WIN_20250411_21_48_59_Pro (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/TranDangHieu/download (1).png b/Dataset/FaceData/processed/TranDangHieu/download (1).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex d7a2377..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/TranDangHieu/download (1).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/TranDangHieu/download (2).png b/Dataset/FaceData/processed/TranDangHieu/download (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex da1b935..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/TranDangHieu/download (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/TranDangHieu/download.png b/Dataset/FaceData/processed/TranDangHieu/download.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 51eee7f..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/TranDangHieu/download.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png b/Dataset/FaceData/processed/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 8257ab5..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png b/Dataset/FaceData/processed/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 45870ff..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png b/Dataset/FaceData/processed/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex f4593ba..0000000\\\\\\\\nBinary files a/Dataset/FaceData/processed/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/processed/bounding_boxes_75121.txt b/Dataset/FaceData/processed/bounding_boxes_75121.txt\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex c490bf8..0000000\\\\\\\\n--- a/Dataset/FaceData/processed/bounding_boxes_75121.txt\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,9 +0,0 @@\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Quang\\\\\\\\\\\\\\\\WIN_20250411_21_48_55_Pro.png 573 373 798 664\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Quang\\\\\\\\\\\\\\\\WIN_20250411_21_48_56_Pro (2).png 572 373 797 658\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Quang\\\\\\\\\\\\\\\\WIN_20250411_21_48_57_Pro (2).png 571 370 804 659\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Quang\\\\\\\\\\\\\\\\WIN_20250411_21_48_56_Pro.png 570 376 805 663\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Quang\\\\\\\\\\\\\\\\WIN_20250411_21_48_58_Pro.png 574 376 802 659\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Quang\\\\\\\\\\\\\\\\WIN_20250411_21_48_57_Pro.png 569 368 805 662\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Quang\\\\\\\\\\\\\\\\WIN_20250411_21_48_59_Pro (2).png 568 370 806 663\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Quang\\\\\\\\\\\\\\\\WIN_20250411_21_48_58_Pro (2).png 569 366 808 663\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Quang\\\\\\\\\\\\\\\\WIN_20250411_21_48_54_Pro.png 574 380 800 659\\\\\\\\ndiff --git a/Dataset/FaceData/processed/bounding_boxes_79144.txt b/Dataset/FaceData/processed/bounding_boxes_79144.txt\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex ae1c6a8..0000000\\\\\\\\n--- a/Dataset/FaceData/processed/bounding_boxes_79144.txt\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,46 +0,0 @@\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_21_Pro.png 506 335 795 706\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_18_Pro (2).png 516 299 808 645\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_18_Pro (3).png 517 303 794 608\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_07_34_Pro.png 569 297 864 704\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_15_Pro.png 493 335 781 698\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_20_Pro.png 521 329 813 711\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_07_33_Pro.png 498 331 803 720\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_15_Pro (2).png 490 340 775 691\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_20_Pro (3).png 518 353 824 720\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_18_Pro.png 521 310 822 683\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_14_Pro (3).png 506 334 807 720\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_13_Pro.png 527 330 824 707\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_15_Pro (3).png 501 325 799 702\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_07_35_Pro.png 491 307 794 681\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_19_Pro (2).png 523 313 803 651\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_07_32_Pro.png 474 344 776 720\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_16_Pro.png 515 323 812 708\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_07_36_Pro.png 482 331 798 720\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_07_32_Pro (2).png 495 343 788 720\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_12_Pro (2).png 529 332 824 708\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_17_Pro (2).png 581 334 858 709\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_16_Pro (2).png 534 325 821 709\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_07_34_Pro (3).png 526 302 825 681\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_07_34_Pro (2).png 563 299 860 702\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_21_Pro (2).png 496 333 771 680\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_19_Pro.png 521 316 793 615\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_22_Pro.png 540 299 826 682\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_13_Pro (3).png 519 341 815 715\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_20_Pro (2).png 526 369 812 720\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_07_33_Pro (2).png 518 321 825 720\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_14_Pro.png 522 338 806 720\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_14_Pro (2).png 517 343 817 720\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_17_Pro.png 574 326 856 720\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_07_35_Pro (2).png 481 331 800 720\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_12_Pro.png 518 322 811 699\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_07_31_Pro.png 521 336 819 720\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_13_Pro (2).png 523 332 818 710\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_07_33_Pro (3).png 532 314 834 719\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_07_31_Pro (3).png 505 324 815 720\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LVQuang\\\\\\\\\\\\\\\\WIN_20250331_09_26_17_Pro (3).png 561 315 848 708\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\TranDangHieu\\\\\\\\\\\\\\\\download (1).png 262 672 767 1357\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\TranDangHieu\\\\\\\\\\\\\\\\download (2).png 152 762 696 1478\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\TranDangHieu\\\\\\\\\\\\\\\\download.png 202 705 750 1444\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Viruss\\\\\\\\\\\\\\\\Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png 50 121 926 1264\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Viruss\\\\\\\\\\\\\\\\Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png 154 108 1417 1799\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Viruss\\\\\\\\\\\\\\\\Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png 364 442 910 1148\\\\\\\\ndiff --git a/Dataset/FaceData/processed/revision_info.txt b/Dataset/FaceData/processed/revision_info.txt\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex f0f591b..0000000\\\\\\\\n--- a/Dataset/FaceData/processed/revision_info.txt\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,7 +0,0 @@\\\\\\\\n-arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25\\\\\\\\n---------------------\\\\\\\\n-tensorflow version: 2.19.0\\\\\\\\n---------------------\\\\\\\\n-git hash: b\\\\\\\\\\\\\\\'69ff1e149c0d84a123d6516ddd82970e65392608\\\\\\\\\\\\\\\'\\\\\\\\n---------------------\\\\\\\\n-b\\\\\\\\\\\\\\\'diff --git a/README.md b/README.md\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex 16ec29c..0000000\\\\\\\\\\\\\\\\n--- a/README.md\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1,11 +0,0 @@\\\\\\\\\\\\\\\\n-# MiAI_FaceRecog_3\\\\\\\\\\\\\\\\n-Nh\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\xadn di\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\xbb\\\\\\\\\\\\\\\\x87n khu\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\xb4n m\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\xb7t kh\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\xa1 chu\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\xa9n x\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\xa1c b\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\xb1ng MTCNN v\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\xa0 Facenet!\\\\\\\\\\\\\\\\n-Ch\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\xa1y tr\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\xaan Tensorflow 2.x\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-Article link: http://miai.vn/2019/09/11/face-recog-2-0-nhan-dien-khuon-mat-trong-video-bang-mtcnn-va-facenet/\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-#M\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\xacAI \\\\\\\\\\\\\\\\n-Fanpage: http://facebook.com/miaiblog<br>\\\\\\\\\\\\\\\\n-Group trao \\\\\\\\\\\\\\\\xc4\\\\\\\\\\\\\\\\x91\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\xbb\\\\\\\\\\\\\\\\x95i, chia s\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\xbb: https://www.facebook.com/groups/miaigroup<br>\\\\\\\\\\\\\\\\n-Website: http://ainoodle.tech<br>\\\\\\\\\\\\\\\\n-Youtube: http://bit.ly/miaiyoutube<br>\\\\\\\\\\\\\\\\ndiff --git a/src/a b/src/a\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex 8b13789..0000000\\\\\\\\\\\\\\\\n--- a/src/a\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1 +0,0 @@\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\ndiff --git a/src/calculate_filtering_metrics.py b/src/calculate_filtering_metrics.py\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex f60b9ae..0000000\\\\\\\\\\\\\\\\n--- a/src/calculate_filtering_metrics.py\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1,128 +0,0 @@\\\\\\\\\\\\\\\\n-"""Calculate filtering metrics for a dataset and store in a .hdf file.\\\\\\\\\\\\\\\\n-"""\\\\\\\\\\\\\\\\n-# MIT License\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\\\\\\\\\n-# SOFTWARE.\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-from __future__ import absolute_import\\\\\\\\\\\\\\\\n-from __future__ import division\\\\\\\\\\\\\\\\n-from __future__ import print_function\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\n-import argparse\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\n-import os\\\\\\\\\\\\\\\\n-import sys\\\\\\\\\\\\\\\\n-import time\\\\\\\\\\\\\\\\n-import h5py\\\\\\\\\\\\\\\\n-import math\\\\\\\\\\\\\\\\n-from tensorflow.python.platform import gfile\\\\\\\\\\\\\\\\n-from six import iteritems\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def main(args):\\\\\\\\\\\\\\\\n-    dataset = facenet.get_dataset(args.dataset_dir)\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\\\\\\\\\n-      \\\\\\\\\\\\\\\\n-        # Get a list of image paths and their labels\\\\\\\\\\\\\\\\n-        image_list, label_list = facenet.get_image_paths_and_labels(dataset)\\\\\\\\\\\\\\\\n-        nrof_images = len(image_list)\\\\\\\\\\\\\\\\n-        image_indices = range(nrof_images)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        image_batch, label_batch = facenet.read_and_augment_data(image_list,\\\\\\\\\\\\\\\\n-            image_indices, args.image_size, args.batch_size, None, \\\\\\\\\\\\\\\\n-            False, False, False, nrof_preprocess_threads=4, shuffle=False)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        model_exp = os.path.expanduser(args.model_file)\\\\\\\\\\\\\\\\n-        with gfile.FastGFile(model_exp,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as f:\\\\\\\\\\\\\\\\n-            graph_def = tf.GraphDef()\\\\\\\\\\\\\\\\n-            graph_def.ParseFromString(f.read())\\\\\\\\\\\\\\\\n-            input_map={\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':image_batch, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'phase_train\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':False}\\\\\\\\\\\\\\\\n-            tf.import_graph_def(graph_def, input_map=input_map, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'net\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        embeddings = tf.get_default_graph().get_tensor_by_name("net/embeddings:0")\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        with tf.Session() as sess:\\\\\\\\\\\\\\\\n-            tf.train.start_queue_runners(sess=sess)\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\n-            embedding_size = int(embeddings.get_shape()[1])\\\\\\\\\\\\\\\\n-            nrof_batches = int(math.ceil(nrof_images / args.batch_size))\\\\\\\\\\\\\\\\n-            nrof_classes = len(dataset)\\\\\\\\\\\\\\\\n-            label_array = np.array(label_list)\\\\\\\\\\\\\\\\n-            class_names = [cls.name for cls in dataset]\\\\\\\\\\\\\\\\n-            nrof_examples_per_class = [ len(cls.image_paths) for cls in dataset ]\\\\\\\\\\\\\\\\n-            class_variance = np.zeros((nrof_classes,))\\\\\\\\\\\\\\\\n-            class_center = np.zeros((nrof_classes,embedding_size))\\\\\\\\\\\\\\\\n-            distance_to_center = np.ones((len(label_list),))*np.NaN\\\\\\\\\\\\\\\\n-            emb_array = np.zeros((0,embedding_size))\\\\\\\\\\\\\\\\n-            idx_array = np.zeros((0,), dtype=np.int32)\\\\\\\\\\\\\\\\n-            lab_array = np.zeros((0,), dtype=np.int32)\\\\\\\\\\\\\\\\n-            index_arr = np.append(0, np.cumsum(nrof_examples_per_class))\\\\\\\\\\\\\\\\n-            for i in range(nrof_batches):\\\\\\\\\\\\\\\\n-                t = time.time()\\\\\\\\\\\\\\\\n-                emb, idx = sess.run([embeddings, label_batch])\\\\\\\\\\\\\\\\n-                emb_array = np.append(emb_array, emb, axis=0)\\\\\\\\\\\\\\\\n-                idx_array = np.append(idx_array, idx, axis=0)\\\\\\\\\\\\\\\\n-                lab_array = np.append(lab_array, label_array[idx], axis=0)\\\\\\\\\\\\\\\\n-                for cls in set(lab_array):\\\\\\\\\\\\\\\\n-                    cls_idx = np.where(lab_array==cls)[0]\\\\\\\\\\\\\\\\n-                    if cls_idx.shape[0]==nrof_examples_per_class[cls]:\\\\\\\\\\\\\\\\n-                        # We have calculated all the embeddings for this class\\\\\\\\\\\\\\\\n-                        i2 = np.argsort(idx_array[cls_idx])\\\\\\\\\\\\\\\\n-                        emb_class = emb_array[cls_idx,:]\\\\\\\\\\\\\\\\n-                        emb_sort = emb_class[i2,:]\\\\\\\\\\\\\\\\n-                        center = np.mean(emb_sort, axis=0)\\\\\\\\\\\\\\\\n-                        diffs = emb_sort - center\\\\\\\\\\\\\\\\n-                        dists_sqr = np.sum(np.square(diffs), axis=1)\\\\\\\\\\\\\\\\n-                        class_variance[cls] = np.mean(dists_sqr)\\\\\\\\\\\\\\\\n-                        class_center[cls,:] = center\\\\\\\\\\\\\\\\n-                        distance_to_center[index_arr[cls]:index_arr[cls+1]] = np.sqrt(dists_sqr)\\\\\\\\\\\\\\\\n-                        emb_array = np.delete(emb_array, cls_idx, axis=0)\\\\\\\\\\\\\\\\n-                        idx_array = np.delete(idx_array, cls_idx, axis=0)\\\\\\\\\\\\\\\\n-                        lab_array = np.delete(lab_array, cls_idx, axis=0)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                        \\\\\\\\\\\\\\\\n-                print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Batch %d in %.3f seconds\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % (i, time.time()-t))\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Writing filtering data to %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % args.data_file_name)\\\\\\\\\\\\\\\\n-            mdict = {\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'class_names\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':class_names, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'image_list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':image_list, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'label_list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':label_list, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'distance_to_center\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':distance_to_center }\\\\\\\\\\\\\\\\n-            with h5py.File(args.data_file_name, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'w\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as f:\\\\\\\\\\\\\\\\n-                for key, value in iteritems(mdict):\\\\\\\\\\\\\\\\n-                    f.create_dataset(key, data=value)\\\\\\\\\\\\\\\\n-                        \\\\\\\\\\\\\\\\n-def parse_arguments(argv):\\\\\\\\\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'dataset_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Path to the directory containing aligned dataset.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'model_file\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'File containing the frozen model in protobuf (.pb) format to use for feature extraction.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data_file_name\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'The name of the file to store filtering data in.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--image_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Image size.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=160)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of images to process in a batch.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=90)\\\\\\\\\\\\\\\\n-    return parser.parse_args(argv)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\\\\\\\\\\\\\ndiff --git a/src/decode_msceleb_dataset.py b/src/decode_msceleb_dataset.py\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex 4556bfa..0000000\\\\\\\\\\\\\\\\n--- a/src/decode_msceleb_dataset.py\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1,87 +0,0 @@\\\\\\\\\\\\\\\\n-"""Decode the MsCelebV1 dataset in TSV (tab separated values) format downloaded from\\\\\\\\\\\\\\\\n-https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/\\\\\\\\\\\\\\\\n-"""\\\\\\\\\\\\\\\\n-# MIT License\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\\\\\\\\\n-# SOFTWARE.\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-from __future__ import absolute_import\\\\\\\\\\\\\\\\n-from __future__ import division\\\\\\\\\\\\\\\\n-from __future__ import print_function\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-from scipy import misc\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\n-import base64\\\\\\\\\\\\\\\\n-import sys\\\\\\\\\\\\\\\\n-import os\\\\\\\\\\\\\\\\n-import cv2\\\\\\\\\\\\\\\\n-import argparse\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-# File format: text files, each line is an image record containing 6 columns, delimited by TAB.\\\\\\\\\\\\\\\\n-# Column1: Freebase MID\\\\\\\\\\\\\\\\n-# Column2: Query/Name\\\\\\\\\\\\\\\\n-# Column3: ImageSearchRank\\\\\\\\\\\\\\\\n-# Column4: ImageURL\\\\\\\\\\\\\\\\n-# Column5: PageURL\\\\\\\\\\\\\\\\n-# Column6: ImageData_Base64Encoded\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def main(args):\\\\\\\\\\\\\\\\n-    output_dir = os.path.expanduser(args.output_dir)\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-    if not os.path.exists(output_dir):\\\\\\\\\\\\\\\\n-        os.mkdir(output_dir)\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-    # Store some git revision info in a text file in the output directory\\\\\\\\\\\\\\\\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\\\\\\\\\\\\\\\\n-    facenet.store_revision_info(src_path, output_dir, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.join(sys.argv))\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    i = 0\\\\\\\\\\\\\\\\n-    for f in args.tsv_files:\\\\\\\\\\\\\\\\n-        for line in f:\\\\\\\\\\\\\\\\n-            fields = line.split(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\t\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-            class_dir = fields[0]\\\\\\\\\\\\\\\\n-            img_name = fields[1] + \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' + fields[4] + \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' + args.output_format\\\\\\\\\\\\\\\\n-            img_string = fields[5]\\\\\\\\\\\\\\\\n-            img_dec_string = base64.b64decode(img_string)\\\\\\\\\\\\\\\\n-            img_data = np.fromstring(img_dec_string, dtype=np.uint8)\\\\\\\\\\\\\\\\n-            img = cv2.imdecode(img_data, cv2.IMREAD_COLOR) #pylint: disable=maybe-no-member\\\\\\\\\\\\\\\\n-            if args.size:\\\\\\\\\\\\\\\\n-                img = misc.imresize(img, (args.size, args.size), interp=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'bilinear\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-            full_class_dir = os.path.join(output_dir, class_dir)\\\\\\\\\\\\\\\\n-            if not os.path.exists(full_class_dir):\\\\\\\\\\\\\\\\n-                os.mkdir(full_class_dir)\\\\\\\\\\\\\\\\n-            full_path = os.path.join(full_class_dir, img_name.replace(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'/\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'_\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'))\\\\\\\\\\\\\\\\n-            cv2.imwrite(full_path, img) #pylint: disable=maybe-no-member\\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'%8d: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % (i, full_path))\\\\\\\\\\\\\\\\n-            i += 1\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'output_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str, help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Output base directory for the image dataset\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'tsv_files\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=argparse.FileType(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'), nargs=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'+\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Input TSV file name(s)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int, help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Images are resized to the given size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--output_format\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str, help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Format of the output images\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'png\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', choices=[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'png\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'jpg\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'])\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    main(parser.parse_args())\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\ndiff --git a/src/download_and_extract.py b/src/download_and_extract.py\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex a835ac2..0000000\\\\\\\\\\\\\\\\n--- a/src/download_and_extract.py\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1,51 +0,0 @@\\\\\\\\\\\\\\\\n-import requests\\\\\\\\\\\\\\\\n-import zipfile\\\\\\\\\\\\\\\\n-import os\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-model_dict = {\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'lfw-subset\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':      \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1B5BQUZuJO-paxdN8UclxeHAR1WnR_Tzi\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'20170131-234652\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0B5MzpY9kBtDVSGM0RmVET2EwVEk\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'20170216-091149\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0B5MzpY9kBtDVTGZjcWkzT3pldDA\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'20170512-110547\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0B5MzpY9kBtDVZ2RpVDYwWmxoSUk\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'20180402-114759\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-    }\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def download_and_extract_file(model_name, data_dir):\\\\\\\\\\\\\\\\n-    file_id = model_dict[model_name]\\\\\\\\\\\\\\\\n-    destination = os.path.join(data_dir, model_name + \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.zip\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    if not os.path.exists(destination):\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Downloading file to %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % destination)\\\\\\\\\\\\\\\\n-        download_file_from_google_drive(file_id, destination)\\\\\\\\\\\\\\\\n-        with zipfile.ZipFile(destination, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as zip_ref:\\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Extracting file to %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % data_dir)\\\\\\\\\\\\\\\\n-            zip_ref.extractall(data_dir)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def download_file_from_google_drive(file_id, destination):\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-        URL = "https://drive.google.com/uc?export=download"\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-        session = requests.Session()\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-        response = session.get(URL, params = { \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'id\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' : file_id }, stream = True)\\\\\\\\\\\\\\\\n-        token = get_confirm_token(response)\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-        if token:\\\\\\\\\\\\\\\\n-            params = { \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'id\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' : file_id, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'confirm\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' : token }\\\\\\\\\\\\\\\\n-            response = session.get(URL, params = params, stream = True)\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-        save_response_content(response, destination)    \\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def get_confirm_token(response):\\\\\\\\\\\\\\\\n-    for key, value in response.cookies.items():\\\\\\\\\\\\\\\\n-        if key.startswith(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'download_warning\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'):\\\\\\\\\\\\\\\\n-            return value\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    return None\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def save_response_content(response, destination):\\\\\\\\\\\\\\\\n-    CHUNK_SIZE = 32768\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    with open(destination, "wb") as f:\\\\\\\\\\\\\\\\n-        for chunk in response.iter_content(CHUNK_SIZE):\\\\\\\\\\\\\\\\n-            if chunk: # filter out keep-alive new chunks\\\\\\\\\\\\\\\\n-                f.write(chunk)\\\\\\\\\\\\\\\\ndiff --git a/src/face_rec.py b/src/face_rec.py\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex f92cccf..0000000\\\\\\\\\\\\\\\\n--- a/src/face_rec.py\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1,135 +0,0 @@\\\\\\\\\\\\\\\\n-from __future__ import absolute_import\\\\\\\\\\\\\\\\n-from __future__ import division\\\\\\\\\\\\\\\\n-from __future__ import print_function\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\n-import argparse\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\n-import os\\\\\\\\\\\\\\\\n-import sys\\\\\\\\\\\\\\\\n-import math\\\\\\\\\\\\\\\\n-import pickle\\\\\\\\\\\\\\\\n-import align.detect_face\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\n-import cv2\\\\\\\\\\\\\\\\n-import collections\\\\\\\\\\\\\\\\n-from sklearn.svm import SVC\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def main():\\\\\\\\\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--path\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Path of the video you want to test on.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0)\\\\\\\\\\\\\\\\n-    args = parser.parse_args()\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    # Cai dat cac tham so can thiet\\\\\\\\\\\\\\\\n-    MINSIZE = 20\\\\\\\\\\\\\\\\n-    THRESHOLD = [0.6, 0.7, 0.7]\\\\\\\\\\\\\\\\n-    FACTOR = 0.709\\\\\\\\\\\\\\\\n-    IMAGE_SIZE = 182\\\\\\\\\\\\\\\\n-    INPUT_IMAGE_SIZE = 160\\\\\\\\\\\\\\\\n-    CLASSIFIER_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Models/facemodel.pkl\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-    VIDEO_PATH = args.path\\\\\\\\\\\\\\\\n-    FACENET_MODEL_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Models/20180402-114759.pb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    # Load model da train de nhan dien khuon mat - thuc chat la classifier\\\\\\\\\\\\\\\\n-    with open(CLASSIFIER_PATH, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as file:\\\\\\\\\\\\\\\\n-        model, class_names = pickle.load(file)\\\\\\\\\\\\\\\\n-    print("Custom Classifier, Successfully loaded")\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Cai dat GPU neu co\\\\\\\\\\\\\\\\n-        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\\\\\\\\\\\\\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        with sess.as_default():\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            # Load model MTCNN phat hien khuon mat\\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-            facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            # Lay tensor input va output\\\\\\\\\\\\\\\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\n-            embedding_size = embeddings.get_shape()[1]\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            # Cai dat cac mang con\\\\\\\\\\\\\\\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            people_detected = set()\\\\\\\\\\\\\\\\n-            person_detected = collections.Counter()\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            # Lay hinh anh tu file video\\\\\\\\\\\\\\\\n-            cap = cv2.VideoCapture(VIDEO_PATH)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            while (cap.isOpened()):\\\\\\\\\\\\\\\\n-                # Doc tung frame\\\\\\\\\\\\\\\\n-                ret, frame = cap.read()\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                # Phat hien khuon mat, tra ve vi tri trong bounding_boxes\\\\\\\\\\\\\\\\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                faces_found = bounding_boxes.shape[0]\\\\\\\\\\\\\\\\n-                try:\\\\\\\\\\\\\\\\n-                    # Neu co it nhat 1 khuon mat trong frame\\\\\\\\\\\\\\\\n-                    if faces_found > 0:\\\\\\\\\\\\\\\\n-                        det = bounding_boxes[:, 0:4]\\\\\\\\\\\\\\\\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\\\\\\\\\n-                        for i in range(faces_found):\\\\\\\\\\\\\\\\n-                            bb[i][0] = det[i][0]\\\\\\\\\\\\\\\\n-                            bb[i][1] = det[i][1]\\\\\\\\\\\\\\\\n-                            bb[i][2] = det[i][2]\\\\\\\\\\\\\\\\n-                            bb[i][3] = det[i][3]\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                            # Cat phan khuon mat tim duoc\\\\\\\\\\\\\\\\n-                            cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\\\\\\\\\n-                            scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\\\\\\\\\\\\\n-                                                interpolation=cv2.INTER_CUBIC)\\\\\\\\\\\\\\\\n-                            scaled = facenet.prewhiten(scaled)\\\\\\\\\\\\\\\\n-                            scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\\\\\\\\\n-                            feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\\\\\\\\\n-                            emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\\\\\\\\\n-                            \\\\\\\\\\\\\\\\n-                            # Dua vao model de classifier\\\\\\\\\\\\\\\\n-                            predictions = model.predict_proba(emb_array)\\\\\\\\\\\\\\\\n-                            best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\\\\\\\\\n-                            best_class_probabilities = predictions[\\\\\\\\\\\\\\\\n-                                np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\\\\\\\\\n-                            \\\\\\\\\\\\\\\\n-                            # Lay ra ten va ty le % cua class co ty le cao nhat\\\\\\\\\\\\\\\\n-                            best_name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\n-                            print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                            # Ve khung mau xanh quanh khuon mat\\\\\\\\\\\\\\\\n-                            cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\\\\\\\\\\\\\\\n-                            text_x = bb[i][0]\\\\\\\\\\\\\\\\n-                            text_y = bb[i][3] + 20\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                            # Neu ty le nhan dang > 0.5 thi hien thi ten\\\\\\\\\\\\\\\\n-                            if best_class_probabilities > 0.5:\\\\\\\\\\\\\\\\n-                                name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\n-                            else:\\\\\\\\\\\\\\\\n-                                # Con neu <=0.5 thi hien thi Unknow\\\\\\\\\\\\\\\\n-                                name = "Unknown"\\\\\\\\\\\\\\\\n-                                \\\\\\\\\\\\\\\\n-                            # Viet text len tren frame    \\\\\\\\\\\\\\\\n-                            cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\\\\\\\\\n-                                        1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\n-                            cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\\\\\\\\\\\\\\\\n-                                        cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\\\\\\\\\n-                                        1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\n-                            person_detected[best_name] += 1\\\\\\\\\\\\\\\\n-                except:\\\\\\\\\\\\\\\\n-                    pass\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                # Hien thi frame len man hinh\\\\\\\\\\\\\\\\n-                cv2.imshow(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Face Recognition\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', frame)\\\\\\\\\\\\\\\\n-                if cv2.waitKey(1) & 0xFF == ord(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'q\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'):\\\\\\\\\\\\\\\\n-                    break\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            cap.release()\\\\\\\\\\\\\\\\n-            cv2.destroyAllWindows()\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-main()\\\\\\\\\\\\\\\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\\\\\\\\\\\\\\\nindex 1a425a5..455f67a 100644\\\\\\\\\\\\\\\\n--- a/src/face_rec_cam.py\\\\\\\\\\\\\\\\n+++ b/src/face_rec_cam.py\\\\\\\\\\\\\\\\n@@ -52,9 +52,10 @@ def main():\\\\\\\\\\\\\\\\n             facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n             # Get input and output tensors\\\\\\\\\\\\\\\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\n+            graph = tf.compat.v1.get_default_graph()\\\\\\\\\\\\\\\\n+            images_placeholder = graph.get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\n+            embeddings = graph.get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\n+            phase_train_placeholder = graph.get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\n             embedding_size = embeddings.get_shape()[1]\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\\\\\\\\\\\\\ndiff --git a/src/freeze_graph.py b/src/freeze_graph.py\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex 3584c18..0000000\\\\\\\\\\\\\\\\n--- a/src/freeze_graph.py\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1,103 +0,0 @@\\\\\\\\\\\\\\\\n-"""Imports a model metagraph and checkpoint file, converts the variables to constants\\\\\\\\\\\\\\\\n-and exports the model as a graphdef protobuf\\\\\\\\\\\\\\\\n-"""\\\\\\\\\\\\\\\\n-# MIT License\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\\\\\\\\\n-# SOFTWARE.\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-from __future__ import absolute_import\\\\\\\\\\\\\\\\n-from __future__ import division\\\\\\\\\\\\\\\\n-from __future__ import print_function\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-from tensorflow.python.framework import graph_util\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\n-import argparse\\\\\\\\\\\\\\\\n-import os\\\\\\\\\\\\\\\\n-import sys\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\n-from six.moves import xrange  # @UnresolvedImport\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def main(args):\\\\\\\\\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\\\\\\\\\n-        with tf.Session() as sess:\\\\\\\\\\\\\\\\n-            # Load the model metagraph and checkpoint\\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Model directory: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % args.model_dir)\\\\\\\\\\\\\\\\n-            meta_file, ckpt_file = facenet.get_model_filenames(os.path.expanduser(args.model_dir))\\\\\\\\\\\\\\\\n-            \\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Metagraph file: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % meta_file)\\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Checkpoint file: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % ckpt_file)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            model_dir_exp = os.path.expanduser(args.model_dir)\\\\\\\\\\\\\\\\n-            saver = tf.train.import_meta_graph(os.path.join(model_dir_exp, meta_file), clear_devices=True)\\\\\\\\\\\\\\\\n-            tf.get_default_session().run(tf.global_variables_initializer())\\\\\\\\\\\\\\\\n-            tf.get_default_session().run(tf.local_variables_initializer())\\\\\\\\\\\\\\\\n-            saver.restore(tf.get_default_session(), os.path.join(model_dir_exp, ckpt_file))\\\\\\\\\\\\\\\\n-            \\\\\\\\\\\\\\\\n-            # Retrieve the protobuf graph definition and fix the batch norm nodes\\\\\\\\\\\\\\\\n-            input_graph_def = sess.graph.as_graph_def()\\\\\\\\\\\\\\\\n-            \\\\\\\\\\\\\\\\n-            # Freeze the graph def\\\\\\\\\\\\\\\\n-            output_graph_def = freeze_graph_def(sess, input_graph_def, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'embeddings,label_batch\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Serialize and dump the output graph to the filesystem\\\\\\\\\\\\\\\\n-        with tf.gfile.GFile(args.output_file, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'wb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as f:\\\\\\\\\\\\\\\\n-            f.write(output_graph_def.SerializeToString())\\\\\\\\\\\\\\\\n-        print("%d ops in the final graph: %s" % (len(output_graph_def.node), args.output_file))\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-def freeze_graph_def(sess, input_graph_def, output_node_names):\\\\\\\\\\\\\\\\n-    for node in input_graph_def.node:\\\\\\\\\\\\\\\\n-        if node.op == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'RefSwitch\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\n-            node.op = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Switch\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-            for index in xrange(len(node.input)):\\\\\\\\\\\\\\\\n-                if \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'moving_\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' in node.input[index]:\\\\\\\\\\\\\\\\n-                    node.input[index] = node.input[index] + \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'/read\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-        elif node.op == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'AssignSub\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\n-            node.op = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Sub\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-            if \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'use_locking\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' in node.attr: del node.attr[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'use_locking\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\']\\\\\\\\\\\\\\\\n-        elif node.op == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'AssignAdd\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\n-            node.op = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Add\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-            if \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'use_locking\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' in node.attr: del node.attr[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'use_locking\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\']\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    # Get the list of important nodes\\\\\\\\\\\\\\\\n-    whitelist_names = []\\\\\\\\\\\\\\\\n-    for node in input_graph_def.node:\\\\\\\\\\\\\\\\n-        if (node.name.startswith(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'InceptionResnet\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') or node.name.startswith(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'embeddings\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') or \\\\\\\\\\\\\\\\n-                node.name.startswith(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'image_batch\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') or node.name.startswith(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'label_batch\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') or\\\\\\\\\\\\\\\\n-                node.name.startswith(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'phase_train\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') or node.name.startswith(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Logits\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')):\\\\\\\\\\\\\\\\n-            whitelist_names.append(node.name)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    # Replace all the variables in the graph with constants of the same values\\\\\\\\\\\\\\\\n-    output_graph_def = graph_util.convert_variables_to_constants(\\\\\\\\\\\\\\\\n-        sess, input_graph_def, output_node_names.split(","),\\\\\\\\\\\\\\\\n-        variable_names_whitelist=whitelist_names)\\\\\\\\\\\\\\\\n-    return output_graph_def\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-def parse_arguments(argv):\\\\\\\\\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'model_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str, \\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Directory containing the metagraph (.meta) file and the checkpoint (ckpt) file containing model parameters\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'output_file\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str, \\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Filename for the exported graphdef protobuf (.pb)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    return parser.parse_args(argv)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\\\\\\\\\\\\\ndiff --git a/src/lfw.py b/src/lfw.py\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex 9194433..0000000\\\\\\\\\\\\\\\\n--- a/src/lfw.py\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1,86 +0,0 @@\\\\\\\\\\\\\\\\n-"""Helper for evaluation on the Labeled Faces in the Wild dataset \\\\\\\\\\\\\\\\n-"""\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-# MIT License\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\\\\\\\\\n-# SOFTWARE.\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-from __future__ import absolute_import\\\\\\\\\\\\\\\\n-from __future__ import division\\\\\\\\\\\\\\\\n-from __future__ import print_function\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-import os\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def evaluate(embeddings, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\\\\\\\\\\\\\\\\n-    # Calculate evaluation metrics\\\\\\\\\\\\\\\\n-    thresholds = np.arange(0, 4, 0.01)\\\\\\\\\\\\\\\\n-    embeddings1 = embeddings[0::2]\\\\\\\\\\\\\\\\n-    embeddings2 = embeddings[1::2]\\\\\\\\\\\\\\\\n-    tpr, fpr, accuracy = facenet.calculate_roc(thresholds, embeddings1, embeddings2,\\\\\\\\\\\\\\\\n-        np.asarray(actual_issame), nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\\\\\\\\\\\\\\\\n-    thresholds = np.arange(0, 4, 0.001)\\\\\\\\\\\\\\\\n-    val, val_std, far = facenet.calculate_val(thresholds, embeddings1, embeddings2,\\\\\\\\\\\\\\\\n-        np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\\\\\\\\\\\\\\\\n-    return tpr, fpr, accuracy, val, val_std, far\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def get_paths(lfw_dir, pairs):\\\\\\\\\\\\\\\\n-    nrof_skipped_pairs = 0\\\\\\\\\\\\\\\\n-    path_list = []\\\\\\\\\\\\\\\\n-    issame_list = []\\\\\\\\\\\\\\\\n-    for pair in pairs:\\\\\\\\\\\\\\\\n-        if len(pair) == 3:\\\\\\\\\\\\\\\\n-            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'_\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' + \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'%04d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % int(pair[1])))\\\\\\\\\\\\\\\\n-            path1 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'_\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' + \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'%04d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % int(pair[2])))\\\\\\\\\\\\\\\\n-            issame = True\\\\\\\\\\\\\\\\n-        elif len(pair) == 4:\\\\\\\\\\\\\\\\n-            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'_\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' + \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'%04d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % int(pair[1])))\\\\\\\\\\\\\\\\n-            path1 = add_extension(os.path.join(lfw_dir, pair[2], pair[2] + \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'_\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' + \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'%04d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % int(pair[3])))\\\\\\\\\\\\\\\\n-            issame = False\\\\\\\\\\\\\\\\n-        if os.path.exists(path0) and os.path.exists(path1):    # Only add the pair if both paths exist\\\\\\\\\\\\\\\\n-            path_list += (path0,path1)\\\\\\\\\\\\\\\\n-            issame_list.append(issame)\\\\\\\\\\\\\\\\n-        else:\\\\\\\\\\\\\\\\n-            nrof_skipped_pairs += 1\\\\\\\\\\\\\\\\n-    if nrof_skipped_pairs>0:\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Skipped %d image pairs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % nrof_skipped_pairs)\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    return path_list, issame_list\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-def add_extension(path):\\\\\\\\\\\\\\\\n-    if os.path.exists(path+\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.jpg\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'):\\\\\\\\\\\\\\\\n-        return path+\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.jpg\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-    elif os.path.exists(path+\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.png\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'):\\\\\\\\\\\\\\\\n-        return path+\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.png\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-    else:\\\\\\\\\\\\\\\\n-        raise RuntimeError(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'No file "%s" with extension png or jpg.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % path)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def read_pairs(pairs_filename):\\\\\\\\\\\\\\\\n-    pairs = []\\\\\\\\\\\\\\\\n-    with open(pairs_filename, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as f:\\\\\\\\\\\\\\\\n-        for line in f.readlines()[1:]:\\\\\\\\\\\\\\\\n-            pair = line.strip().split()\\\\\\\\\\\\\\\\n-            pairs.append(pair)\\\\\\\\\\\\\\\\n-    return np.array(pairs)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\ndiff --git a/src/train_softmax.py b/src/train_softmax.py\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex 6b0b28b..0000000\\\\\\\\\\\\\\\\n--- a/src/train_softmax.py\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1,580 +0,0 @@\\\\\\\\\\\\\\\\n-"""Training a face recognizer with TensorFlow using softmax cross entropy loss\\\\\\\\\\\\\\\\n-"""\\\\\\\\\\\\\\\\n-# MIT License\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\\\\\\\\\n-# SOFTWARE.\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-from __future__ import absolute_import\\\\\\\\\\\\\\\\n-from __future__ import division\\\\\\\\\\\\\\\\n-from __future__ import print_function\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-from datetime import datetime\\\\\\\\\\\\\\\\n-import os.path\\\\\\\\\\\\\\\\n-import time\\\\\\\\\\\\\\\\n-import sys\\\\\\\\\\\\\\\\n-import random\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\n-import importlib\\\\\\\\\\\\\\\\n-import argparse\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\n-import lfw\\\\\\\\\\\\\\\\n-import h5py\\\\\\\\\\\\\\\\n-import math\\\\\\\\\\\\\\\\n-import tensorflow.contrib.slim as slim\\\\\\\\\\\\\\\\n-from tensorflow.python.ops import data_flow_ops\\\\\\\\\\\\\\\\n-from tensorflow.python.framework import ops\\\\\\\\\\\\\\\\n-from tensorflow.python.ops import array_ops\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def main(args):\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-    network = importlib.import_module(args.model_def)\\\\\\\\\\\\\\\\n-    image_size = (args.image_size, args.image_size)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    subdir = datetime.strftime(datetime.now(), \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'%Y%m%d-%H%M%S\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\\\\\\\\\\\\\\\\n-    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'t exist\\\\\\\\\\\\\\\\n-        os.makedirs(log_dir)\\\\\\\\\\\\\\\\n-    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\\\\\\\\\\\\\\\\n-    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'t exist\\\\\\\\\\\\\\\\n-        os.makedirs(model_dir)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    stat_file_name = os.path.join(log_dir, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'stat.h5\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    # Write arguments to a text file\\\\\\\\\\\\\\\\n-    facenet.write_arguments_to_file(args, os.path.join(log_dir, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'arguments.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'))\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-    # Store some git revision info in a text file in the log directory\\\\\\\\\\\\\\\\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\\\\\\\\\\\\\\\\n-    facenet.store_revision_info(src_path, log_dir, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.join(sys.argv))\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    np.random.seed(seed=args.seed)\\\\\\\\\\\\\\\\n-    random.seed(args.seed)\\\\\\\\\\\\\\\\n-    dataset = facenet.get_dataset(args.data_dir)\\\\\\\\\\\\\\\\n-    if args.filter_filename:\\\\\\\\\\\\\\\\n-        dataset = filter_dataset(dataset, os.path.expanduser(args.filter_filename), \\\\\\\\\\\\\\\\n-            args.filter_percentile, args.filter_min_nrof_images_per_class)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-    if args.validation_set_split_ratio>0.0:\\\\\\\\\\\\\\\\n-        train_set, val_set = facenet.split_dataset(dataset, args.validation_set_split_ratio, args.min_nrof_val_images_per_class, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'SPLIT_IMAGES\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    else:\\\\\\\\\\\\\\\\n-        train_set, val_set = dataset, []\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-    nrof_classes = len(train_set)\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Model directory: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % model_dir)\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Log directory: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % log_dir)\\\\\\\\\\\\\\\\n-    pretrained_model = None\\\\\\\\\\\\\\\\n-    if args.pretrained_model:\\\\\\\\\\\\\\\\n-        pretrained_model = os.path.expanduser(args.pretrained_model)\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Pre-trained model: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % pretrained_model)\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    if args.lfw_dir:\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'LFW directory: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % args.lfw_dir)\\\\\\\\\\\\\\\\n-        # Read the file containing the pairs used for testing\\\\\\\\\\\\\\\\n-        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\\\\\\\\\\\\\\\\n-        # Get the paths for the corresponding images\\\\\\\\\\\\\\\\n-        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\\\\\\\\\n-        tf.set_random_seed(args.seed)\\\\\\\\\\\\\\\\n-        global_step = tf.Variable(0, trainable=False)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        # Get a list of image paths and their labels\\\\\\\\\\\\\\\\n-        image_list, label_list = facenet.get_image_paths_and_labels(train_set)\\\\\\\\\\\\\\\\n-        assert len(image_list)>0, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'The training set should not be empty\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        val_image_list, val_label_list = facenet.get_image_paths_and_labels(val_set)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Create a queue that produces indices into the image_list and label_list \\\\\\\\\\\\\\\\n-        labels = ops.convert_to_tensor(label_list, dtype=tf.int32)\\\\\\\\\\\\\\\\n-        range_size = array_ops.shape(labels)[0]\\\\\\\\\\\\\\\\n-        index_queue = tf.train.range_input_producer(range_size, num_epochs=None,\\\\\\\\\\\\\\\\n-                             shuffle=True, seed=None, capacity=32)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        index_dequeue_op = index_queue.dequeue_many(args.batch_size*args.epoch_size, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'index_dequeue\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        learning_rate_placeholder = tf.placeholder(tf.float32, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'learning_rate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        batch_size_placeholder = tf.placeholder(tf.int32, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        phase_train_placeholder = tf.placeholder(tf.bool, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'phase_train\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'image_paths\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'labels\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'control\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        nrof_preprocess_threads = 4\\\\\\\\\\\\\\\\n-        input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\\\\\\\\\\\\\\\\n-                                    dtypes=[tf.string, tf.int32, tf.int32],\\\\\\\\\\\\\\\\n-                                    shapes=[(1,), (1,), (1,)],\\\\\\\\\\\\\\\\n-                                    shared_name=None, name=None)\\\\\\\\\\\\\\\\n-        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder, control_placeholder], name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'enqueue_op\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        image_batch, label_batch = facenet.create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        image_batch = tf.identity(image_batch, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'image_batch\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        image_batch = tf.identity(image_batch, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        label_batch = tf.identity(label_batch, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'label_batch\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of classes in training set: %d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % nrof_classes)\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of examples in training set: %d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % len(image_list))\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of classes in validation set: %d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % len(val_set))\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of examples in validation set: %d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % len(val_image_list))\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Building training graph\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        # Build the inference graph\\\\\\\\\\\\\\\\n-        prelogits, _ = network.inference(image_batch, args.keep_probability, \\\\\\\\\\\\\\\\n-            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size, \\\\\\\\\\\\\\\\n-            weight_decay=args.weight_decay)\\\\\\\\\\\\\\\\n-        logits = slim.fully_connected(prelogits, len(train_set), activation_fn=None, \\\\\\\\\\\\\\\\n-                weights_initializer=slim.initializers.xavier_initializer(), \\\\\\\\\\\\\\\\n-                weights_regularizer=slim.l2_regularizer(args.weight_decay),\\\\\\\\\\\\\\\\n-                scope=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Logits\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', reuse=False)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'embeddings\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Norm for the prelogits\\\\\\\\\\\\\\\\n-        eps = 1e-4\\\\\\\\\\\\\\\\n-        prelogits_norm = tf.reduce_mean(tf.norm(tf.abs(prelogits)+eps, ord=args.prelogits_norm_p, axis=1))\\\\\\\\\\\\\\\\n-        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_norm * args.prelogits_norm_loss_factor)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Add center loss\\\\\\\\\\\\\\\\n-        prelogits_center_loss, _ = facenet.center_loss(prelogits, label_batch, args.center_loss_alfa, nrof_classes)\\\\\\\\\\\\\\\\n-        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_center_loss * args.center_loss_factor)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\\\\\\\\\\\\\\\\n-            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\\\\\\\\\\\\\\\\n-        tf.summary.scalar(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'learning_rate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', learning_rate)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Calculate the average cross entropy loss across the batch\\\\\\\\\\\\\\\\n-        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\\\\\\\\\\\\\\\\n-            labels=label_batch, logits=logits, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'cross_entropy_per_example\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'cross_entropy\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        tf.add_to_collection(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'losses\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', cross_entropy_mean)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        correct_prediction = tf.cast(tf.equal(tf.argmax(logits, 1), tf.cast(label_batch, tf.int64)), tf.float32)\\\\\\\\\\\\\\\\n-        accuracy = tf.reduce_mean(correct_prediction)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        # Calculate the total losses\\\\\\\\\\\\\\\\n-        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\\\\\\\\\\\\\\\\n-        total_loss = tf.add_n([cross_entropy_mean] + regularization_losses, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'total_loss\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Build a Graph that trains the model with one batch of examples and updates the model parameters\\\\\\\\\\\\\\\\n-        train_op = facenet.train(total_loss, global_step, args.optimizer, \\\\\\\\\\\\\\\\n-            learning_rate, args.moving_average_decay, tf.global_variables(), args.log_histograms)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        # Create a saver\\\\\\\\\\\\\\\\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Build the summary operation based on the TF collection of Summaries.\\\\\\\\\\\\\\\\n-        summary_op = tf.summary.merge_all()\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Start running operations on the Graph.\\\\\\\\\\\\\\\\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\\\\\\\\\\\\\\\\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\\\\\\\\\\\\\n-        sess.run(tf.global_variables_initializer())\\\\\\\\\\\\\\\\n-        sess.run(tf.local_variables_initializer())\\\\\\\\\\\\\\\\n-        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\\\\\\\\\\\\\\\\n-        coord = tf.train.Coordinator()\\\\\\\\\\\\\\\\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        with sess.as_default():\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            if pretrained_model:\\\\\\\\\\\\\\\\n-                print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Restoring pretrained model: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % pretrained_model)\\\\\\\\\\\\\\\\n-                saver.restore(sess, pretrained_model)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            # Training and validation loop\\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Running training\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-            nrof_steps = args.max_nrof_epochs*args.epoch_size\\\\\\\\\\\\\\\\n-            nrof_val_samples = int(math.ceil(args.max_nrof_epochs / args.validate_every_n_epochs))   # Validate every validate_every_n_epochs as well as in the last epoch\\\\\\\\\\\\\\\\n-            stat = {\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'loss\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'center_loss\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'reg_loss\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'xent_loss\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'prelogits_norm\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'accuracy\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': np.zeros((nrof_steps,), np.float32),\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'val_loss\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': np.zeros((nrof_val_samples,), np.float32),\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'val_xent_loss\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': np.zeros((nrof_val_samples,), np.float32),\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'val_accuracy\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': np.zeros((nrof_val_samples,), np.float32),\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'lfw_accuracy\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'lfw_valrate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'learning_rate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'time_train\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'time_validate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'time_evaluate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': np.zeros((args.max_nrof_epochs,), np.float32),\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'prelogits_hist\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': np.zeros((args.max_nrof_epochs, 1000), np.float32),\\\\\\\\\\\\\\\\n-              }\\\\\\\\\\\\\\\\n-            for epoch in range(1,args.max_nrof_epochs+1):\\\\\\\\\\\\\\\\n-                step = sess.run(global_step, feed_dict=None)\\\\\\\\\\\\\\\\n-                # Train for one epoch\\\\\\\\\\\\\\\\n-                t = time.time()\\\\\\\\\\\\\\\\n-                cont = train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder,\\\\\\\\\\\\\\\\n-                    learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, global_step, \\\\\\\\\\\\\\\\n-                    total_loss, train_op, summary_op, summary_writer, regularization_losses, args.learning_rate_schedule_file,\\\\\\\\\\\\\\\\n-                    stat, cross_entropy_mean, accuracy, learning_rate,\\\\\\\\\\\\\\\\n-                    prelogits, prelogits_center_loss, args.random_rotate, args.random_crop, args.random_flip, prelogits_norm, args.prelogits_hist_max, args.use_fixed_image_standardization)\\\\\\\\\\\\\\\\n-                stat[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'time_train\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'][epoch-1] = time.time() - t\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\n-                if not cont:\\\\\\\\\\\\\\\\n-                    break\\\\\\\\\\\\\\\\n-                  \\\\\\\\\\\\\\\\n-                t = time.time()\\\\\\\\\\\\\\\\n-                if len(val_image_list)>0 and ((epoch-1) % args.validate_every_n_epochs == args.validate_every_n_epochs-1 or epoch==args.max_nrof_epochs):\\\\\\\\\\\\\\\\n-                    validate(args, sess, epoch, val_image_list, val_label_list, enqueue_op, image_paths_placeholder, labels_placeholder, control_placeholder,\\\\\\\\\\\\\\\\n-                        phase_train_placeholder, batch_size_placeholder, \\\\\\\\\\\\\\\\n-                        stat, total_loss, regularization_losses, cross_entropy_mean, accuracy, args.validate_every_n_epochs, args.use_fixed_image_standardization)\\\\\\\\\\\\\\\\n-                stat[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'time_validate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'][epoch-1] = time.time() - t\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                # Save variables and the metagraph if it doesn\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'t exist already\\\\\\\\\\\\\\\\n-                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, epoch)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                # Evaluate on LFW\\\\\\\\\\\\\\\\n-                t = time.time()\\\\\\\\\\\\\\\\n-                if args.lfw_dir:\\\\\\\\\\\\\\\\n-                    evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, \\\\\\\\\\\\\\\\n-                        embeddings, label_batch, lfw_paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, log_dir, step, summary_writer, stat, epoch, \\\\\\\\\\\\\\\\n-                        args.lfw_distance_metric, args.lfw_subtract_mean, args.lfw_use_flipped_images, args.use_fixed_image_standardization)\\\\\\\\\\\\\\\\n-                stat[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'time_evaluate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'][epoch-1] = time.time() - t\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Saving statistics\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-                with h5py.File(stat_file_name, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'w\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as f:\\\\\\\\\\\\\\\\n-                    for key, value in stat.iteritems():\\\\\\\\\\\\\\\\n-                        f.create_dataset(key, data=value)\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    return model_dir\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-def find_threshold(var, percentile):\\\\\\\\\\\\\\\\n-    hist, bin_edges = np.histogram(var, 100)\\\\\\\\\\\\\\\\n-    cdf = np.float32(np.cumsum(hist)) / np.sum(hist)\\\\\\\\\\\\\\\\n-    bin_centers = (bin_edges[:-1]+bin_edges[1:])/2\\\\\\\\\\\\\\\\n-    #plt.plot(bin_centers, cdf)\\\\\\\\\\\\\\\\n-    threshold = np.interp(percentile*0.01, cdf, bin_centers)\\\\\\\\\\\\\\\\n-    return threshold\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-def filter_dataset(dataset, data_filename, percentile, min_nrof_images_per_class):\\\\\\\\\\\\\\\\n-    with h5py.File(data_filename,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as f:\\\\\\\\\\\\\\\\n-        distance_to_center = np.array(f.get(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'distance_to_center\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'))\\\\\\\\\\\\\\\\n-        label_list = np.array(f.get(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'label_list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'))\\\\\\\\\\\\\\\\n-        image_list = np.array(f.get(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'image_list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'))\\\\\\\\\\\\\\\\n-        distance_to_center_threshold = find_threshold(distance_to_center, percentile)\\\\\\\\\\\\\\\\n-        indices = np.where(distance_to_center>=distance_to_center_threshold)[0]\\\\\\\\\\\\\\\\n-        filtered_dataset = dataset\\\\\\\\\\\\\\\\n-        removelist = []\\\\\\\\\\\\\\\\n-        for i in indices:\\\\\\\\\\\\\\\\n-            label = label_list[i]\\\\\\\\\\\\\\\\n-            image = image_list[i]\\\\\\\\\\\\\\\\n-            if image in filtered_dataset[label].image_paths:\\\\\\\\\\\\\\\\n-                filtered_dataset[label].image_paths.remove(image)\\\\\\\\\\\\\\\\n-            if len(filtered_dataset[label].image_paths)<min_nrof_images_per_class:\\\\\\\\\\\\\\\\n-                removelist.append(label)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        ix = sorted(list(set(removelist)), reverse=True)\\\\\\\\\\\\\\\\n-        for i in ix:\\\\\\\\\\\\\\\\n-            del(filtered_dataset[i])\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    return filtered_dataset\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-def train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder, \\\\\\\\\\\\\\\\n-      learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, step, \\\\\\\\\\\\\\\\n-      loss, train_op, summary_op, summary_writer, reg_losses, learning_rate_schedule_file, \\\\\\\\\\\\\\\\n-      stat, cross_entropy_mean, accuracy, \\\\\\\\\\\\\\\\n-      learning_rate, prelogits, prelogits_center_loss, random_rotate, random_crop, random_flip, prelogits_norm, prelogits_hist_max, use_fixed_image_standardization):\\\\\\\\\\\\\\\\n-    batch_number = 0\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    if args.learning_rate>0.0:\\\\\\\\\\\\\\\\n-        lr = args.learning_rate\\\\\\\\\\\\\\\\n-    else:\\\\\\\\\\\\\\\\n-        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-    if lr<=0:\\\\\\\\\\\\\\\\n-        return False \\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    index_epoch = sess.run(index_dequeue_op)\\\\\\\\\\\\\\\\n-    label_epoch = np.array(label_list)[index_epoch]\\\\\\\\\\\\\\\\n-    image_epoch = np.array(image_list)[index_epoch]\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    # Enqueue one epoch of image paths and labels\\\\\\\\\\\\\\\\n-    labels_array = np.expand_dims(np.array(label_epoch),1)\\\\\\\\\\\\\\\\n-    image_paths_array = np.expand_dims(np.array(image_epoch),1)\\\\\\\\\\\\\\\\n-    control_value = facenet.RANDOM_ROTATE * random_rotate + facenet.RANDOM_CROP * random_crop + facenet.RANDOM_FLIP * random_flip + facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization\\\\\\\\\\\\\\\\n-    control_array = np.ones_like(labels_array) * control_value\\\\\\\\\\\\\\\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    # Training loop\\\\\\\\\\\\\\\\n-    train_time = 0\\\\\\\\\\\\\\\\n-    while batch_number < args.epoch_size:\\\\\\\\\\\\\\\\n-        start_time = time.time()\\\\\\\\\\\\\\\\n-        feed_dict = {learning_rate_placeholder: lr, phase_train_placeholder:True, batch_size_placeholder:args.batch_size}\\\\\\\\\\\\\\\\n-        tensor_list = [loss, train_op, step, reg_losses, prelogits, cross_entropy_mean, learning_rate, prelogits_norm, accuracy, prelogits_center_loss]\\\\\\\\\\\\\\\\n-        if batch_number % 100 == 0:\\\\\\\\\\\\\\\\n-            loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_, summary_str = sess.run(tensor_list + [summary_op], feed_dict=feed_dict)\\\\\\\\\\\\\\\\n-            summary_writer.add_summary(summary_str, global_step=step_)\\\\\\\\\\\\\\\\n-        else:\\\\\\\\\\\\\\\\n-            loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_ = sess.run(tensor_list, feed_dict=feed_dict)\\\\\\\\\\\\\\\\n-         \\\\\\\\\\\\\\\\n-        duration = time.time() - start_time\\\\\\\\\\\\\\\\n-        stat[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'loss\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'][step_-1] = loss_\\\\\\\\\\\\\\\\n-        stat[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'center_loss\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'][step_-1] = center_loss_\\\\\\\\\\\\\\\\n-        stat[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'reg_loss\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'][step_-1] = np.sum(reg_losses_)\\\\\\\\\\\\\\\\n-        stat[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'xent_loss\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'][step_-1] = cross_entropy_mean_\\\\\\\\\\\\\\\\n-        stat[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'prelogits_norm\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'][step_-1] = prelogits_norm_\\\\\\\\\\\\\\\\n-        stat[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'learning_rate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'][epoch-1] = lr_\\\\\\\\\\\\\\\\n-        stat[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'accuracy\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'][step_-1] = accuracy_\\\\\\\\\\\\\\\\n-        stat[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'prelogits_hist\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'][epoch-1,:] += np.histogram(np.minimum(np.abs(prelogits_), prelogits_hist_max), bins=1000, range=(0.0, prelogits_hist_max))[0]\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        duration = time.time() - start_time\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Epoch: [%d][%d/%d]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\tTime %.3f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\tLoss %2.3f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\tXent %2.3f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\tRegLoss %2.3f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\tAccuracy %2.3f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\tLr %2.5f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\tCl %2.3f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' %\\\\\\\\\\\\\\\\n-              (epoch, batch_number+1, args.epoch_size, duration, loss_, cross_entropy_mean_, np.sum(reg_losses_), accuracy_, lr_, center_loss_))\\\\\\\\\\\\\\\\n-        batch_number += 1\\\\\\\\\\\\\\\\n-        train_time += duration\\\\\\\\\\\\\\\\n-    # Add validation loss and accuracy to summary\\\\\\\\\\\\\\\\n-    summary = tf.Summary()\\\\\\\\\\\\\\\\n-    #pylint: disable=maybe-no-member\\\\\\\\\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'time/total\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', simple_value=train_time)\\\\\\\\\\\\\\\\n-    summary_writer.add_summary(summary, global_step=step_)\\\\\\\\\\\\\\\\n-    return True\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def validate(args, sess, epoch, image_list, label_list, enqueue_op, image_paths_placeholder, labels_placeholder, control_placeholder,\\\\\\\\\\\\\\\\n-             phase_train_placeholder, batch_size_placeholder, \\\\\\\\\\\\\\\\n-             stat, loss, regularization_losses, cross_entropy_mean, accuracy, validate_every_n_epochs, use_fixed_image_standardization):\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Running forward pass on validation set\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    nrof_batches = len(label_list) // args.lfw_batch_size\\\\\\\\\\\\\\\\n-    nrof_images = nrof_batches * args.lfw_batch_size\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    # Enqueue one epoch of image paths and labels\\\\\\\\\\\\\\\\n-    labels_array = np.expand_dims(np.array(label_list[:nrof_images]),1)\\\\\\\\\\\\\\\\n-    image_paths_array = np.expand_dims(np.array(image_list[:nrof_images]),1)\\\\\\\\\\\\\\\\n-    control_array = np.ones_like(labels_array, np.int32)*facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization\\\\\\\\\\\\\\\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    loss_array = np.zeros((nrof_batches,), np.float32)\\\\\\\\\\\\\\\\n-    xent_array = np.zeros((nrof_batches,), np.float32)\\\\\\\\\\\\\\\\n-    accuracy_array = np.zeros((nrof_batches,), np.float32)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    # Training loop\\\\\\\\\\\\\\\\n-    start_time = time.time()\\\\\\\\\\\\\\\\n-    for i in range(nrof_batches):\\\\\\\\\\\\\\\\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:args.lfw_batch_size}\\\\\\\\\\\\\\\\n-        loss_, cross_entropy_mean_, accuracy_ = sess.run([loss, cross_entropy_mean, accuracy], feed_dict=feed_dict)\\\\\\\\\\\\\\\\n-        loss_array[i], xent_array[i], accuracy_array[i] = (loss_, cross_entropy_mean_, accuracy_)\\\\\\\\\\\\\\\\n-        if i % 10 == 9:\\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', end=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-            sys.stdout.flush()\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    duration = time.time() - start_time\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    val_index = (epoch-1)//validate_every_n_epochs\\\\\\\\\\\\\\\\n-    stat[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'val_loss\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'][val_index] = np.mean(loss_array)\\\\\\\\\\\\\\\\n-    stat[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'val_xent_loss\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'][val_index] = np.mean(xent_array)\\\\\\\\\\\\\\\\n-    stat[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'val_accuracy\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'][val_index] = np.mean(accuracy_array)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Validation Epoch: %d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\tTime %.3f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\tLoss %2.3f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\tXent %2.3f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\tAccuracy %2.3f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' %\\\\\\\\\\\\\\\\n-          (epoch, duration, np.mean(loss_array), np.mean(xent_array), np.mean(accuracy_array)))\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, \\\\\\\\\\\\\\\\n-        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, log_dir, step, summary_writer, stat, epoch, distance_metric, subtract_mean, use_flipped_images, use_fixed_image_standardization):\\\\\\\\\\\\\\\\n-    start_time = time.time()\\\\\\\\\\\\\\\\n-    # Run forward pass to calculate embeddings\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Runnning forward pass on LFW images\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    # Enqueue one epoch of image paths and labels\\\\\\\\\\\\\\\\n-    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\\\\\\\\\\\\\\\\n-    nrof_flips = 2 if use_flipped_images else 1\\\\\\\\\\\\\\\\n-    nrof_images = nrof_embeddings * nrof_flips\\\\\\\\\\\\\\\\n-    labels_array = np.expand_dims(np.arange(0,nrof_images),1)\\\\\\\\\\\\\\\\n-    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\\\\\\\\\\\\\\\\n-    control_array = np.zeros_like(labels_array, np.int32)\\\\\\\\\\\\\\\\n-    if use_fixed_image_standardization:\\\\\\\\\\\\\\\\n-        control_array += np.ones_like(labels_array)*facenet.FIXED_STANDARDIZATION\\\\\\\\\\\\\\\\n-    if use_flipped_images:\\\\\\\\\\\\\\\\n-        # Flip every second image\\\\\\\\\\\\\\\\n-        control_array += (labels_array % 2)*facenet.FLIP\\\\\\\\\\\\\\\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    embedding_size = int(embeddings.get_shape()[1])\\\\\\\\\\\\\\\\n-    assert nrof_images % batch_size == 0, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'The number of LFW images must be an integer multiple of the LFW batch size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-    nrof_batches = nrof_images // batch_size\\\\\\\\\\\\\\\\n-    emb_array = np.zeros((nrof_images, embedding_size))\\\\\\\\\\\\\\\\n-    lab_array = np.zeros((nrof_images,))\\\\\\\\\\\\\\\\n-    for i in range(nrof_batches):\\\\\\\\\\\\\\\\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}\\\\\\\\\\\\\\\\n-        emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)\\\\\\\\\\\\\\\\n-        lab_array[lab] = lab\\\\\\\\\\\\\\\\n-        emb_array[lab, :] = emb\\\\\\\\\\\\\\\\n-        if i % 10 == 9:\\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', end=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-            sys.stdout.flush()\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    embeddings = np.zeros((nrof_embeddings, embedding_size*nrof_flips))\\\\\\\\\\\\\\\\n-    if use_flipped_images:\\\\\\\\\\\\\\\\n-        # Concatenate embeddings for flipped and non flipped version of the images\\\\\\\\\\\\\\\\n-        embeddings[:,:embedding_size] = emb_array[0::2,:]\\\\\\\\\\\\\\\\n-        embeddings[:,embedding_size:] = emb_array[1::2,:]\\\\\\\\\\\\\\\\n-    else:\\\\\\\\\\\\\\\\n-        embeddings = emb_array\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    assert np.array_equal(lab_array, np.arange(nrof_images))==True, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-    _, _, accuracy, val, val_std, far = lfw.evaluate(embeddings, actual_issame, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Accuracy: %2.5f+-%2.5f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % (np.mean(accuracy), np.std(accuracy)))\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % (val, val_std, far))\\\\\\\\\\\\\\\\n-    lfw_time = time.time() - start_time\\\\\\\\\\\\\\\\n-    # Add validation loss and accuracy to summary\\\\\\\\\\\\\\\\n-    summary = tf.Summary()\\\\\\\\\\\\\\\\n-    #pylint: disable=maybe-no-member\\\\\\\\\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'lfw/accuracy\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', simple_value=np.mean(accuracy))\\\\\\\\\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'lfw/val_rate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', simple_value=val)\\\\\\\\\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'time/lfw\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', simple_value=lfw_time)\\\\\\\\\\\\\\\\n-    summary_writer.add_summary(summary, step)\\\\\\\\\\\\\\\\n-    with open(os.path.join(log_dir,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'lfw_result.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'),\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'at\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as f:\\\\\\\\\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'%d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\t%.5f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\t%.5f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % (step, np.mean(accuracy), val))\\\\\\\\\\\\\\\\n-    stat[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'lfw_accuracy\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'][epoch-1] = np.mean(accuracy)\\\\\\\\\\\\\\\\n-    stat[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'lfw_valrate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'][epoch-1] = val\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\\\\\\\\\\\\\\\\n-    # Save the model checkpoint\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Saving variables\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    start_time = time.time()\\\\\\\\\\\\\\\\n-    checkpoint_path = os.path.join(model_dir, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'model-%s.ckpt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % model_name)\\\\\\\\\\\\\\\\n-    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\\\\\\\\\\\\\\\\n-    save_time_variables = time.time() - start_time\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Variables saved in %.2f seconds\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % save_time_variables)\\\\\\\\\\\\\\\\n-    metagraph_filename = os.path.join(model_dir, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'model-%s.meta\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % model_name)\\\\\\\\\\\\\\\\n-    save_time_metagraph = 0  \\\\\\\\\\\\\\\\n-    if not os.path.exists(metagraph_filename):\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Saving metagraph\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        start_time = time.time()\\\\\\\\\\\\\\\\n-        saver.export_meta_graph(metagraph_filename)\\\\\\\\\\\\\\\\n-        save_time_metagraph = time.time() - start_time\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Metagraph saved in %.2f seconds\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % save_time_metagraph)\\\\\\\\\\\\\\\\n-    summary = tf.Summary()\\\\\\\\\\\\\\\\n-    #pylint: disable=maybe-no-member\\\\\\\\\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'time/save_variables\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', simple_value=save_time_variables)\\\\\\\\\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'time/save_metagraph\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', simple_value=save_time_metagraph)\\\\\\\\\\\\\\\\n-    summary_writer.add_summary(summary, step)\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def parse_arguments(argv):\\\\\\\\\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--logs_base_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str, \\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Directory where to write event logs.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'~/logs/facenet\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--models_base_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Directory where to write trained models and checkpoints.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'~/models/facenet\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--gpu_memory_fraction\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Upper bound on the amount of GPU memory that will be used by the process.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=1.0)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--pretrained_model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Load a pretrained model before training starts.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--data_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Path to the data directory containing aligned face patches.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-        default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--model_def\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Model definition. Points to a module containing the definition of the inference graph.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'models.inception_resnet_v1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--max_nrof_epochs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of epochs to run.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=500)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of images to process in a batch.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=90)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--image_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Image size (height, width) in pixels.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=160)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--epoch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of batches per epoch.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=1000)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--embedding_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Dimensionality of the embedding.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=128)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--random_crop\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' +\\\\\\\\\\\\\\\\n-         \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'If the size of the images in the data directory is equal to image_size no cropping is performed\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--random_flip\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Performs random horizontal flipping of training images.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--random_rotate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Performs random rotations of training images.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--use_fixed_image_standardization\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Performs fixed standardization of images.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--keep_probability\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Keep probability of dropout for the fully connected layer(s).\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=1.0)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--weight_decay\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'L2 weight regularization.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0.0)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--center_loss_factor\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Center loss factor.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0.0)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--center_loss_alfa\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Center update rate for center loss.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0.95)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--prelogits_norm_loss_factor\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Loss based on the norm of the activations in the prelogits layer.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0.0)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--prelogits_norm_p\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Norm to use for prelogits norm loss.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=1.0)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--prelogits_hist_max\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'The max value for the prelogits histogram.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=10.0)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--optimizer\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str, choices=[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'ADAGRAD\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'ADADELTA\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'ADAM\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'RMSPROP\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'MOM\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'],\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'The optimization algorithm to use\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'ADAGRAD\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--learning_rate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Initial learning rate. If set to a negative value a learning rate \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' +\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'schedule can be specified in the file "learning_rate_schedule.txt"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0.1)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--learning_rate_decay_epochs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of epochs between learning rate decay.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=100)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--learning_rate_decay_factor\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Learning rate decay factor.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=1.0)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--moving_average_decay\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Exponential decay for tracking of training parameters.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0.9999)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--seed\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Random seed.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=666)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--nrof_preprocess_threads\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of preprocessing (data loading and augmentation) threads.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=4)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--log_histograms\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Enables logging of weight/bias histograms in tensorboard.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--learning_rate_schedule_file\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data/learning_rate_schedule.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--filter_filename\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'File containing image data used for dataset filtering\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--filter_percentile\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Keep only the percentile images closed to its class center\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=100.0)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--filter_min_nrof_images_per_class\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Keep only the classes with this number of examples or more\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--validate_every_n_epochs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of epoch between validation\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=5)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--validation_set_split_ratio\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'The ratio of the total dataset to use for validation\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0.0)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--min_nrof_val_images_per_class\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Classes with fewer images will be removed from the validation set\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0)\\\\\\\\\\\\\\\\n- \\\\\\\\\\\\\\\\n-    # Parameters for validation on LFW\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'The file containing the pairs to use for validation.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data/pairs.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Path to the data directory containing aligned face patches.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of images to process in a batch in the LFW test set.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=100)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of folds to use for cross validation. Mainly used for testing.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=10)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_distance_metric\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Type of distance metric to use. 0: Euclidian, 1:Cosine similarity distance.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_use_flipped_images\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Concatenates embeddings for the image and its horizontally flipped counterpart.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_subtract_mean\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Subtract feature mean before calculating distance.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    return parser.parse_args(argv)\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\\\\\\\\\\\\\ndiff --git a/src/train_tripletloss.py b/src/train_tripletloss.py\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex d6df19a..0000000\\\\\\\\\\\\\\\\n--- a/src/train_tripletloss.py\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1,486 +0,0 @@\\\\\\\\\\\\\\\\n-"""Training a face recognizer with TensorFlow based on the FaceNet paper\\\\\\\\\\\\\\\\n-FaceNet: A Unified Embedding for Face Recognition and Clustering: http://arxiv.org/abs/1503.03832\\\\\\\\\\\\\\\\n-"""\\\\\\\\\\\\\\\\n-# MIT License\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\\\\\\\\\n-# SOFTWARE.\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-from __future__ import absolute_import\\\\\\\\\\\\\\\\n-from __future__ import division\\\\\\\\\\\\\\\\n-from __future__ import print_function\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-from datetime import datetime\\\\\\\\\\\\\\\\n-import os.path\\\\\\\\\\\\\\\\n-import time\\\\\\\\\\\\\\\\n-import sys\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\n-import importlib\\\\\\\\\\\\\\\\n-import itertools\\\\\\\\\\\\\\\\n-import argparse\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\n-import lfw\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-from tensorflow.python.ops import data_flow_ops\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-from six.moves import xrange  # @UnresolvedImport\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def main(args):\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-    network = importlib.import_module(args.model_def)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    subdir = datetime.strftime(datetime.now(), \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'%Y%m%d-%H%M%S\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\\\\\\\\\\\\\\\\n-    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'t exist\\\\\\\\\\\\\\\\n-        os.makedirs(log_dir)\\\\\\\\\\\\\\\\n-    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\\\\\\\\\\\\\\\\n-    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'t exist\\\\\\\\\\\\\\\\n-        os.makedirs(model_dir)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    # Write arguments to a text file\\\\\\\\\\\\\\\\n-    facenet.write_arguments_to_file(args, os.path.join(log_dir, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'arguments.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'))\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-    # Store some git revision info in a text file in the log directory\\\\\\\\\\\\\\\\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\\\\\\\\\\\\\\\\n-    facenet.store_revision_info(src_path, log_dir, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.join(sys.argv))\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    np.random.seed(seed=args.seed)\\\\\\\\\\\\\\\\n-    train_set = facenet.get_dataset(args.data_dir)\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Model directory: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % model_dir)\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Log directory: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % log_dir)\\\\\\\\\\\\\\\\n-    if args.pretrained_model:\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Pre-trained model: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % os.path.expanduser(args.pretrained_model))\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    if args.lfw_dir:\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'LFW directory: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % args.lfw_dir)\\\\\\\\\\\\\\\\n-        # Read the file containing the pairs used for testing\\\\\\\\\\\\\\\\n-        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\\\\\\\\\\\\\\\\n-        # Get the paths for the corresponding images\\\\\\\\\\\\\\\\n-        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\\\\\\\\\n-        tf.set_random_seed(args.seed)\\\\\\\\\\\\\\\\n-        global_step = tf.Variable(0, trainable=False)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Placeholder for the learning rate\\\\\\\\\\\\\\\\n-        learning_rate_placeholder = tf.placeholder(tf.float32, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'learning_rate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        batch_size_placeholder = tf.placeholder(tf.int32, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        phase_train_placeholder = tf.placeholder(tf.bool, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'phase_train\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,3), name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'image_paths\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        labels_placeholder = tf.placeholder(tf.int64, shape=(None,3), name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'labels\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        input_queue = data_flow_ops.FIFOQueue(capacity=100000,\\\\\\\\\\\\\\\\n-                                    dtypes=[tf.string, tf.int64],\\\\\\\\\\\\\\\\n-                                    shapes=[(3,), (3,)],\\\\\\\\\\\\\\\\n-                                    shared_name=None, name=None)\\\\\\\\\\\\\\\\n-        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder])\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        nrof_preprocess_threads = 4\\\\\\\\\\\\\\\\n-        images_and_labels = []\\\\\\\\\\\\\\\\n-        for _ in range(nrof_preprocess_threads):\\\\\\\\\\\\\\\\n-            filenames, label = input_queue.dequeue()\\\\\\\\\\\\\\\\n-            images = []\\\\\\\\\\\\\\\\n-            for filename in tf.unstack(filenames):\\\\\\\\\\\\\\\\n-                file_contents = tf.read_file(filename)\\\\\\\\\\\\\\\\n-                image = tf.image.decode_image(file_contents, channels=3)\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\n-                if args.random_crop:\\\\\\\\\\\\\\\\n-                    image = tf.random_crop(image, [args.image_size, args.image_size, 3])\\\\\\\\\\\\\\\\n-                else:\\\\\\\\\\\\\\\\n-                    image = tf.image.resize_image_with_crop_or_pad(image, args.image_size, args.image_size)\\\\\\\\\\\\\\\\n-                if args.random_flip:\\\\\\\\\\\\\\\\n-                    image = tf.image.random_flip_left_right(image)\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-                #pylint: disable=no-member\\\\\\\\\\\\\\\\n-                image.set_shape((args.image_size, args.image_size, 3))\\\\\\\\\\\\\\\\n-                images.append(tf.image.per_image_standardization(image))\\\\\\\\\\\\\\\\n-            images_and_labels.append([images, label])\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-        image_batch, labels_batch = tf.train.batch_join(\\\\\\\\\\\\\\\\n-            images_and_labels, batch_size=batch_size_placeholder, \\\\\\\\\\\\\\\\n-            shapes=[(args.image_size, args.image_size, 3), ()], enqueue_many=True,\\\\\\\\\\\\\\\\n-            capacity=4 * nrof_preprocess_threads * args.batch_size,\\\\\\\\\\\\\\\\n-            allow_smaller_final_batch=True)\\\\\\\\\\\\\\\\n-        image_batch = tf.identity(image_batch, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'image_batch\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        image_batch = tf.identity(image_batch, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        labels_batch = tf.identity(labels_batch, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'label_batch\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Build the inference graph\\\\\\\\\\\\\\\\n-        prelogits, _ = network.inference(image_batch, args.keep_probability, \\\\\\\\\\\\\\\\n-            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size,\\\\\\\\\\\\\\\\n-            weight_decay=args.weight_decay)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'embeddings\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        # Split embeddings into anchor, positive and negative and calculate triplet loss\\\\\\\\\\\\\\\\n-        anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\\\\\\\\\\\\\\\\n-        triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\\\\\\\\\\\\\\\\n-            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\\\\\\\\\\\\\\\\n-        tf.summary.scalar(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'learning_rate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', learning_rate)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Calculate the total losses\\\\\\\\\\\\\\\\n-        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\\\\\\\\\\\\\\\\n-        total_loss = tf.add_n([triplet_loss] + regularization_losses, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'total_loss\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Build a Graph that trains the model with one batch of examples and updates the model parameters\\\\\\\\\\\\\\\\n-        train_op = facenet.train(total_loss, global_step, args.optimizer, \\\\\\\\\\\\\\\\n-            learning_rate, args.moving_average_decay, tf.global_variables())\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        # Create a saver\\\\\\\\\\\\\\\\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Build the summary operation based on the TF collection of Summaries.\\\\\\\\\\\\\\\\n-        summary_op = tf.summary.merge_all()\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Start running operations on the Graph.\\\\\\\\\\\\\\\\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\\\\\\\\\\\\\\\\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))        \\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Initialize variables\\\\\\\\\\\\\\\\n-        sess.run(tf.global_variables_initializer(), feed_dict={phase_train_placeholder:True})\\\\\\\\\\\\\\\\n-        sess.run(tf.local_variables_initializer(), feed_dict={phase_train_placeholder:True})\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\\\\\\\\\\\\\\\\n-        coord = tf.train.Coordinator()\\\\\\\\\\\\\\\\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        with sess.as_default():\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            if args.pretrained_model:\\\\\\\\\\\\\\\\n-                print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Restoring pretrained model: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % args.pretrained_model)\\\\\\\\\\\\\\\\n-                saver.restore(sess, os.path.expanduser(args.pretrained_model))\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            # Training and validation loop\\\\\\\\\\\\\\\\n-            epoch = 0\\\\\\\\\\\\\\\\n-            while epoch < args.max_nrof_epochs:\\\\\\\\\\\\\\\\n-                step = sess.run(global_step, feed_dict=None)\\\\\\\\\\\\\\\\n-                epoch = step // args.epoch_size\\\\\\\\\\\\\\\\n-                # Train for one epoch\\\\\\\\\\\\\\\\n-                train(args, sess, train_set, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\\\\\\\\\\\\\\\\n-                    batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \\\\\\\\\\\\\\\\n-                    embeddings, total_loss, train_op, summary_op, summary_writer, args.learning_rate_schedule_file,\\\\\\\\\\\\\\\\n-                    args.embedding_size, anchor, positive, negative, triplet_loss)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                # Save variables and the metagraph if it doesn\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'t exist already\\\\\\\\\\\\\\\\n-                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, step)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                # Evaluate on LFW\\\\\\\\\\\\\\\\n-                if args.lfw_dir:\\\\\\\\\\\\\\\\n-                    evaluate(sess, lfw_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \\\\\\\\\\\\\\\\n-                            batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, args.batch_size, \\\\\\\\\\\\\\\\n-                            args.lfw_nrof_folds, log_dir, step, summary_writer, args.embedding_size)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    return model_dir\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\\\\\\\\\\\\\\\\n-          batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \\\\\\\\\\\\\\\\n-          embeddings, loss, train_op, summary_op, summary_writer, learning_rate_schedule_file,\\\\\\\\\\\\\\\\n-          embedding_size, anchor, positive, negative, triplet_loss):\\\\\\\\\\\\\\\\n-    batch_number = 0\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    if args.learning_rate>0.0:\\\\\\\\\\\\\\\\n-        lr = args.learning_rate\\\\\\\\\\\\\\\\n-    else:\\\\\\\\\\\\\\\\n-        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\\\\\\\\\\\\\\\\n-    while batch_number < args.epoch_size:\\\\\\\\\\\\\\\\n-        # Sample people randomly from the dataset\\\\\\\\\\\\\\\\n-        image_paths, num_per_class = sample_people(dataset, args.people_per_batch, args.images_per_person)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Running forward pass on sampled images: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', end=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        start_time = time.time()\\\\\\\\\\\\\\\\n-        nrof_examples = args.people_per_batch * args.images_per_person\\\\\\\\\\\\\\\\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\\\\\\\\\\\\\\\\n-        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\\\\\\\\\\\\\n-        sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\\\\\\\\\\\\\n-        emb_array = np.zeros((nrof_examples, embedding_size))\\\\\\\\\\\\\\\\n-        nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\\\\\\\\\\\\\\\\n-        for i in range(nrof_batches):\\\\\\\\\\\\\\\\n-            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\\\\\\\\\\\\\\\\n-            emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size, \\\\\\\\\\\\\\\\n-                learning_rate_placeholder: lr, phase_train_placeholder: True})\\\\\\\\\\\\\\\\n-            emb_array[lab,:] = emb\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'%.3f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % (time.time()-start_time))\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Select triplets based on the embeddings\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Selecting suitable triplets for training\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        triplets, nrof_random_negs, nrof_triplets = select_triplets(emb_array, num_per_class, \\\\\\\\\\\\\\\\n-            image_paths, args.people_per_batch, args.alpha)\\\\\\\\\\\\\\\\n-        selection_time = time.time() - start_time\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'(nrof_random_negs, nrof_triplets) = (%d, %d): time=%.3f seconds\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % \\\\\\\\\\\\\\\\n-            (nrof_random_negs, nrof_triplets, selection_time))\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Perform training on the selected triplets\\\\\\\\\\\\\\\\n-        nrof_batches = int(np.ceil(nrof_triplets*3/args.batch_size))\\\\\\\\\\\\\\\\n-        triplet_paths = list(itertools.chain(*triplets))\\\\\\\\\\\\\\\\n-        labels_array = np.reshape(np.arange(len(triplet_paths)),(-1,3))\\\\\\\\\\\\\\\\n-        triplet_paths_array = np.reshape(np.expand_dims(np.array(triplet_paths),1), (-1,3))\\\\\\\\\\\\\\\\n-        sess.run(enqueue_op, {image_paths_placeholder: triplet_paths_array, labels_placeholder: labels_array})\\\\\\\\\\\\\\\\n-        nrof_examples = len(triplet_paths)\\\\\\\\\\\\\\\\n-        train_time = 0\\\\\\\\\\\\\\\\n-        i = 0\\\\\\\\\\\\\\\\n-        emb_array = np.zeros((nrof_examples, embedding_size))\\\\\\\\\\\\\\\\n-        loss_array = np.zeros((nrof_triplets,))\\\\\\\\\\\\\\\\n-        summary = tf.Summary()\\\\\\\\\\\\\\\\n-        step = 0\\\\\\\\\\\\\\\\n-        while i < nrof_batches:\\\\\\\\\\\\\\\\n-            start_time = time.time()\\\\\\\\\\\\\\\\n-            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\\\\\\\\\\\\\\\\n-            feed_dict = {batch_size_placeholder: batch_size, learning_rate_placeholder: lr, phase_train_placeholder: True}\\\\\\\\\\\\\\\\n-            err, _, step, emb, lab = sess.run([loss, train_op, global_step, embeddings, labels_batch], feed_dict=feed_dict)\\\\\\\\\\\\\\\\n-            emb_array[lab,:] = emb\\\\\\\\\\\\\\\\n-            loss_array[i] = err\\\\\\\\\\\\\\\\n-            duration = time.time() - start_time\\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Epoch: [%d][%d/%d]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\tTime %.3f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\tLoss %2.3f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' %\\\\\\\\\\\\\\\\n-                  (epoch, batch_number+1, args.epoch_size, duration, err))\\\\\\\\\\\\\\\\n-            batch_number += 1\\\\\\\\\\\\\\\\n-            i += 1\\\\\\\\\\\\\\\\n-            train_time += duration\\\\\\\\\\\\\\\\n-            summary.value.add(tag=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'loss\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', simple_value=err)\\\\\\\\\\\\\\\\n-            \\\\\\\\\\\\\\\\n-        # Add validation loss and accuracy to summary\\\\\\\\\\\\\\\\n-        #pylint: disable=maybe-no-member\\\\\\\\\\\\\\\\n-        summary.value.add(tag=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'time/selection\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', simple_value=selection_time)\\\\\\\\\\\\\\\\n-        summary_writer.add_summary(summary, step)\\\\\\\\\\\\\\\\n-    return step\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-def select_triplets(embeddings, nrof_images_per_class, image_paths, people_per_batch, alpha):\\\\\\\\\\\\\\\\n-    """ Select the triplets for training\\\\\\\\\\\\\\\\n-    """\\\\\\\\\\\\\\\\n-    trip_idx = 0\\\\\\\\\\\\\\\\n-    emb_start_idx = 0\\\\\\\\\\\\\\\\n-    num_trips = 0\\\\\\\\\\\\\\\\n-    triplets = []\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    # VGG Face: Choosing good triplets is crucial and should strike a balance between\\\\\\\\\\\\\\\\n-    #  selecting informative (i.e. challenging) examples and swamping training with examples that\\\\\\\\\\\\\\\\n-    #  are too hard. This is achieve by extending each pair (a, p) to a triplet (a, p, n) by sampling\\\\\\\\\\\\\\\\n-    #  the image n at random, but only between the ones that violate the triplet loss margin. The\\\\\\\\\\\\\\\\n-    #  latter is a form of hard-negative mining, but it is not as aggressive (and much cheaper) than\\\\\\\\\\\\\\\\n-    #  choosing the maximally violating example, as often done in structured output learning.\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    for i in xrange(people_per_batch):\\\\\\\\\\\\\\\\n-        nrof_images = int(nrof_images_per_class[i])\\\\\\\\\\\\\\\\n-        for j in xrange(1,nrof_images):\\\\\\\\\\\\\\\\n-            a_idx = emb_start_idx + j - 1\\\\\\\\\\\\\\\\n-            neg_dists_sqr = np.sum(np.square(embeddings[a_idx] - embeddings), 1)\\\\\\\\\\\\\\\\n-            for pair in xrange(j, nrof_images): # For every possible positive pair.\\\\\\\\\\\\\\\\n-                p_idx = emb_start_idx + pair\\\\\\\\\\\\\\\\n-                pos_dist_sqr = np.sum(np.square(embeddings[a_idx]-embeddings[p_idx]))\\\\\\\\\\\\\\\\n-                neg_dists_sqr[emb_start_idx:emb_start_idx+nrof_images] = np.NaN\\\\\\\\\\\\\\\\n-                #all_neg = np.where(np.logical_and(neg_dists_sqr-pos_dist_sqr<alpha, pos_dist_sqr<neg_dists_sqr))[0]  # FaceNet selection\\\\\\\\\\\\\\\\n-                all_neg = np.where(neg_dists_sqr-pos_dist_sqr<alpha)[0] # VGG Face selecction\\\\\\\\\\\\\\\\n-                nrof_random_negs = all_neg.shape[0]\\\\\\\\\\\\\\\\n-                if nrof_random_negs>0:\\\\\\\\\\\\\\\\n-                    rnd_idx = np.random.randint(nrof_random_negs)\\\\\\\\\\\\\\\\n-                    n_idx = all_neg[rnd_idx]\\\\\\\\\\\\\\\\n-                    triplets.append((image_paths[a_idx], image_paths[p_idx], image_paths[n_idx]))\\\\\\\\\\\\\\\\n-                    #print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Triplet %d: (%d, %d, %d), pos_dist=%2.6f, neg_dist=%2.6f (%d, %d, %d, %d, %d)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % \\\\\\\\\\\\\\\\n-                    #    (trip_idx, a_idx, p_idx, n_idx, pos_dist_sqr, neg_dists_sqr[n_idx], nrof_random_negs, rnd_idx, i, j, emb_start_idx))\\\\\\\\\\\\\\\\n-                    trip_idx += 1\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                num_trips += 1\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        emb_start_idx += nrof_images\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    np.random.shuffle(triplets)\\\\\\\\\\\\\\\\n-    return triplets, num_trips, len(triplets)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def sample_people(dataset, people_per_batch, images_per_person):\\\\\\\\\\\\\\\\n-    nrof_images = people_per_batch * images_per_person\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-    # Sample classes from the dataset\\\\\\\\\\\\\\\\n-    nrof_classes = len(dataset)\\\\\\\\\\\\\\\\n-    class_indices = np.arange(nrof_classes)\\\\\\\\\\\\\\\\n-    np.random.shuffle(class_indices)\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    i = 0\\\\\\\\\\\\\\\\n-    image_paths = []\\\\\\\\\\\\\\\\n-    num_per_class = []\\\\\\\\\\\\\\\\n-    sampled_class_indices = []\\\\\\\\\\\\\\\\n-    # Sample images from these classes until we have enough\\\\\\\\\\\\\\\\n-    while len(image_paths)<nrof_images:\\\\\\\\\\\\\\\\n-        class_index = class_indices[i]\\\\\\\\\\\\\\\\n-        nrof_images_in_class = len(dataset[class_index])\\\\\\\\\\\\\\\\n-        image_indices = np.arange(nrof_images_in_class)\\\\\\\\\\\\\\\\n-        np.random.shuffle(image_indices)\\\\\\\\\\\\\\\\n-        nrof_images_from_class = min(nrof_images_in_class, images_per_person, nrof_images-len(image_paths))\\\\\\\\\\\\\\\\n-        idx = image_indices[0:nrof_images_from_class]\\\\\\\\\\\\\\\\n-        image_paths_for_class = [dataset[class_index].image_paths[j] for j in idx]\\\\\\\\\\\\\\\\n-        sampled_class_indices += [class_index]*nrof_images_from_class\\\\\\\\\\\\\\\\n-        image_paths += image_paths_for_class\\\\\\\\\\\\\\\\n-        num_per_class.append(nrof_images_from_class)\\\\\\\\\\\\\\\\n-        i+=1\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-    return image_paths, num_per_class\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def evaluate(sess, image_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \\\\\\\\\\\\\\\\n-        batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, batch_size, \\\\\\\\\\\\\\\\n-        nrof_folds, log_dir, step, summary_writer, embedding_size):\\\\\\\\\\\\\\\\n-    start_time = time.time()\\\\\\\\\\\\\\\\n-    # Run forward pass to calculate embeddings\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Running forward pass on LFW images: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', end=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    nrof_images = len(actual_issame)*2\\\\\\\\\\\\\\\\n-    assert(len(image_paths)==nrof_images)\\\\\\\\\\\\\\\\n-    labels_array = np.reshape(np.arange(nrof_images),(-1,3))\\\\\\\\\\\\\\\\n-    image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\\\\\\\\\\\\\\\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\\\\\\\\\\\\\\\\n-    emb_array = np.zeros((nrof_images, embedding_size))\\\\\\\\\\\\\\\\n-    nrof_batches = int(np.ceil(nrof_images / batch_size))\\\\\\\\\\\\\\\\n-    label_check_array = np.zeros((nrof_images,))\\\\\\\\\\\\\\\\n-    for i in xrange(nrof_batches):\\\\\\\\\\\\\\\\n-        batch_size = min(nrof_images-i*batch_size, batch_size)\\\\\\\\\\\\\\\\n-        emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size,\\\\\\\\\\\\\\\\n-            learning_rate_placeholder: 0.0, phase_train_placeholder: False})\\\\\\\\\\\\\\\\n-        emb_array[lab,:] = emb\\\\\\\\\\\\\\\\n-        label_check_array[lab] = 1\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'%.3f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % (time.time()-start_time))\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    assert(np.all(label_check_array==1))\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    _, _, accuracy, val, val_std, far = lfw.evaluate(emb_array, actual_issame, nrof_folds=nrof_folds)\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Accuracy: %1.3f+-%1.3f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % (np.mean(accuracy), np.std(accuracy)))\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % (val, val_std, far))\\\\\\\\\\\\\\\\n-    lfw_time = time.time() - start_time\\\\\\\\\\\\\\\\n-    # Add validation loss and accuracy to summary\\\\\\\\\\\\\\\\n-    summary = tf.Summary()\\\\\\\\\\\\\\\\n-    #pylint: disable=maybe-no-member\\\\\\\\\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'lfw/accuracy\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', simple_value=np.mean(accuracy))\\\\\\\\\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'lfw/val_rate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', simple_value=val)\\\\\\\\\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'time/lfw\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', simple_value=lfw_time)\\\\\\\\\\\\\\\\n-    summary_writer.add_summary(summary, step)\\\\\\\\\\\\\\\\n-    with open(os.path.join(log_dir,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'lfw_result.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'),\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'at\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as f:\\\\\\\\\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'%d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\t%.5f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\t%.5f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % (step, np.mean(accuracy), val))\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\\\\\\\\\\\\\\\\n-    # Save the model checkpoint\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Saving variables\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    start_time = time.time()\\\\\\\\\\\\\\\\n-    checkpoint_path = os.path.join(model_dir, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'model-%s.ckpt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % model_name)\\\\\\\\\\\\\\\\n-    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\\\\\\\\\\\\\\\\n-    save_time_variables = time.time() - start_time\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Variables saved in %.2f seconds\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % save_time_variables)\\\\\\\\\\\\\\\\n-    metagraph_filename = os.path.join(model_dir, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'model-%s.meta\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % model_name)\\\\\\\\\\\\\\\\n-    save_time_metagraph = 0  \\\\\\\\\\\\\\\\n-    if not os.path.exists(metagraph_filename):\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Saving metagraph\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        start_time = time.time()\\\\\\\\\\\\\\\\n-        saver.export_meta_graph(metagraph_filename)\\\\\\\\\\\\\\\\n-        save_time_metagraph = time.time() - start_time\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Metagraph saved in %.2f seconds\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % save_time_metagraph)\\\\\\\\\\\\\\\\n-    summary = tf.Summary()\\\\\\\\\\\\\\\\n-    #pylint: disable=maybe-no-member\\\\\\\\\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'time/save_variables\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', simple_value=save_time_variables)\\\\\\\\\\\\\\\\n-    summary.value.add(tag=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'time/save_metagraph\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', simple_value=save_time_metagraph)\\\\\\\\\\\\\\\\n-    summary_writer.add_summary(summary, step)\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-def get_learning_rate_from_file(filename, epoch):\\\\\\\\\\\\\\\\n-    with open(filename, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'r\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as f:\\\\\\\\\\\\\\\\n-        for line in f.readlines():\\\\\\\\\\\\\\\\n-            line = line.split(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'#\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', 1)[0]\\\\\\\\\\\\\\\\n-            if line:\\\\\\\\\\\\\\\\n-                par = line.strip().split(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-                e = int(par[0])\\\\\\\\\\\\\\\\n-                lr = float(par[1])\\\\\\\\\\\\\\\\n-                if e <= epoch:\\\\\\\\\\\\\\\\n-                    learning_rate = lr\\\\\\\\\\\\\\\\n-                else:\\\\\\\\\\\\\\\\n-                    return learning_rate\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def parse_arguments(argv):\\\\\\\\\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--logs_base_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str, \\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Directory where to write event logs.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'~/logs/facenet\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--models_base_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Directory where to write trained models and checkpoints.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'~/models/facenet\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--gpu_memory_fraction\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Upper bound on the amount of GPU memory that will be used by the process.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=1.0)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--pretrained_model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Load a pretrained model before training starts.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--data_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Path to the data directory containing aligned face patches.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-        default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--model_def\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Model definition. Points to a module containing the definition of the inference graph.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'models.inception_resnet_v1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--max_nrof_epochs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of epochs to run.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=500)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of images to process in a batch.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=90)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--image_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Image size (height, width) in pixels.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=160)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--people_per_batch\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of people per batch.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=45)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--images_per_person\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of images per person.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=40)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--epoch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of batches per epoch.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=1000)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--alpha\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Positive to negative triplet distance margin.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0.2)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--embedding_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Dimensionality of the embedding.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=128)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--random_crop\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' +\\\\\\\\\\\\\\\\n-         \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'If the size of the images in the data directory is equal to image_size no cropping is performed\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--random_flip\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Performs random horizontal flipping of training images.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--keep_probability\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Keep probability of dropout for the fully connected layer(s).\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=1.0)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--weight_decay\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'L2 weight regularization.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0.0)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--optimizer\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str, choices=[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'ADAGRAD\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'ADADELTA\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'ADAM\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'RMSPROP\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'MOM\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'],\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'The optimization algorithm to use\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'ADAGRAD\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--learning_rate\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Initial learning rate. If set to a negative value a learning rate \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' +\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'schedule can be specified in the file "learning_rate_schedule.txt"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0.1)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--learning_rate_decay_epochs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of epochs between learning rate decay.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=100)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--learning_rate_decay_factor\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Learning rate decay factor.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=1.0)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--moving_average_decay\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=float,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Exponential decay for tracking of training parameters.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0.9999)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--seed\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Random seed.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=666)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--learning_rate_schedule_file\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data/learning_rate_schedule.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    # Parameters for validation on LFW\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'The file containing the pairs to use for validation.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data/pairs.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Path to the data directory containing aligned face patches.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of folds to use for cross validation. Mainly used for testing.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=10)\\\\\\\\\\\\\\\\n-    return parser.parse_args(argv)\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\\\\\\\\\\\\\ndiff --git a/src/validate_on_lfw.py b/src/validate_on_lfw.py\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex ac456c5..0000000\\\\\\\\\\\\\\\\n--- a/src/validate_on_lfw.py\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1,164 +0,0 @@\\\\\\\\\\\\\\\\n-"""Validate a face recognizer on the "Labeled Faces in the Wild" dataset (http://vis-www.cs.umass.edu/lfw/).\\\\\\\\\\\\\\\\n-Embeddings are calculated using the pairs from http://vis-www.cs.umass.edu/lfw/pairs.txt and the ROC curve\\\\\\\\\\\\\\\\n-is calculated and plotted. Both the model metagraph and the model parameters need to exist\\\\\\\\\\\\\\\\n-in the same directory, and the metagraph should have the extension \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.meta\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.\\\\\\\\\\\\\\\\n-"""\\\\\\\\\\\\\\\\n-# MIT License\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\\\\\\\\\n-# SOFTWARE.\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-from __future__ import absolute_import\\\\\\\\\\\\\\\\n-from __future__ import division\\\\\\\\\\\\\\\\n-from __future__ import print_function\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\n-import argparse\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\n-import lfw\\\\\\\\\\\\\\\\n-import os\\\\\\\\\\\\\\\\n-import sys\\\\\\\\\\\\\\\\n-from tensorflow.python.ops import data_flow_ops\\\\\\\\\\\\\\\\n-from sklearn import metrics\\\\\\\\\\\\\\\\n-from scipy.optimize import brentq\\\\\\\\\\\\\\\\n-from scipy import interpolate\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def main(args):\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\\\\\\\\\n-      \\\\\\\\\\\\\\\\n-        with tf.Session() as sess:\\\\\\\\\\\\\\\\n-            \\\\\\\\\\\\\\\\n-            # Read the file containing the pairs used for testing\\\\\\\\\\\\\\\\n-            pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            # Get the paths for the corresponding images\\\\\\\\\\\\\\\\n-            paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\\\\\\\\\\\\\\\\n-            \\\\\\\\\\\\\\\\n-            image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'image_paths\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-            labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'labels\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-            batch_size_placeholder = tf.placeholder(tf.int32, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-            control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'control\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-            phase_train_placeholder = tf.placeholder(tf.bool, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'phase_train\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n- \\\\\\\\\\\\\\\\n-            nrof_preprocess_threads = 4\\\\\\\\\\\\\\\\n-            image_size = (args.image_size, args.image_size)\\\\\\\\\\\\\\\\n-            eval_input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\\\\\\\\\\\\\\\\n-                                        dtypes=[tf.string, tf.int32, tf.int32],\\\\\\\\\\\\\\\\n-                                        shapes=[(1,), (1,), (1,)],\\\\\\\\\\\\\\\\n-                                        shared_name=None, name=None)\\\\\\\\\\\\\\\\n-            eval_enqueue_op = eval_input_queue.enqueue_many([image_paths_placeholder, labels_placeholder, control_placeholder], name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'eval_enqueue_op\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-            image_batch, label_batch = facenet.create_input_pipeline(eval_input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder)\\\\\\\\\\\\\\\\n-     \\\\\\\\\\\\\\\\n-            # Load the model\\\\\\\\\\\\\\\\n-            input_map = {\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'image_batch\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': image_batch, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'label_batch\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': label_batch, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'phase_train\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\': phase_train_placeholder}\\\\\\\\\\\\\\\\n-            facenet.load_model(args.model, input_map=input_map)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            # Get output tensor\\\\\\\\\\\\\\\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\n-#              \\\\\\\\\\\\\\\\n-            coord = tf.train.Coordinator()\\\\\\\\\\\\\\\\n-            tf.train.start_queue_runners(coord=coord, sess=sess)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            evaluate(sess, eval_enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\\\\\\\\\\\\\\\\n-                embeddings, label_batch, paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, args.distance_metric, args.subtract_mean,\\\\\\\\\\\\\\\\n-                args.use_flipped_images, args.use_fixed_image_standardization)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-              \\\\\\\\\\\\\\\\n-def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\\\\\\\\\\\\\\\\n-        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, distance_metric, subtract_mean, use_flipped_images, use_fixed_image_standardization):\\\\\\\\\\\\\\\\n-    # Run forward pass to calculate embeddings\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Runnning forward pass on LFW images\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    # Enqueue one epoch of image paths and labels\\\\\\\\\\\\\\\\n-    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\\\\\\\\\\\\\\\\n-    nrof_flips = 2 if use_flipped_images else 1\\\\\\\\\\\\\\\\n-    nrof_images = nrof_embeddings * nrof_flips\\\\\\\\\\\\\\\\n-    labels_array = np.expand_dims(np.arange(0,nrof_images),1)\\\\\\\\\\\\\\\\n-    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\\\\\\\\\\\\\\\\n-    control_array = np.zeros_like(labels_array, np.int32)\\\\\\\\\\\\\\\\n-    if use_fixed_image_standardization:\\\\\\\\\\\\\\\\n-        control_array += np.ones_like(labels_array)*facenet.FIXED_STANDARDIZATION\\\\\\\\\\\\\\\\n-    if use_flipped_images:\\\\\\\\\\\\\\\\n-        # Flip every second image\\\\\\\\\\\\\\\\n-        control_array += (labels_array % 2)*facenet.FLIP\\\\\\\\\\\\\\\\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    embedding_size = int(embeddings.get_shape()[1])\\\\\\\\\\\\\\\\n-    assert nrof_images % batch_size == 0, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'The number of LFW images must be an integer multiple of the LFW batch size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-    nrof_batches = nrof_images // batch_size\\\\\\\\\\\\\\\\n-    emb_array = np.zeros((nrof_images, embedding_size))\\\\\\\\\\\\\\\\n-    lab_array = np.zeros((nrof_images,))\\\\\\\\\\\\\\\\n-    for i in range(nrof_batches):\\\\\\\\\\\\\\\\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}\\\\\\\\\\\\\\\\n-        emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)\\\\\\\\\\\\\\\\n-        lab_array[lab] = lab\\\\\\\\\\\\\\\\n-        emb_array[lab, :] = emb\\\\\\\\\\\\\\\\n-        if i % 10 == 9:\\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', end=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-            sys.stdout.flush()\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    embeddings = np.zeros((nrof_embeddings, embedding_size*nrof_flips))\\\\\\\\\\\\\\\\n-    if use_flipped_images:\\\\\\\\\\\\\\\\n-        # Concatenate embeddings for flipped and non flipped version of the images\\\\\\\\\\\\\\\\n-        embeddings[:,:embedding_size] = emb_array[0::2,:]\\\\\\\\\\\\\\\\n-        embeddings[:,embedding_size:] = emb_array[1::2,:]\\\\\\\\\\\\\\\\n-    else:\\\\\\\\\\\\\\\\n-        embeddings = emb_array\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    assert np.array_equal(lab_array, np.arange(nrof_images))==True, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-    tpr, fpr, accuracy, val, val_std, far = lfw.evaluate(embeddings, actual_issame, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Accuracy: %2.5f+-%2.5f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % (np.mean(accuracy), np.std(accuracy)))\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % (val, val_std, far))\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    auc = metrics.auc(fpr, tpr)\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Area Under Curve (AUC): %1.3f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % auc)\\\\\\\\\\\\\\\\n-    eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\\\\\\\\\\\\\\\\n-    print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Equal Error Rate (EER): %1.3f\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % eer)\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-def parse_arguments(argv):\\\\\\\\\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'lfw_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Path to the data directory containing aligned LFW face patches.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of images to process in a batch in the LFW test set.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=100)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str, \\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--image_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Image size (height, width) in pixels.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=160)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=str,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'The file containing the pairs to use for validation.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data/pairs.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Number of folds to use for cross validation. Mainly used for testing.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=10)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--distance_metric\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', type=int,\\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Distance metric  0:euclidian, 1:cosine similarity.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0)\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--use_flipped_images\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Concatenates embeddings for the image and its horizontally flipped counterpart.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--subtract_mean\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Subtract feature mean before calculating distance.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--use_fixed_image_standardization\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\n-        help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Performs fixed standardization of images.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', action=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'store_true\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    return parser.parse_args(argv)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\\\\\\\\\\\\\ndiff --git a/test/a b/test/a\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex 8b13789..0000000\\\\\\\\\\\\\\\\n--- a/test/a\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1 +0,0 @@\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\ndiff --git a/test/batch_norm_test.py b/test/batch_norm_test.py\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex 48cfd55..0000000\\\\\\\\\\\\\\\\n--- a/test/batch_norm_test.py\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1,66 +0,0 @@\\\\\\\\\\\\\\\\n-# MIT License\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\\\\\\\\\n-# SOFTWARE.\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-import unittest\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\n-import models\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\n-import numpy.testing as testing\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-class BatchNormTest(unittest.TestCase):\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    @unittest.skip("Skip batch norm test case")\\\\\\\\\\\\\\\\n-    def testBatchNorm(self):\\\\\\\\\\\\\\\\n-      \\\\\\\\\\\\\\\\n-        tf.set_random_seed(123)\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-        x = tf.placeholder(tf.float32, [None, 20, 20, 10], name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'input\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        phase_train = tf.placeholder(tf.bool, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'phase_train\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        # generate random noise to pass into batch norm\\\\\\\\\\\\\\\\n-        #x_gen = tf.random_normal([50,20,20,10])\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        bn = models.network.batch_norm(x, phase_train)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        init = tf.global_variables_initializer()\\\\\\\\\\\\\\\\n-        sess = tf.Session(config=tf.ConfigProto())\\\\\\\\\\\\\\\\n-        sess.run(init)\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-        with sess.as_default():\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-            #generate a constant variable to pass into batch norm\\\\\\\\\\\\\\\\n-            y = np.random.normal(0, 1, size=(50,20,20,10))\\\\\\\\\\\\\\\\n-            \\\\\\\\\\\\\\\\n-            feed_dict = {x: y, phase_train: True}\\\\\\\\\\\\\\\\n-            sess.run(bn, feed_dict=feed_dict)\\\\\\\\\\\\\\\\n-            \\\\\\\\\\\\\\\\n-            feed_dict = {x: y, phase_train: False}\\\\\\\\\\\\\\\\n-            y1 = sess.run(bn, feed_dict=feed_dict)\\\\\\\\\\\\\\\\n-            y2 = sess.run(bn, feed_dict=feed_dict)\\\\\\\\\\\\\\\\n-            \\\\\\\\\\\\\\\\n-            testing.assert_almost_equal(y1, y2, 10, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Output from two forward passes with phase_train==false should be equal\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-if __name__ == "__main__":\\\\\\\\\\\\\\\\n-    unittest.main()\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\\\\\\\\\ndiff --git a/test/center_loss_test.py b/test/center_loss_test.py\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex 196cd11..0000000\\\\\\\\\\\\\\\\n--- a/test/center_loss_test.py\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1,87 +0,0 @@\\\\\\\\\\\\\\\\n-# MIT License\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\\\\\\\\\n-# SOFTWARE.\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-import unittest\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-class CenterLossTest(unittest.TestCase):\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    def testCenterLoss(self):\\\\\\\\\\\\\\\\n-        batch_size = 16\\\\\\\\\\\\\\\\n-        nrof_features = 2\\\\\\\\\\\\\\\\n-        nrof_classes = 16\\\\\\\\\\\\\\\\n-        alfa = 0.5\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        with tf.Graph().as_default():\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-            features = tf.placeholder(tf.float32, shape=(batch_size, nrof_features), name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'features\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-            labels = tf.placeholder(tf.int32, shape=(batch_size,), name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'labels\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            # Define center loss\\\\\\\\\\\\\\\\n-            center_loss, centers = facenet.center_loss(features, labels, alfa, nrof_classes)\\\\\\\\\\\\\\\\n-            \\\\\\\\\\\\\\\\n-            label_to_center = np.array( [ \\\\\\\\\\\\\\\\n-                 [-3,-3],  [-3,-1],  [-3,1],  [-3,3],\\\\\\\\\\\\\\\\n-                 [-1,-3],  [-1,-1],  [-1,1],  [-1,3],\\\\\\\\\\\\\\\\n-                 [ 1,-3],  [ 1,-1],  [ 1,1],  [ 1,3],\\\\\\\\\\\\\\\\n-                 [ 3,-3],  [ 3,-1],  [ 3,1],  [ 3,3] \\\\\\\\\\\\\\\\n-                 ])\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\n-            sess = tf.Session()\\\\\\\\\\\\\\\\n-            with sess.as_default():\\\\\\\\\\\\\\\\n-                sess.run(tf.global_variables_initializer())\\\\\\\\\\\\\\\\n-                np.random.seed(seed=666)\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\n-                for _ in range(0,100):\\\\\\\\\\\\\\\\n-                    # Create array of random labels\\\\\\\\\\\\\\\\n-                    lbls = np.random.randint(low=0, high=nrof_classes, size=(batch_size,))\\\\\\\\\\\\\\\\n-                    feats = create_features(label_to_center, batch_size, nrof_features, lbls)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                    center_loss_, centers_ = sess.run([center_loss, centers], feed_dict={features:feats, labels:lbls})\\\\\\\\\\\\\\\\n-                    \\\\\\\\\\\\\\\\n-                # After a large number of updates the estimated centers should be close to the true ones\\\\\\\\\\\\\\\\n-                np.testing.assert_almost_equal(centers_, label_to_center, decimal=5, err_msg=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Incorrect estimated centers\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-                np.testing.assert_almost_equal(center_loss_, 0.0, decimal=5, err_msg=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Incorrect center loss\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def create_features(label_to_center, batch_size, nrof_features, labels):\\\\\\\\\\\\\\\\n-    # Map label to center\\\\\\\\\\\\\\\\n-#     label_to_center_dict = { \\\\\\\\\\\\\\\\n-#          0:(-3,-3),  1:(-3,-1),  2:(-3,1),  3:(-3,3),\\\\\\\\\\\\\\\\n-#          4:(-1,-3),  5:(-1,-1),  6:(-1,1),  7:(-1,3),\\\\\\\\\\\\\\\\n-#          8:( 1,-3),  9:( 1,-1), 10:( 1,1), 11:( 1,3),\\\\\\\\\\\\\\\\n-#         12:( 3,-3), 13:( 3,-1), 14:( 3,1), 15:( 3,3),\\\\\\\\\\\\\\\\n-#         }\\\\\\\\\\\\\\\\n-    # Create array of features corresponding to the labels\\\\\\\\\\\\\\\\n-    feats = np.zeros((batch_size, nrof_features))\\\\\\\\\\\\\\\\n-    for i in range(batch_size):\\\\\\\\\\\\\\\\n-        cntr =  label_to_center[labels[i]]\\\\\\\\\\\\\\\\n-        for j in range(nrof_features):\\\\\\\\\\\\\\\\n-            feats[i,j] = cntr[j]\\\\\\\\\\\\\\\\n-    return feats\\\\\\\\\\\\\\\\n-                      \\\\\\\\\\\\\\\\n-if __name__ == "__main__":\\\\\\\\\\\\\\\\n-    unittest.main()\\\\\\\\\\\\\\\\ndiff --git a/test/restore_test.py b/test/restore_test.py\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex befb04d..0000000\\\\\\\\\\\\\\\\n--- a/test/restore_test.py\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1,181 +0,0 @@\\\\\\\\\\\\\\\\n-# MIT License\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\\\\\\\\\n-# SOFTWARE.\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-import unittest\\\\\\\\\\\\\\\\n-import tempfile\\\\\\\\\\\\\\\\n-import os\\\\\\\\\\\\\\\\n-import shutil\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-class TrainTest(unittest.TestCase):\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-    @classmethod\\\\\\\\\\\\\\\\n-    def setUpClass(self):\\\\\\\\\\\\\\\\n-        self.tmp_dir = tempfile.mkdtemp()\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-    @classmethod\\\\\\\\\\\\\\\\n-    def tearDownClass(self):\\\\\\\\\\\\\\\\n-        # Recursively remove the temporary directory\\\\\\\\\\\\\\\\n-        shutil.rmtree(self.tmp_dir)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    def test_restore_noema(self):\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        # Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\\\\\\\\\\\\\\\\n-        x_data = np.random.rand(100).astype(np.float32)\\\\\\\\\\\\\\\\n-        y_data = x_data * 0.1 + 0.3\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        # Try to find values for W and b that compute y_data = W * x_data + b\\\\\\\\\\\\\\\\n-        # (We know that W should be 0.1 and b 0.3, but TensorFlow will\\\\\\\\\\\\\\\\n-        # figure that out for us.)\\\\\\\\\\\\\\\\n-        W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'W\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        b = tf.Variable(tf.zeros([1]), name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'b\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        y = W * x_data + b\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        # Minimize the mean squared errors.\\\\\\\\\\\\\\\\n-        loss = tf.reduce_mean(tf.square(y - y_data))\\\\\\\\\\\\\\\\n-        optimizer = tf.train.GradientDescentOptimizer(0.5)\\\\\\\\\\\\\\\\n-        train = optimizer.minimize(loss)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        # Before starting, initialize the variables.  We will \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'run\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' this first.\\\\\\\\\\\\\\\\n-        init = tf.global_variables_initializer()\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        saver = tf.train.Saver(tf.trainable_variables())\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        # Launch the graph.\\\\\\\\\\\\\\\\n-        sess = tf.Session()\\\\\\\\\\\\\\\\n-        sess.run(init)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        # Fit the line.\\\\\\\\\\\\\\\\n-        for _ in range(201):\\\\\\\\\\\\\\\\n-            sess.run(train)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        w_reference = sess.run(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'W:0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        b_reference = sess.run(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'b:0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        saver.save(sess, os.path.join(self.tmp_dir, "model_ex1"))\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        tf.reset_default_graph()\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        saver = tf.train.import_meta_graph(os.path.join(self.tmp_dir, "model_ex1.meta"))\\\\\\\\\\\\\\\\n-        sess = tf.Session()\\\\\\\\\\\\\\\\n-        saver.restore(sess, os.path.join(self.tmp_dir, "model_ex1"))\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        w_restored = sess.run(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'W:0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        b_restored = sess.run(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'b:0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        self.assertAlmostEqual(w_reference, w_restored, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Restored model use different weight than the original model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        self.assertAlmostEqual(b_reference, b_restored, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Restored model use different weight than the original model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    @unittest.skip("Skip restore EMA test case for now")\\\\\\\\\\\\\\\\n-    def test_restore_ema(self):\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        # Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\\\\\\\\\\\\\\\\n-        x_data = np.random.rand(100).astype(np.float32)\\\\\\\\\\\\\\\\n-        y_data = x_data * 0.1 + 0.3\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        # Try to find values for W and b that compute y_data = W * x_data + b\\\\\\\\\\\\\\\\n-        # (We know that W should be 0.1 and b 0.3, but TensorFlow will\\\\\\\\\\\\\\\\n-        # figure that out for us.)\\\\\\\\\\\\\\\\n-        W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'W\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        b = tf.Variable(tf.zeros([1]), name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'b\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        y = W * x_data + b\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        # Minimize the mean squared errors.\\\\\\\\\\\\\\\\n-        loss = tf.reduce_mean(tf.square(y - y_data))\\\\\\\\\\\\\\\\n-        optimizer = tf.train.GradientDescentOptimizer(0.5)\\\\\\\\\\\\\\\\n-        opt_op = optimizer.minimize(loss)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Track the moving averages of all trainable variables.\\\\\\\\\\\\\\\\n-        ema = tf.train.ExponentialMovingAverage(decay=0.9999)\\\\\\\\\\\\\\\\n-        averages_op = ema.apply(tf.trainable_variables())\\\\\\\\\\\\\\\\n-        with tf.control_dependencies([opt_op]):\\\\\\\\\\\\\\\\n-            train_op = tf.group(averages_op)\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-        # Before starting, initialize the variables.  We will \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'run\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' this first.\\\\\\\\\\\\\\\\n-        init = tf.global_variables_initializer()\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        saver = tf.train.Saver(tf.trainable_variables())\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        # Launch the graph.\\\\\\\\\\\\\\\\n-        sess = tf.Session()\\\\\\\\\\\\\\\\n-        sess.run(init)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        # Fit the line.\\\\\\\\\\\\\\\\n-        for _ in range(201):\\\\\\\\\\\\\\\\n-            sess.run(train_op)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        w_reference = sess.run(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'W/ExponentialMovingAverage:0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        b_reference = sess.run(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'b/ExponentialMovingAverage:0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        saver.save(sess, os.path.join(self.tmp_dir, "model_ex1"))\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\n-        tf.reset_default_graph()\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        tf.train.import_meta_graph(os.path.join(self.tmp_dir, "model_ex1.meta"))\\\\\\\\\\\\\\\\n-        sess = tf.Session()\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'------------------------------------------------------\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        for var in tf.global_variables():\\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'all variables: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' + var.op.name)\\\\\\\\\\\\\\\\n-        for var in tf.trainable_variables():\\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'normal variable: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' + var.op.name)\\\\\\\\\\\\\\\\n-        for var in tf.moving_average_variables():\\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'ema variable: \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' + var.op.name)\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'------------------------------------------------------\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        mode = 1\\\\\\\\\\\\\\\\n-        restore_vars = {}\\\\\\\\\\\\\\\\n-        if mode == 0:\\\\\\\\\\\\\\\\n-            ema = tf.train.ExponentialMovingAverage(1.0)\\\\\\\\\\\\\\\\n-            for var in tf.trainable_variables():\\\\\\\\\\\\\\\\n-                print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'%s: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % (ema.average_name(var), var.op.name))\\\\\\\\\\\\\\\\n-                restore_vars[ema.average_name(var)] = var\\\\\\\\\\\\\\\\n-        elif mode == 1:\\\\\\\\\\\\\\\\n-            for var in tf.trainable_variables():\\\\\\\\\\\\\\\\n-                ema_name = var.op.name + \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'/ExponentialMovingAverage\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-                print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'%s: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % (ema_name, var.op.name))\\\\\\\\\\\\\\\\n-                restore_vars[ema_name] = var\\\\\\\\\\\\\\\\n-            \\\\\\\\\\\\\\\\n-        saver = tf.train.Saver(restore_vars, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'ema_restore\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        saver.restore(sess, os.path.join(self.tmp_dir, "model_ex1"))\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        w_restored = sess.run(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'W:0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        b_restored = sess.run(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'b:0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        self.assertAlmostEqual(w_reference, w_restored, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Restored model modes not use the EMA filtered weight\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        self.assertAlmostEqual(b_reference, b_restored, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Restored model modes not use the EMA filtered bias\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-# Create a checkpoint file pointing to the model\\\\\\\\\\\\\\\\n-def create_checkpoint_file(model_dir, model_file):\\\\\\\\\\\\\\\\n-    checkpoint_filename = os.path.join(model_dir, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'checkpoint\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    full_model_filename = os.path.join(model_dir, model_file)\\\\\\\\\\\\\\\\n-    with open(checkpoint_filename, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'w\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as f:\\\\\\\\\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'model_checkpoint_path: "%s"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % full_model_filename)\\\\\\\\\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'all_model_checkpoint_paths: "%s"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % full_model_filename)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-if __name__ == "__main__":\\\\\\\\\\\\\\\\n-    unittest.main()\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\\\\\\\\\ndiff --git a/test/train_test.py b/test/train_test.py\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex 12cd663..0000000\\\\\\\\\\\\\\\\n--- a/test/train_test.py\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1,246 +0,0 @@\\\\\\\\\\\\\\\\n-# MIT License\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\\\\\\\\\n-# SOFTWARE.\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-import unittest\\\\\\\\\\\\\\\\n-import tempfile\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\n-import cv2\\\\\\\\\\\\\\\\n-import os\\\\\\\\\\\\\\\\n-import shutil\\\\\\\\\\\\\\\\n-import download_and_extract  # @UnresolvedImport\\\\\\\\\\\\\\\\n-import subprocess\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def memory_usage_psutil():\\\\\\\\\\\\\\\\n-    # return the memory usage in MB\\\\\\\\\\\\\\\\n-    import psutil\\\\\\\\\\\\\\\\n-    process = psutil.Process(os.getpid())\\\\\\\\\\\\\\\\n-    mem = process.memory_info()[0] / float(2 ** 20)\\\\\\\\\\\\\\\\n-    return mem\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def align_dataset_if_needed(self):\\\\\\\\\\\\\\\\n-    if not os.path.exists(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data/lfw_aligned\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'):\\\\\\\\\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'src/align/align_dataset_mtcnn.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data/lfw\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data/lfw_aligned\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--image_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'160\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--margin\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'32\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' ]\\\\\\\\\\\\\\\\n-        subprocess.call(argv)\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-class TrainTest(unittest.TestCase):\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-    @classmethod\\\\\\\\\\\\\\\\n-    def setUpClass(self):\\\\\\\\\\\\\\\\n-        self.tmp_dir = tempfile.mkdtemp()\\\\\\\\\\\\\\\\n-        self.dataset_dir = os.path.join(self.tmp_dir, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'dataset\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        create_mock_dataset(self.dataset_dir, 160)\\\\\\\\\\\\\\\\n-        self.lfw_pairs_file = create_mock_lfw_pairs(self.tmp_dir)\\\\\\\\\\\\\\\\n-        print(self.lfw_pairs_file)\\\\\\\\\\\\\\\\n-        self.pretrained_model_name = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'20180402-114759\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-        download_and_extract.download_and_extract_file(self.pretrained_model_name, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data/\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        download_and_extract.download_and_extract_file(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'lfw-subset\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data/\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        self.model_file = os.path.join(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.pretrained_model_name, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'model-%s.ckpt-275\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % self.pretrained_model_name)\\\\\\\\\\\\\\\\n-        self.pretrained_model = os.path.join(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.pretrained_model_name)\\\\\\\\\\\\\\\\n-        self.frozen_graph_filename = os.path.join(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.pretrained_model_name+\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.pb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Memory utilization (SetUpClass): %.3f MB\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % memory_usage_psutil())\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    @classmethod\\\\\\\\\\\\\\\\n-    def tearDownClass(self):\\\\\\\\\\\\\\\\n-        # Recursively remove the temporary directory\\\\\\\\\\\\\\\\n-        shutil.rmtree(self.tmp_dir)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    def tearDown(self):\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Memory utilization (TearDown): %.3f MB\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % memory_usage_psutil())\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    def test_training_classifier_inception_resnet_v1(self):\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'test_training_classifier_inception_resnet_v1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'src/train_softmax.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--logs_base_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--models_base_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--data_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--model_def\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'models.inception_resnet_v1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--epoch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--max_nrof_epochs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.lfw_pairs_file,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--nrof_preprocess_threads\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' ]\\\\\\\\\\\\\\\\n-        subprocess.call(argv)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    def test_training_classifier_inception_resnet_v2(self):\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'test_training_classifier_inception_resnet_v2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'src/train_softmax.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--logs_base_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--models_base_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--data_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--model_def\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'models.inception_resnet_v2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--epoch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--max_nrof_epochs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.lfw_pairs_file,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' ]\\\\\\\\\\\\\\\\n-        subprocess.call(argv)\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-    def test_training_classifier_squeezenet(self):\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'test_training_classifier_squeezenet\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'src/train_softmax.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--logs_base_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--models_base_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--data_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--model_def\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'models.squeezenet\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--epoch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--max_nrof_epochs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.lfw_pairs_file,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--nrof_preprocess_threads\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' ]\\\\\\\\\\\\\\\\n-        subprocess.call(argv)\\\\\\\\\\\\\\\\n- \\\\\\\\\\\\\\\\n-    def test_train_tripletloss_inception_resnet_v1(self):\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'test_train_tripletloss_inception_resnet_v1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'src/train_tripletloss.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--logs_base_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--models_base_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--data_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--model_def\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'models.inception_resnet_v1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--epoch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--max_nrof_epochs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'6\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--people_per_batch\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--images_per_person\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'3\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.lfw_pairs_file,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' ]\\\\\\\\\\\\\\\\n-        subprocess.call(argv)\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-    def test_finetune_tripletloss_inception_resnet_v1(self):\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'test_finetune_tripletloss_inception_resnet_v1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'src/train_tripletloss.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--logs_base_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--models_base_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.tmp_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--data_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--model_def\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'models.inception_resnet_v1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--pretrained_model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.model_file,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--embedding_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'512\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--epoch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--max_nrof_epochs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'6\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--people_per_batch\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--images_per_person\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'3\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.lfw_pairs_file,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_dir\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.dataset_dir,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' ]\\\\\\\\\\\\\\\\n-        subprocess.call(argv)\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-    def test_compare(self):\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'test_compare\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'src/compare.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                os.path.join(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data/\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.pretrained_model_name),\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data/images/Anthony_Hopkins_0001.jpg\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data/images/Anthony_Hopkins_0002.jpg\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' ]\\\\\\\\\\\\\\\\n-        subprocess.call(argv)\\\\\\\\\\\\\\\\n-         \\\\\\\\\\\\\\\\n-    def test_validate_on_lfw(self):\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'test_validate_on_lfw\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        align_dataset_if_needed(self)\\\\\\\\\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'src/validate_on_lfw.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data/lfw_aligned\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                self.pretrained_model,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data/lfw/pairs_small.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'6\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\']\\\\\\\\\\\\\\\\n-        subprocess.call(argv)\\\\\\\\\\\\\\\\n- \\\\\\\\\\\\\\\\n-    def test_validate_on_lfw_frozen_graph(self):\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'test_validate_on_lfw_frozen_graph\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        self.pretrained_model = os.path.join(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'data\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.pretrained_model_name)\\\\\\\\\\\\\\\\n-        frozen_model = os.path.join(self.pretrained_model, self.pretrained_model_name+\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.pb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'src/validate_on_lfw.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                self.dataset_dir,\\\\\\\\\\\\\\\\n-                frozen_model,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_pairs\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', self.lfw_pairs_file,\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_nrof_folds\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--lfw_batch_size\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'6\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\']\\\\\\\\\\\\\\\\n-        subprocess.call(argv)\\\\\\\\\\\\\\\\n- \\\\\\\\\\\\\\\\n-    def test_freeze_graph(self):\\\\\\\\\\\\\\\\n-        print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'test_freeze_graph\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        argv = [\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'python\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'src/freeze_graph.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',\\\\\\\\\\\\\\\\n-                self.pretrained_model,\\\\\\\\\\\\\\\\n-                self.frozen_graph_filename ]\\\\\\\\\\\\\\\\n-        subprocess.call(argv)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-# Create a mock dataset with random pixel images\\\\\\\\\\\\\\\\n-def create_mock_dataset(dataset_dir, image_size):\\\\\\\\\\\\\\\\n-   \\\\\\\\\\\\\\\\n-    nrof_persons = 3\\\\\\\\\\\\\\\\n-    nrof_images_per_person = 2\\\\\\\\\\\\\\\\n-    np.random.seed(seed=666)\\\\\\\\\\\\\\\\n-    os.mkdir(dataset_dir)\\\\\\\\\\\\\\\\n-    for i in range(nrof_persons):\\\\\\\\\\\\\\\\n-        class_name = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'%04d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % (i+1)\\\\\\\\\\\\\\\\n-        class_dir = os.path.join(dataset_dir, class_name)\\\\\\\\\\\\\\\\n-        os.mkdir(class_dir)\\\\\\\\\\\\\\\\n-        for j in range(nrof_images_per_person):\\\\\\\\\\\\\\\\n-            img_name = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'%04d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % (j+1)\\\\\\\\\\\\\\\\n-            img_path = os.path.join(class_dir, class_name+\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'_\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'+img_name + \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.png\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-            img = np.random.uniform(low=0.0, high=255.0, size=(image_size,image_size,3))\\\\\\\\\\\\\\\\n-            cv2.imwrite(img_path, img) #@UndefinedVariable\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-# Create a mock LFW pairs file\\\\\\\\\\\\\\\\n-def create_mock_lfw_pairs(tmp_dir):\\\\\\\\\\\\\\\\n-    pairs_filename = os.path.join(tmp_dir, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'pairs_mock.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    with open(pairs_filename, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'w\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as f:\\\\\\\\\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'10 300\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0001 1 2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0001 1 0002 1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0002 1 0003 1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0001 1 0003 1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0002 1 2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0001 2 0002 2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0002 2 0003 2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0001 2 0003 2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0003 1 2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0001 1 0002 2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0002 1 0003 2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        f.write(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0001 1 0003 2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    return pairs_filename\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-if __name__ == "__main__":\\\\\\\\\\\\\\\\n-    unittest.main()\\\\\\\\\\\\\\\\n-    \\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\\\\\\\\\ndiff --git a/test/triplet_loss_test.py b/test/triplet_loss_test.py\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex 2648b30..0000000\\\\\\\\\\\\\\\\n--- a/test/triplet_loss_test.py\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1,54 +0,0 @@\\\\\\\\\\\\\\\\n-# MIT License\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\\\\\\\\\n-# \\\\\\\\\\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\\\\\\\\\n-# SOFTWARE.\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-import unittest\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-class DemuxEmbeddingsTest(unittest.TestCase):\\\\\\\\\\\\\\\\n-  \\\\\\\\\\\\\\\\n-    def testDemuxEmbeddings(self):\\\\\\\\\\\\\\\\n-        batch_size = 3*12\\\\\\\\\\\\\\\\n-        embedding_size = 16\\\\\\\\\\\\\\\\n-        alpha = 0.2\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-        with tf.Graph().as_default():\\\\\\\\\\\\\\\\n-        \\\\\\\\\\\\\\\\n-            embeddings = tf.placeholder(tf.float64, shape=(batch_size, embedding_size), name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'embeddings\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-            anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,embedding_size]), 3, 1)\\\\\\\\\\\\\\\\n-            triplet_loss = facenet.triplet_loss(anchor, positive, negative, alpha)\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\n-            sess = tf.Session()\\\\\\\\\\\\\\\\n-            with sess.as_default():\\\\\\\\\\\\\\\\n-                np.random.seed(seed=666)\\\\\\\\\\\\\\\\n-                emb = np.random.uniform(size=(batch_size, embedding_size))\\\\\\\\\\\\\\\\n-                tf_triplet_loss = sess.run(triplet_loss, feed_dict={embeddings:emb})\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                pos_dist_sqr = np.sum(np.square(emb[0::3,:]-emb[1::3,:]),1)\\\\\\\\\\\\\\\\n-                neg_dist_sqr = np.sum(np.square(emb[0::3,:]-emb[2::3,:]),1)\\\\\\\\\\\\\\\\n-                np_triplet_loss = np.mean(np.maximum(0.0, pos_dist_sqr - neg_dist_sqr + alpha))\\\\\\\\\\\\\\\\n-                \\\\\\\\\\\\\\\\n-                np.testing.assert_almost_equal(tf_triplet_loss, np_triplet_loss, decimal=5, err_msg=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Triplet loss is incorrect\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-                      \\\\\\\\\\\\\\\\n-if __name__ == "__main__":\\\\\\\\\\\\\\\\n-    unittest.main()\\\\\\\\\\\\\\\\ndiff --git a/video/a b/video/a\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex 8b13789..0000000\\\\\\\\\\\\\\\\n--- a/video/a\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1 +0,0 @@\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\ndiff --git a/video/camtest.mp4 b/video/camtest.mp4\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex a503c89..0000000\\\\\\\\\\\\\\\\nBinary files a/video/camtest.mp4 and /dev/null differ\\\\\\\\\\\\\\\'\\\\\\\\n\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_54_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_54_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 132948b..0000000\\\\\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_54_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_55_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_55_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex d589b1a..0000000\\\\\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_55_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro (2).jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex d47e20d..0000000\\\\\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex fbe6de7..0000000\\\\\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro (2).jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 3e5846d..0000000\\\\\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex b23bcfa..0000000\\\\\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro (2).jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex bb832e9..0000000\\\\\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex a0b4312..0000000\\\\\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_59_Pro (2).jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_59_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 928f751..0000000\\\\\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_59_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/raw/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg b/Dataset/FaceData/raw/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 6537490..0000000\\\\\\\\nBinary files a/Dataset/FaceData/raw/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/raw/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg b/Dataset/FaceData/raw/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 8f6b0f9..0000000\\\\\\\\nBinary files a/Dataset/FaceData/raw/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/raw/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg b/Dataset/FaceData/raw/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex f74031f..0000000\\\\\\\\nBinary files a/Dataset/FaceData/raw/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.jpeg b/Dataset/FaceData/test/data/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.jpeg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex b5ebe28..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.jpeg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.jpeg b/Dataset/FaceData/test/data/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.jpeg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex af9bfea..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.jpeg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.jpeg b/Dataset/FaceData/test/data/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.jpeg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 28c29dc..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.jpeg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.jpeg b/Dataset/FaceData/test/data/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.jpeg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 979d22b..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.jpeg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.jpeg b/Dataset/FaceData/test/data/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.jpeg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 95a477c..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.jpeg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.jpeg b/Dataset/FaceData/test/data/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.jpeg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 208ec5c..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.jpeg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/ChuQuangHuy/IMG_0349.HEIC b/Dataset/FaceData/test/data/ChuQuangHuy/IMG_0349.HEIC\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 345acbc..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/ChuQuangHuy/IMG_0349.HEIC and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.jpg b/Dataset/FaceData/test/data/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex b038c0c..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.jpg b/Dataset/FaceData/test/data/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex e3de1ee..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro (3).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 967a588..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro (3).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex c4e9ec9..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex ab17f1f..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex e9e3371..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex dcd5b17..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (3).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex bdf69fe..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (3).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex e5e0d42..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex cf7525e..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (3).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex f22e4b7..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (3).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 56ac445..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex a72c6b8..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex e4e969a..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_36_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_36_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 94d80b4..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_36_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex ef3d4f9..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex d7333e8..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex f8bec2a..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (3).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 42fdad1..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (3).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 9851605..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 63fd876..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (3).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 90c377d..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (3).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex f669175..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 5ba014d..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (3).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 17c4b93..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (3).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 7c91590..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 16d41be..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 4271bdd..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 0646f0b..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (3).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex eeb4397..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (3).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex dba3891..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex b2c8e83..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (3).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex a6e8ebf..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (3).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex d163084..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 38971bf..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 5d9bd5c..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 9903ab8..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (3).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 5f8c977..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (3).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 6db75f8..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 2daaf72..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex e84c735..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_22_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_22_Pro.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 7888261..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_22_Pro.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_0327.JPG b/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_0327.JPG\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 9f03d5d..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_0327.JPG and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092236.jpg b/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092236.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex e587b19..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092236.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092237.jpg b/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092237.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex cf24b36..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092237.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092241.jpg b/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092241.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex a505a8a..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092241.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/NgocKem/IMG_1742972407559.jpg b/Dataset/FaceData/test/data/NgocKem/IMG_1742972407559.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 0483ff8..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/NgocKem/IMG_1742972407559.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/NgocKem/IMG_1742972428266.jpg b/Dataset/FaceData/test/data/NgocKem/IMG_1742972428266.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 96d77fc..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/NgocKem/IMG_1742972428266.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/NgocKem/IMG_1742972431298.jpg b/Dataset/FaceData/test/data/NgocKem/IMG_1742972431298.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex bd4c93c..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/NgocKem/IMG_1742972431298.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.jpeg b/Dataset/FaceData/test/data/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.jpeg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 6aba153..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.jpeg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/Phuong/Messenger_creation_7343AEA7-71DD-40DB-B8C3-9D6F0CED60F9.jpeg b/Dataset/FaceData/test/data/Phuong/Messenger_creation_7343AEA7-71DD-40DB-B8C3-9D6F0CED60F9.jpeg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex d41ca36..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/Phuong/Messenger_creation_7343AEA7-71DD-40DB-B8C3-9D6F0CED60F9.jpeg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.jpeg b/Dataset/FaceData/test/data/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.jpeg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 161aa4b..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.jpeg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173023.jpg b/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173023.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 4982c4f..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173023.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173025.jpg b/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173025.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 85c8a30..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173025.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173026.jpg b/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173026.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex a18bfd6..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173026.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173028.jpg b/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173028.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 5b6d155..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173028.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.jpg b/Dataset/FaceData/test/data/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex c688251..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/TranDangHieu/download (1).jpg b/Dataset/FaceData/test/data/TranDangHieu/download (1).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex e0c8116..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/TranDangHieu/download (1).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/TranDangHieu/download (2).jpg b/Dataset/FaceData/test/data/TranDangHieu/download (2).jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex ef5f43d..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/TranDangHieu/download (2).jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/TranDangHieu/download.jpg b/Dataset/FaceData/test/data/TranDangHieu/download.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 3d6e0fb..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/TranDangHieu/download.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134208.jpg b/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134208.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 104e453..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134208.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134210.jpg b/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134210.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex c3ee227..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134210.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134211.jpg b/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134211.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 77cc3f0..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134211.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png b/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 68c44d6..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png b/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex f2dc8a1..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png b/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 9b27a7d..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135142.jpg b/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135142.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 5b6bf55..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135142.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135145.jpg b/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135145.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 46005cc..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135145.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135147.jpg b/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135147.jpg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 0c45781..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135147.jpg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg b/Dataset/FaceData/test/data/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 6537490..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg b/Dataset/FaceData/test/data/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 8f6b0f9..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg b/Dataset/FaceData/test/data/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex f74031f..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.jpeg b/Dataset/FaceData/test/data/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.jpeg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 7b62bc3..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.jpeg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.jpeg b/Dataset/FaceData/test/data/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.jpeg\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 1be1f19..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.jpeg and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.png b/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 87978ce..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.png b/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 5269259..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.png b/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex cd8c24c..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.png b/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 0650f16..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.png b/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 714bc33..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.png b/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 9b3ac40..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/485290655_1069958524968534_6128511440722701069_n.png b/Dataset/FaceData/test/data/txl/BuiCongSon/485290655_1069958524968534_6128511440722701069_n.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex cd86dc1..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/485290655_1069958524968534_6128511440722701069_n.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/485424995_1386922852752466_129752981138586263_n.png b/Dataset/FaceData/test/data/txl/BuiCongSon/485424995_1386922852752466_129752981138586263_n.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 90013f6..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/485424995_1386922852752466_129752981138586263_n.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/485464001_1219012669643907_2610181536763391909_n.png b/Dataset/FaceData/test/data/txl/BuiCongSon/485464001_1219012669643907_2610181536763391909_n.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 31c9d0c..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/485464001_1219012669643907_2610181536763391909_n.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/485512577_2113779945729669_160018376255303225_n.png b/Dataset/FaceData/test/data/txl/BuiCongSon/485512577_2113779945729669_160018376255303225_n.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex efbe736..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/485512577_2113779945729669_160018376255303225_n.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/z6377532752047_4b7b3c7d460239f92fa3cec037738eca.png b/Dataset/FaceData/test/data/txl/BuiCongSon/z6377532752047_4b7b3c7d460239f92fa3cec037738eca.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 09d25b3..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/z6377532752047_4b7b3c7d460239f92fa3cec037738eca.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151626.png b/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151626.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 66a927d..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151626.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151720.png b/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151720.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 5554b2f..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151720.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151731.png b/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151731.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex b92b066..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151731.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151738.png b/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151738.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 852ab42..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151738.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.png b/Dataset/FaceData/test/data/txl/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex f6ad820..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.png b/Dataset/FaceData/test/data/txl/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 8021733..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_1742972296094.png b/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_1742972296094.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex e7677d6..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_1742972296094.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_20250326_134739.png b/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_20250326_134739.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex d8efdad..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_20250326_134739.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_0327.png b/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_0327.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 1037e8e..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_0327.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092236.png b/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092236.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 9be2f92..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092236.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092237.png b/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092237.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 17b5d34..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092237.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092241.png b/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092241.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 97fcfdb..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092241.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972407559.png b/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972407559.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 66c7877..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972407559.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972428266.png b/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972428266.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 47db00e..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972428266.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972431298.png b/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972431298.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 1bcc4bf..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972431298.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.png b/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 6d2cfbd..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.png b/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex d1d90a5..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Thao/851D4C65-07BE-4220-8488-84C40C1A79D6.png b/Dataset/FaceData/test/data/txl/Thao/851D4C65-07BE-4220-8488-84C40C1A79D6.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex d2170a7..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/Thao/851D4C65-07BE-4220-8488-84C40C1A79D6.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Thao/98E5A551-3DC2-4A85-9497-AFA4A845325A.png b/Dataset/FaceData/test/data/txl/Thao/98E5A551-3DC2-4A85-9497-AFA4A845325A.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 055d27a..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/Thao/98E5A551-3DC2-4A85-9497-AFA4A845325A.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173023.png b/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173023.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex f95fdd9..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173023.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173025.png b/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173025.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex f2c1813..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173025.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173026.png b/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173026.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 2541428..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173026.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173028.png b/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173028.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 1eae995..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173028.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.png b/Dataset/FaceData/test/data/txl/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 0678197..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TranDangHieu/download (1).png b/Dataset/FaceData/test/data/txl/TranDangHieu/download (1).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex d7a2377..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/TranDangHieu/download (1).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TranDangHieu/download (2).png b/Dataset/FaceData/test/data/txl/TranDangHieu/download (2).png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex da1b935..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/TranDangHieu/download (2).png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TranDangHieu/download.png b/Dataset/FaceData/test/data/txl/TranDangHieu/download.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 51eee7f..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/TranDangHieu/download.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134208.png b/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134208.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex a3ffc1f..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134208.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134210.png b/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134210.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 49694a3..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134210.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134211.png b/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134211.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 7af9479..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134211.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG1.png b/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG1.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 4e78809..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG1.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG2.png b/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG2.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 21b37f7..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG2.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png b/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 971746e..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png b/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 275fcb7..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png b/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 29af910..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135142.png b/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135142.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 0149809..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135142.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135145.png b/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135145.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 2a6a9b4..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135145.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135147.png b/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135147.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 83a6450..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135147.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png b/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 8257ab5..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png b/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 45870ff..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png b/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex f4593ba..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.png b/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 942d65d..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.png b/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.png\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 6872f3e..0000000\\\\\\\\nBinary files a/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.png and /dev/null differ\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/bounding_boxes_47191.txt b/Dataset/FaceData/test/data/txl/bounding_boxes_47191.txt\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex d314c07..0000000\\\\\\\\n--- a/Dataset/FaceData/test/data/txl/bounding_boxes_47191.txt\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,55 +0,0 @@\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\TrangNhung\\\\\\\\\\\\\\\\IMG_20250326_134210.png 895 349 1544 1272\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\TrangNhung\\\\\\\\\\\\\\\\IMG_20250326_134208.png 535 336 1175 1157\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\TrangNhung\\\\\\\\\\\\\\\\IMG_20250326_134211.png 586 360 1301 1293\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Xemesis\\\\\\\\\\\\\\\\Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.png 348 648 886 1345\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Xemesis\\\\\\\\\\\\\\\\Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.png 0 293 772 1203\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Emma\\\\\\\\\\\\\\\\IMG_20250326_151626.png 967 170 1656 1050\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Emma\\\\\\\\\\\\\\\\IMG_20250326_151731.png 362 264 1458 1704\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Emma\\\\\\\\\\\\\\\\IMG_20250326_151738.png 380 538 1544 1985\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Emma\\\\\\\\\\\\\\\\IMG_20250326_151720.png 417 326 1549 1712\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\TrinhHuuTuan\\\\\\\\\\\\\\\\IMG1.png 199 545 810 1319\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\TrinhHuuTuan\\\\\\\\\\\\\\\\IMG2.png 298 659 907 1438\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\BotIu\\\\\\\\\\\\\\\\Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.png 589 739 1206 1280\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\BotIu\\\\\\\\\\\\\\\\Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.png 338 362 954 1198\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\BotIu\\\\\\\\\\\\\\\\Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.png 527 904 1096 1457\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\TrinhTranPhuongTuan\\\\\\\\\\\\\\\\Screenshot 2025-03-26 152756.png 73 208 429 642\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\TrinhTranPhuongTuan\\\\\\\\\\\\\\\\Screenshot 2025-03-26 152811.png 20 150 367 604\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\TrinhTranPhuongTuan\\\\\\\\\\\\\\\\Screenshot 2025-03-26 152730.png 73 210 394 633\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Viruss\\\\\\\\\\\\\\\\Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png 154 108 1417 1799\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Viruss\\\\\\\\\\\\\\\\Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png 364 442 910 1148\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Viruss\\\\\\\\\\\\\\\\Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png 50 121 926 1264\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\NgocKem\\\\\\\\\\\\\\\\IMG_1742972407559.png 304 247 820 976\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\NgocKem\\\\\\\\\\\\\\\\IMG_1742972428266.png 300 233 870 981\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\NgocKem\\\\\\\\\\\\\\\\IMG_1742972431298.png 180 168 796 998\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\ThaoNguyen\\\\\\\\\\\\\\\\IMG_20250325_173023.png 389 530 1184 1500\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\ThaoNguyen\\\\\\\\\\\\\\\\IMG_20250325_173028.png 490 449 1140 1231\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\ThaoNguyen\\\\\\\\\\\\\\\\IMG_20250325_173025.png 513 544 1284 1399\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\ThaoNguyen\\\\\\\\\\\\\\\\IMG_20250325_173026.png 489 328 1176 1142\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\AnhTu\\\\\\\\\\\\\\\\Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.png 321 361 1163 1438\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\AnhTu\\\\\\\\\\\\\\\\Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.png 217 635 1053 1802\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\AnhTu\\\\\\\\\\\\\\\\Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.png 474 420 1348 1596\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Thao\\\\\\\\\\\\\\\\98E5A551-3DC2-4A85-9497-AFA4A845325A.png 287 854 841 1536\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Thao\\\\\\\\\\\\\\\\851D4C65-07BE-4220-8488-84C40C1A79D6.png 163 460 841 1386\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Phuong\\\\\\\\\\\\\\\\Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.png 383 266 1204 1442\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Phuong\\\\\\\\\\\\\\\\Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.png 398 511 855 1063\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Phuong\\\\\\\\\\\\\\\\Messenger_creation_7343AEA7-71DD-40DB-B8C3-9D6F0CED60F9.png\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\ThienAn\\\\\\\\\\\\\\\\att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.png 357 494 1083 1463\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LeAnhTrang\\\\\\\\\\\\\\\\IMG_1742972296094.png 436 138 965 850\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\LeAnhTrang\\\\\\\\\\\\\\\\IMG_20250326_134739.png 417 628 1522 2056\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Jack\\\\\\\\\\\\\\\\A3FED369-0E1F-4064-830E-FBFD69D77C85.png 111 709 831 1654\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\Jack\\\\\\\\\\\\\\\\2209C349-3B30-47AC-89C1-1152E1A42FFC.png 224 606 982 1624\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\NghiemAnhHieu\\\\\\\\\\\\\\\\IMG_20250327_092236.png 274 800 1031 1820\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\NghiemAnhHieu\\\\\\\\\\\\\\\\IMG_20250327_092241.png 486 1016 840 1487\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\NghiemAnhHieu\\\\\\\\\\\\\\\\IMG_0327.png 83 74 764 794\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\NghiemAnhHieu\\\\\\\\\\\\\\\\IMG_20250327_092237.png 566 1129 1142 1953\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\TranDangHieu\\\\\\\\\\\\\\\\download (2).png 152 762 696 1478\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\TranDangHieu\\\\\\\\\\\\\\\\download (1).png 262 672 767 1357\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\TranDangHieu\\\\\\\\\\\\\\\\download.png 202 705 750 1444\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\BuiCongSon\\\\\\\\\\\\\\\\485424995_1386922852752466_129752981138586263_n.png 320 554 1058 1544\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\BuiCongSon\\\\\\\\\\\\\\\\485512577_2113779945729669_160018376255303225_n.png 318 390 1184 1528\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\BuiCongSon\\\\\\\\\\\\\\\\485464001_1219012669643907_2610181536763391909_n.png 461 308 1375 1574\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\BuiCongSon\\\\\\\\\\\\\\\\z6377532752047_4b7b3c7d460239f92fa3cec037738eca.png 462 527 1017 1272\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\BuiCongSon\\\\\\\\\\\\\\\\485290655_1069958524968534_6128511440722701069_n.png 376 472 1198 1582\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\TuSenna\\\\\\\\\\\\\\\\IMG_20250326_135145.png 80 165 468 685\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\TuSenna\\\\\\\\\\\\\\\\IMG_20250326_135147.png 225 199 579 683\\\\\\\\n-Dataset/FaceData/processed\\\\\\\\\\\\\\\\TuSenna\\\\\\\\\\\\\\\\IMG_20250326_135142.png 171 209 549 722\\\\\\\\ndiff --git a/Dataset/FaceData/test/data/txl/revision_info.txt b/Dataset/FaceData/test/data/txl/revision_info.txt\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 33dea13..0000000\\\\\\\\n--- a/Dataset/FaceData/test/data/txl/revision_info.txt\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,7 +0,0 @@\\\\\\\\n-arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25\\\\\\\\n---------------------\\\\\\\\n-tensorflow version: 2.19.0\\\\\\\\n---------------------\\\\\\\\n-git hash: b\\\\\\\\\\\\\\\'69ff1e149c0d84a123d6516ddd82970e65392608\\\\\\\\\\\\\\\'\\\\\\\\n---------------------\\\\\\\\n-b\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\'\\\\\\\\n\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\ndiff --git a/cmd.txt b/cmd.txt\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex a3d455b..0000000\\\\\\\\n--- a/cmd.txt\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,6 +0,0 @@\\\\\\\\n-python src/align_dataset_mtcnn.py  Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32  --random_order --gpu_memory_fraction 0.25\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-python src/classifier.py TRAIN Dataset/FaceData/processed Models/20180402-114759.pb Models/facemodel.pkl --batch_size 1000\\\\\\\\n-\\\\\\\\n-python src/face_rec_cam.py \\\\\\\\n\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\ndiff --git a/src/compare.py b/src/compare.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex bc53cc4..0000000\\\\\\\\n--- a/src/compare.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,130 +0,0 @@\\\\\\\\n-"""Performs face alignment and calculates L2 distance between the embeddings of images."""\\\\\\\\n-\\\\\\\\n-# MIT License\\\\\\\\n-# \\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\n-# \\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\n-# \\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\n-# \\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\n-# SOFTWARE.\\\\\\\\n-\\\\\\\\n-from __future__ import absolute_import\\\\\\\\n-from __future__ import division\\\\\\\\n-from __future__ import print_function\\\\\\\\n-\\\\\\\\n-from scipy import misc\\\\\\\\n-import tensorflow as tf\\\\\\\\n-import numpy as np\\\\\\\\n-import sys\\\\\\\\n-import os\\\\\\\\n-import copy\\\\\\\\n-import argparse\\\\\\\\n-import facenet\\\\\\\\n-import align.detect_face\\\\\\\\n-\\\\\\\\n-def main(args):\\\\\\\\n-\\\\\\\\n-    images = load_and_align_data(args.image_files, args.image_size, args.margin, args.gpu_memory_fraction)\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\n-\\\\\\\\n-        with tf.Session() as sess:\\\\\\\\n-      \\\\\\\\n-            # Load the model\\\\\\\\n-            facenet.load_model(args.model)\\\\\\\\n-    \\\\\\\\n-            # Get input and output tensors\\\\\\\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\n-\\\\\\\\n-            # Run forward pass to calculate embeddings\\\\\\\\n-            feed_dict = { images_placeholder: images, phase_train_placeholder:False }\\\\\\\\n-            emb = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\n-            \\\\\\\\n-            nrof_images = len(args.image_files)\\\\\\\\n-\\\\\\\\n-            print(\\\\\\\\\\\\\\\'Images:\\\\\\\\\\\\\\\')\\\\\\\\n-            for i in range(nrof_images):\\\\\\\\n-                print(\\\\\\\\\\\\\\\'%1d: %s\\\\\\\\\\\\\\\' % (i, args.image_files[i]))\\\\\\\\n-            print(\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-            \\\\\\\\n-            # Print distance matrix\\\\\\\\n-            print(\\\\\\\\\\\\\\\'Distance matrix\\\\\\\\\\\\\\\')\\\\\\\\n-            print(\\\\\\\\\\\\\\\'    \\\\\\\\\\\\\\\', end=\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-            for i in range(nrof_images):\\\\\\\\n-                print(\\\\\\\\\\\\\\\'    %1d     \\\\\\\\\\\\\\\' % i, end=\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-            print(\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-            for i in range(nrof_images):\\\\\\\\n-                print(\\\\\\\\\\\\\\\'%1d  \\\\\\\\\\\\\\\' % i, end=\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-                for j in range(nrof_images):\\\\\\\\n-                    dist = np.sqrt(np.sum(np.square(np.subtract(emb[i,:], emb[j,:]))))\\\\\\\\n-                    print(\\\\\\\\\\\\\\\'  %1.4f  \\\\\\\\\\\\\\\' % dist, end=\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-                print(\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\n-            \\\\\\\\n-            \\\\\\\\n-def load_and_align_data(image_paths, image_size, margin, gpu_memory_fraction):\\\\\\\\n-\\\\\\\\n-    minsize = 20 # minimum size of face\\\\\\\\n-    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\\\\\\\\\\\\\\\'s threshold\\\\\\\\n-    factor = 0.709 # scale factor\\\\\\\\n-    \\\\\\\\n-    print(\\\\\\\\\\\\\\\'Creating networks and loading parameters\\\\\\\\\\\\\\\')\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\\\\\\\\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\\\\\n-        with sess.as_default():\\\\\\\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\\\\\\\\n-  \\\\\\\\n-    tmp_image_paths=copy.copy(image_paths)\\\\\\\\n-    img_list = []\\\\\\\\n-    for image in tmp_image_paths:\\\\\\\\n-        img = misc.imread(os.path.expanduser(image), mode=\\\\\\\\\\\\\\\'RGB\\\\\\\\\\\\\\\')\\\\\\\\n-        img_size = np.asarray(img.shape)[0:2]\\\\\\\\n-        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\\\\\\\\n-        if len(bounding_boxes) < 1:\\\\\\\\n-          image_paths.remove(image)\\\\\\\\n-          print("can\\\\\\\\\\\\\\\'t detect face, remove ", image)\\\\\\\\n-          continue\\\\\\\\n-        det = np.squeeze(bounding_boxes[0,0:4])\\\\\\\\n-        bb = np.zeros(4, dtype=np.int32)\\\\\\\\n-        bb[0] = np.maximum(det[0]-margin/2, 0)\\\\\\\\n-        bb[1] = np.maximum(det[1]-margin/2, 0)\\\\\\\\n-        bb[2] = np.minimum(det[2]+margin/2, img_size[1])\\\\\\\\n-        bb[3] = np.minimum(det[3]+margin/2, img_size[0])\\\\\\\\n-        cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\\\\\\\\n-        aligned = misc.imresize(cropped, (image_size, image_size), interp=\\\\\\\\\\\\\\\'bilinear\\\\\\\\\\\\\\\')\\\\\\\\n-        prewhitened = facenet.prewhiten(aligned)\\\\\\\\n-        img_list.append(prewhitened)\\\\\\\\n-    images = np.stack(img_list)\\\\\\\\n-    return images\\\\\\\\n-\\\\\\\\n-def parse_arguments(argv):\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\n-    \\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'model\\\\\\\\\\\\\\\', type=str, \\\\\\\\n-        help=\\\\\\\\\\\\\\\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'image_files\\\\\\\\\\\\\\\', type=str, nargs=\\\\\\\\\\\\\\\'+\\\\\\\\\\\\\\\', help=\\\\\\\\\\\\\\\'Images to compare\\\\\\\\\\\\\\\')\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--image_size\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Image size (height, width) in pixels.\\\\\\\\\\\\\\\', default=160)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--margin\\\\\\\\\\\\\\\', type=int,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Margin for the crop around the bounding box (height, width) in pixels.\\\\\\\\\\\\\\\', default=44)\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--gpu_memory_fraction\\\\\\\\\\\\\\\', type=float,\\\\\\\\n-        help=\\\\\\\\\\\\\\\'Upper bound on the amount of GPU memory that will be used by the process.\\\\\\\\\\\\\\\', default=1.0)\\\\\\\\n-    return parser.parse_args(argv)\\\\\\\\n-\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\':\\\\\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\\\\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 455f67a..0000000\\\\\\\\n--- a/src/face_rec_cam.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,135 +0,0 @@\\\\\\\\n-from __future__ import absolute_import\\\\\\\\n-from __future__ import division\\\\\\\\n-from __future__ import print_function\\\\\\\\n-\\\\\\\\n-import tensorflow as tf\\\\\\\\n-from imutils.video import VideoStream\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-import argparse\\\\\\\\n-import facenet\\\\\\\\n-import imutils\\\\\\\\n-import os\\\\\\\\n-import sys\\\\\\\\n-import math\\\\\\\\n-import pickle\\\\\\\\n-import align.detect_face\\\\\\\\n-import numpy as np\\\\\\\\n-import cv2\\\\\\\\n-import collections\\\\\\\\n-from sklearn.svm import SVC\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-def main():\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--path\\\\\\\\\\\\\\\', help=\\\\\\\\\\\\\\\'Path of the video you want to test on.\\\\\\\\\\\\\\\', default=0)\\\\\\\\n-    args = parser.parse_args()\\\\\\\\n-\\\\\\\\n-    MINSIZE = 20\\\\\\\\n-    THRESHOLD = [0.6, 0.7, 0.7]\\\\\\\\n-    FACTOR = 0.709\\\\\\\\n-    IMAGE_SIZE = 182\\\\\\\\n-    INPUT_IMAGE_SIZE = 160\\\\\\\\n-    CLASSIFIER_PATH = \\\\\\\\\\\\\\\'Models/facemodel.pkl\\\\\\\\\\\\\\\'\\\\\\\\n-    VIDEO_PATH = args.path\\\\\\\\n-    FACENET_MODEL_PATH = \\\\\\\\\\\\\\\'Models/20180402-114759.pb\\\\\\\\\\\\\\\'\\\\\\\\n-\\\\\\\\n-    # Load The Custom Classifier\\\\\\\\n-    with open(CLASSIFIER_PATH, \\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\') as file:\\\\\\\\n-        model, class_names = pickle.load(file)\\\\\\\\n-    print("Custom Classifier, Successfully loaded")\\\\\\\\n-\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\n-\\\\\\\\n-        # Cai dat GPU neu co\\\\\\\\n-        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\\\\\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\\\\\n-\\\\\\\\n-        with sess.as_default():\\\\\\\\n-\\\\\\\\n-            # Load the model\\\\\\\\n-            print(\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\')\\\\\\\\n-            facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\n-\\\\\\\\n-            # Get input and output tensors\\\\\\\\n-            graph = tf.compat.v1.get_default_graph()\\\\\\\\n-            images_placeholder = graph.get_tensor_by_name("input:0")\\\\\\\\n-            embeddings = graph.get_tensor_by_name("embeddings:0")\\\\\\\\n-            phase_train_placeholder = graph.get_tensor_by_name("phase_train:0")\\\\\\\\n-            embedding_size = embeddings.get_shape()[1]\\\\\\\\n-\\\\\\\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\\\\\n-\\\\\\\\n-            people_detected = set()\\\\\\\\n-            person_detected = collections.Counter()\\\\\\\\n-\\\\\\\\n-            cap  = VideoStream(src=0).start()\\\\\\\\n-\\\\\\\\n-            while (True):\\\\\\\\n-                frame = cap.read()\\\\\\\\n-                frame = imutils.resize(frame, width=600)\\\\\\\\n-                frame = cv2.flip(frame, 1)\\\\\\\\n-\\\\\\\\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\n-\\\\\\\\n-                faces_found = bounding_boxes.shape[0]\\\\\\\\n-                try:\\\\\\\\n-                    if faces_found > 1:\\\\\\\\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\n-                                    1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\n-                    elif faces_found > 0:\\\\\\\\n-                        det = bounding_boxes[:, 0:4]\\\\\\\\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\n-                        for i in range(faces_found):\\\\\\\\n-                            bb[i][0] = det[i][0]\\\\\\\\n-                            bb[i][1] = det[i][1]\\\\\\\\n-                            bb[i][2] = det[i][2]\\\\\\\\n-                            bb[i][3] = det[i][3]\\\\\\\\n-                            print(bb[i][3]-bb[i][1])\\\\\\\\n-                            print(frame.shape[0])\\\\\\\\n-                            print((bb[i][3]-bb[i][1])/frame.shape[0])\\\\\\\\n-                            if (bb[i][3]-bb[i][1])/frame.shape[0]>0.25:\\\\\\\\n-                                cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\\\\\n-                                                    interpolation=cv2.INTER_CUBIC)\\\\\\\\n-                                scaled = facenet.prewhiten(scaled)\\\\\\\\n-                                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\n-                                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\n-                                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\n-\\\\\\\\n-                                predictions = model.predict_proba(emb_array)\\\\\\\\n-                                best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\n-                                best_class_probabilities = predictions[\\\\\\\\n-                                    np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\n-                                best_name = class_names[best_class_indices[0]]\\\\\\\\n-                                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-                                if best_class_probabilities > 0.8:\\\\\\\\n-                                    cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\\\\\\\n-                                    text_x = bb[i][0]\\\\\\\\n-                                    text_y = bb[i][3] + 20\\\\\\\\n-\\\\\\\\n-                                    name = class_names[best_class_indices[0]]\\\\\\\\n-                                    cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\n-                                    cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\\\\\\\\n-                                                cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\n-                                    person_detected[best_name] += 1\\\\\\\\n-                                else:\\\\\\\\n-                                    name = "Unknown"\\\\\\\\n-\\\\\\\\n-                except:\\\\\\\\n-                    pass\\\\\\\\n-\\\\\\\\n-                cv2.imshow(\\\\\\\\\\\\\\\'Face Recognition\\\\\\\\\\\\\\\', frame)\\\\\\\\n-                if cv2.waitKey(1) & 0xFF == ord(\\\\\\\\\\\\\\\'q\\\\\\\\\\\\\\\'):\\\\\\\\n-                    break\\\\\\\\n-\\\\\\\\n-            cap.release()\\\\\\\\n-            cv2.destroyAllWindows()\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-main()\\\\\\\\ndiff --git a/src/face_rec_flask.py b/src/face_rec_flask.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 3f7a27b..0000000\\\\\\\\n--- a/src/face_rec_flask.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,118 +0,0 @@\\\\\\\\n-from __future__ import absolute_import\\\\\\\\n-from __future__ import division\\\\\\\\n-from __future__ import print_function\\\\\\\\n-\\\\\\\\n-from flask import Flask\\\\\\\\n-from flask import render_template , request\\\\\\\\n-from flask_cors import CORS, cross_origin\\\\\\\\n-import tensorflow as tf\\\\\\\\n-import argparse\\\\\\\\n-import facenet\\\\\\\\n-import os\\\\\\\\n-import sys\\\\\\\\n-import math\\\\\\\\n-import pickle\\\\\\\\n-import align.detect_face\\\\\\\\n-import numpy as np\\\\\\\\n-import cv2\\\\\\\\n-import collections\\\\\\\\n-from sklearn.svm import SVC\\\\\\\\n-import base64\\\\\\\\n-\\\\\\\\n-MINSIZE = 20\\\\\\\\n-THRESHOLD = [0.6, 0.7, 0.7]\\\\\\\\n-FACTOR = 0.709\\\\\\\\n-IMAGE_SIZE = 182\\\\\\\\n-INPUT_IMAGE_SIZE = 160\\\\\\\\n-CLASSIFIER_PATH = \\\\\\\\\\\\\\\'./Models/facemodel.pkl\\\\\\\\\\\\\\\'\\\\\\\\n-FACENET_MODEL_PATH = \\\\\\\\\\\\\\\'./Models/20180402-114759.pb\\\\\\\\\\\\\\\'\\\\\\\\n-\\\\\\\\n-# Load The Custom Classifier\\\\\\\\n-with open(CLASSIFIER_PATH, \\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\') as file:\\\\\\\\n-    model, class_names = pickle.load(file)\\\\\\\\n-print("Custom Classifier, Successfully loaded")\\\\\\\\n-\\\\\\\\n-tf.Graph().as_default()\\\\\\\\n-\\\\\\\\n-# Cai dat GPU neu co\\\\\\\\n-gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\\\\\n-sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-# Load the model\\\\\\\\n-print(\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\')\\\\\\\\n-facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\n-\\\\\\\\n-# Get input and output tensors\\\\\\\\n-images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\n-embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\n-phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\n-embedding_size = embeddings.get_shape()[1]\\\\\\\\n-pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "align")\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-app = Flask(__name__)\\\\\\\\n-CORS(app)\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-@app.route(\\\\\\\\\\\\\\\'/\\\\\\\\\\\\\\\')\\\\\\\\n-@cross_origin()\\\\\\\\n-def index():\\\\\\\\n-    return "OK!";\\\\\\\\n-\\\\\\\\n-@app.route(\\\\\\\\\\\\\\\'/recog\\\\\\\\\\\\\\\', methods=[\\\\\\\\\\\\\\\'POST\\\\\\\\\\\\\\\'])\\\\\\\\n-@cross_origin()\\\\\\\\n-def upload_img_file():\\\\\\\\n-    if request.method == \\\\\\\\\\\\\\\'POST\\\\\\\\\\\\\\\':\\\\\\\\n-        # base 64\\\\\\\\n-        name="Unknown"\\\\\\\\n-        f = request.form.get(\\\\\\\\\\\\\\\'image\\\\\\\\\\\\\\\')\\\\\\\\n-        w = int(request.form.get(\\\\\\\\\\\\\\\'w\\\\\\\\\\\\\\\'))\\\\\\\\n-        h = int(request.form.get(\\\\\\\\\\\\\\\'h\\\\\\\\\\\\\\\'))\\\\\\\\n-\\\\\\\\n-        decoded_string = base64.b64decode(f)\\\\\\\\n-        frame = np.fromstring(decoded_string, dtype=np.uint8)\\\\\\\\n-        #frame = frame.reshape(w,h,3)\\\\\\\\n-        frame = cv2.imdecode(frame, cv2.IMREAD_ANYCOLOR)  # cv2.IMREAD_COLOR in OpenCV 3.1\\\\\\\\n-\\\\\\\\n-        bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\n-\\\\\\\\n-        faces_found = bounding_boxes.shape[0]\\\\\\\\n-\\\\\\\\n-        if faces_found > 0:\\\\\\\\n-            det = bounding_boxes[:, 0:4]\\\\\\\\n-            bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\n-            for i in range(faces_found):\\\\\\\\n-                bb[i][0] = det[i][0]\\\\\\\\n-                bb[i][1] = det[i][1]\\\\\\\\n-                bb[i][2] = det[i][2]\\\\\\\\n-                bb[i][3] = det[i][3]\\\\\\\\n-                cropped = frame\\\\\\\\n-                #cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\n-                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\\\\\n-                                    interpolation=cv2.INTER_CUBIC)\\\\\\\\n-                scaled = facenet.prewhiten(scaled)\\\\\\\\n-                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\n-                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\n-                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\n-                predictions = model.predict_proba(emb_array)\\\\\\\\n-                best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\n-                best_class_probabilities = predictions[\\\\\\\\n-                    np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\n-                best_name = class_names[best_class_indices[0]]\\\\\\\\n-                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\\\\\n-\\\\\\\\n-                if best_class_probabilities > 0.8:\\\\\\\\n-                    name = class_names[best_class_indices[0]]\\\\\\\\n-                else:\\\\\\\\n-                    name = "Unknown"\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-        return name;\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\':\\\\\\\\n-    app.run(debug=True, host=\\\\\\\\\\\\\\\'0.0.0.0\\\\\\\\\\\\\\\',port=\\\\\\\\\\\\\\\'8000\\\\\\\\\\\\\\\')\\\\\\\\n-\\\\\\\\ndiff --git a/src/models/__init__.py b/src/models/__init__.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex efa6252..0000000\\\\\\\\n--- a/src/models/__init__.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,2 +0,0 @@\\\\\\\\n-# flake8: noqa\\\\\\\\n-\\\\\\\\ndiff --git a/src/models/dummy.py b/src/models/dummy.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 7afe1ef..0000000\\\\\\\\n--- a/src/models/dummy.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,54 +0,0 @@\\\\\\\\n-"""Dummy model used only for testing\\\\\\\\n-"""\\\\\\\\n-# MIT License\\\\\\\\n-# \\\\\\\\n-# Copyright (c) 2016 David Sandberg\\\\\\\\n-# \\\\\\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\\\\\n-# in the Software without restriction, including without limitation the rights\\\\\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\\\\\n-# furnished to do so, subject to the following conditions:\\\\\\\\n-# \\\\\\\\n-# The above copyright notice and this permission notice shall be included in all\\\\\\\\n-# copies or substantial portions of the Software.\\\\\\\\n-# \\\\\\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\\\\\n-# SOFTWARE.\\\\\\\\n-\\\\\\\\n-from __future__ import absolute_import\\\\\\\\n-from __future__ import division\\\\\\\\n-from __future__ import print_function\\\\\\\\n-\\\\\\\\n-import tensorflow as tf\\\\\\\\n-import tensorflow.contrib.slim as slim\\\\\\\\n-import numpy as np\\\\\\\\n-  \\\\\\\\n-def inference(images, keep_probability, phase_train=True,  # @UnusedVariable\\\\\\\\n-              bottleneck_layer_size=128, bottleneck_layer_activation=None, weight_decay=0.0, reuse=None):  # @UnusedVariable\\\\\\\\n-    batch_norm_params = {\\\\\\\\n-        # Decay for the moving averages.\\\\\\\\n-        \\\\\\\\\\\\\\\'decay\\\\\\\\\\\\\\\': 0.995,\\\\\\\\n-        # epsilon to prevent 0s in variance.\\\\\\\\n-        \\\\\\\\\\\\\\\'epsilon\\\\\\\\\\\\\\\': 0.001,\\\\\\\\n-        # force in-place updates of mean and variance estimates\\\\\\\\n-        \\\\\\\\\\\\\\\'updates_collections\\\\\\\\\\\\\\\': None,\\\\\\\\n-        # Moving averages ends up in the trainable variables collection\\\\\\\\n-        \\\\\\\\\\\\\\\'variables_collections\\\\\\\\\\\\\\\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\\\\\\\\n-    }\\\\\\\\n-    \\\\\\\\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\\\\\\\\n-                        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\\\\\\\\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\\\\\\\\n-                        normalizer_fn=slim.batch_norm,\\\\\\\\n-                        normalizer_params=batch_norm_params):\\\\\\\\n-        size = np.prod(images.get_shape()[1:].as_list())\\\\\\\\n-        net = slim.fully_connected(tf.reshape(images, (-1,size)), bottleneck_layer_size, activation_fn=None, \\\\\\\\n-                scope=\\\\\\\\\\\\\\\'Bottleneck\\\\\\\\\\\\\\\', reuse=False)\\\\\\\\n-        return net, None\\\\\\\\ndiff --git a/src/models/inception_resnet_v1.py b/src/models/inception_resnet_v1.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 475e81b..0000000\\\\\\\\n--- a/src/models/inception_resnet_v1.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,246 +0,0 @@\\\\\\\\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\\\\\\\\n-#\\\\\\\\n-# Licensed under the Apache License, Version 2.0 (the "License");\\\\\\\\n-# you may not use this file except in compliance with the License.\\\\\\\\n-# You may obtain a copy of the License at\\\\\\\\n-#\\\\\\\\n-# http://www.apache.org/licenses/LICENSE-2.0\\\\\\\\n-#\\\\\\\\n-# Unless required by applicable law or agreed to in writing, software\\\\\\\\n-# distributed under the License is distributed on an "AS IS" BASIS,\\\\\\\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\\\\\\\n-# See the License for the specific language governing permissions and\\\\\\\\n-# limitations under the License.\\\\\\\\n-# ==============================================================================\\\\\\\\n-\\\\\\\\n-"""Contains the definition of the Inception Resnet V1 architecture.\\\\\\\\n-As described in http://arxiv.org/abs/1602.07261.\\\\\\\\n-  Inception-v4, Inception-ResNet and the Impact of Residual Connections\\\\\\\\n-    on Learning\\\\\\\\n-  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\\\\\\\\n-"""\\\\\\\\n-from __future__ import absolute_import\\\\\\\\n-from __future__ import division\\\\\\\\n-from __future__ import print_function\\\\\\\\n-\\\\\\\\n-import tensorflow as tf\\\\\\\\n-import tensorflow.contrib.slim as slim\\\\\\\\n-\\\\\\\\n-# Inception-Resnet-A\\\\\\\\n-def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\\\\\\\n-    """Builds the 35x35 resnet block."""\\\\\\\\n-    with tf.variable_scope(scope, \\\\\\\\\\\\\\\'Block35\\\\\\\\\\\\\\\', [net], reuse=reuse):\\\\\\\\n-        with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_0\\\\\\\\\\\\\\\'):\\\\\\\\n-            tower_conv = slim.conv2d(net, 32, 1, scope=\\\\\\\\\\\\\\\'Conv2d_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-        with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_1\\\\\\\\\\\\\\\'):\\\\\\\\n-            tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\\\\\\\\\\\\\\\'Conv2d_0b_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-        with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_2\\\\\\\\\\\\\\\'):\\\\\\\\n-            tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-            tower_conv2_1 = slim.conv2d(tower_conv2_0, 32, 3, scope=\\\\\\\\\\\\\\\'Conv2d_0b_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-            tower_conv2_2 = slim.conv2d(tower_conv2_1, 32, 3, scope=\\\\\\\\\\\\\\\'Conv2d_0c_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-        mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\\\\\\\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\\\\\\\n-                         activation_fn=None, scope=\\\\\\\\\\\\\\\'Conv2d_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-        net += scale * up\\\\\\\\n-        if activation_fn:\\\\\\\\n-            net = activation_fn(net)\\\\\\\\n-    return net\\\\\\\\n-\\\\\\\\n-# Inception-Resnet-B\\\\\\\\n-def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\\\\\\\n-    """Builds the 17x17 resnet block."""\\\\\\\\n-    with tf.variable_scope(scope, \\\\\\\\\\\\\\\'Block17\\\\\\\\\\\\\\\', [net], reuse=reuse):\\\\\\\\n-        with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_0\\\\\\\\\\\\\\\'):\\\\\\\\n-            tower_conv = slim.conv2d(net, 128, 1, scope=\\\\\\\\\\\\\\\'Conv2d_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-        with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_1\\\\\\\\\\\\\\\'):\\\\\\\\n-            tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 128, [1, 7],\\\\\\\\n-                                        scope=\\\\\\\\\\\\\\\'Conv2d_0b_1x7\\\\\\\\\\\\\\\')\\\\\\\\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 128, [7, 1],\\\\\\\\n-                                        scope=\\\\\\\\\\\\\\\'Conv2d_0c_7x1\\\\\\\\\\\\\\\')\\\\\\\\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\\\\\\\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\\\\\\\n-                         activation_fn=None, scope=\\\\\\\\\\\\\\\'Conv2d_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-        net += scale * up\\\\\\\\n-        if activation_fn:\\\\\\\\n-            net = activation_fn(net)\\\\\\\\n-    return net\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-# Inception-Resnet-C\\\\\\\\n-def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\\\\\\\n-    """Builds the 8x8 resnet block."""\\\\\\\\n-    with tf.variable_scope(scope, \\\\\\\\\\\\\\\'Block8\\\\\\\\\\\\\\\', [net], reuse=reuse):\\\\\\\\n-        with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_0\\\\\\\\\\\\\\\'):\\\\\\\\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\\\\\\\\\\\\\\\'Conv2d_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-        with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_1\\\\\\\\\\\\\\\'):\\\\\\\\n-            tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 192, [1, 3],\\\\\\\\n-                                        scope=\\\\\\\\\\\\\\\'Conv2d_0b_1x3\\\\\\\\\\\\\\\')\\\\\\\\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [3, 1],\\\\\\\\n-                                        scope=\\\\\\\\\\\\\\\'Conv2d_0c_3x1\\\\\\\\\\\\\\\')\\\\\\\\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\\\\\\\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\\\\\\\n-                         activation_fn=None, scope=\\\\\\\\\\\\\\\'Conv2d_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-        net += scale * up\\\\\\\\n-        if activation_fn:\\\\\\\\n-            net = activation_fn(net)\\\\\\\\n-    return net\\\\\\\\n-  \\\\\\\\n-def reduction_a(net, k, l, m, n):\\\\\\\\n-    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_0\\\\\\\\\\\\\\\'):\\\\\\\\n-        tower_conv = slim.conv2d(net, n, 3, stride=2, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                 scope=\\\\\\\\\\\\\\\'Conv2d_1a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_1\\\\\\\\\\\\\\\'):\\\\\\\\n-        tower_conv1_0 = slim.conv2d(net, k, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-        tower_conv1_1 = slim.conv2d(tower_conv1_0, l, 3,\\\\\\\\n-                                    scope=\\\\\\\\\\\\\\\'Conv2d_0b_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-        tower_conv1_2 = slim.conv2d(tower_conv1_1, m, 3,\\\\\\\\n-                                    stride=2, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                    scope=\\\\\\\\\\\\\\\'Conv2d_1a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_2\\\\\\\\\\\\\\\'):\\\\\\\\n-        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                     scope=\\\\\\\\\\\\\\\'MaxPool_1a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-    net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\\\\\\\\n-    return net\\\\\\\\n-\\\\\\\\n-def reduction_b(net):\\\\\\\\n-    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_0\\\\\\\\\\\\\\\'):\\\\\\\\n-        tower_conv = slim.conv2d(net, 256, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\\\\\\\\n-                                   padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\', scope=\\\\\\\\\\\\\\\'Conv2d_1a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_1\\\\\\\\\\\\\\\'):\\\\\\\\n-        tower_conv1 = slim.conv2d(net, 256, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-        tower_conv1_1 = slim.conv2d(tower_conv1, 256, 3, stride=2,\\\\\\\\n-                                    padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\', scope=\\\\\\\\\\\\\\\'Conv2d_1a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_2\\\\\\\\\\\\\\\'):\\\\\\\\n-        tower_conv2 = slim.conv2d(net, 256, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-        tower_conv2_1 = slim.conv2d(tower_conv2, 256, 3,\\\\\\\\n-                                    scope=\\\\\\\\\\\\\\\'Conv2d_0b_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-        tower_conv2_2 = slim.conv2d(tower_conv2_1, 256, 3, stride=2,\\\\\\\\n-                                    padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\', scope=\\\\\\\\\\\\\\\'Conv2d_1a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_3\\\\\\\\\\\\\\\'):\\\\\\\\n-        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                     scope=\\\\\\\\\\\\\\\'MaxPool_1a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-    net = tf.concat([tower_conv_1, tower_conv1_1,\\\\\\\\n-                        tower_conv2_2, tower_pool], 3)\\\\\\\\n-    return net\\\\\\\\n-  \\\\\\\\n-def inference(images, keep_probability, phase_train=True, \\\\\\\\n-              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\\\\\\\\n-    batch_norm_params = {\\\\\\\\n-        # Decay for the moving averages.\\\\\\\\n-        \\\\\\\\\\\\\\\'decay\\\\\\\\\\\\\\\': 0.995,\\\\\\\\n-        # epsilon to prevent 0s in variance.\\\\\\\\n-        \\\\\\\\\\\\\\\'epsilon\\\\\\\\\\\\\\\': 0.001,\\\\\\\\n-        # force in-place updates of mean and variance estimates\\\\\\\\n-        \\\\\\\\\\\\\\\'updates_collections\\\\\\\\\\\\\\\': None,\\\\\\\\n-        # Moving averages ends up in the trainable variables collection\\\\\\\\n-        \\\\\\\\\\\\\\\'variables_collections\\\\\\\\\\\\\\\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\\\\\\\\n-    }\\\\\\\\n-    \\\\\\\\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\\\\\\\\n-                        weights_initializer=slim.initializers.xavier_initializer(), \\\\\\\\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\\\\\\\\n-                        normalizer_fn=slim.batch_norm,\\\\\\\\n-                        normalizer_params=batch_norm_params):\\\\\\\\n-        return inception_resnet_v1(images, is_training=phase_train,\\\\\\\\n-              dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size, reuse=reuse)\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-def inception_resnet_v1(inputs, is_training=True,\\\\\\\\n-                        dropout_keep_prob=0.8,\\\\\\\\n-                        bottleneck_layer_size=128,\\\\\\\\n-                        reuse=None, \\\\\\\\n-                        scope=\\\\\\\\\\\\\\\'InceptionResnetV1\\\\\\\\\\\\\\\'):\\\\\\\\n-    """Creates the Inception Resnet V1 model.\\\\\\\\n-    Args:\\\\\\\\n-      inputs: a 4-D tensor of size [batch_size, height, width, 3].\\\\\\\\n-      num_classes: number of predicted classes.\\\\\\\\n-      is_training: whether is training or not.\\\\\\\\n-      dropout_keep_prob: float, the fraction to keep before final layer.\\\\\\\\n-      reuse: whether or not the network and its variables should be reused. To be\\\\\\\\n-        able to reuse \\\\\\\\\\\\\\\'scope\\\\\\\\\\\\\\\' must be given.\\\\\\\\n-      scope: Optional variable_scope.\\\\\\\\n-    Returns:\\\\\\\\n-      logits: the logits outputs of the model.\\\\\\\\n-      end_points: the set of end_points from the inception model.\\\\\\\\n-    """\\\\\\\\n-    end_points = {}\\\\\\\\n-  \\\\\\\\n-    with tf.variable_scope(scope, \\\\\\\\\\\\\\\'InceptionResnetV1\\\\\\\\\\\\\\\', [inputs], reuse=reuse):\\\\\\\\n-        with slim.arg_scope([slim.batch_norm, slim.dropout],\\\\\\\\n-                            is_training=is_training):\\\\\\\\n-            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\\\\\\\\n-                                stride=1, padding=\\\\\\\\\\\\\\\'SAME\\\\\\\\\\\\\\\'):\\\\\\\\n-      \\\\\\\\n-                # 149 x 149 x 32\\\\\\\\n-                net = slim.conv2d(inputs, 32, 3, stride=2, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                  scope=\\\\\\\\\\\\\\\'Conv2d_1a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Conv2d_1a_3x3\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                # 147 x 147 x 32\\\\\\\\n-                net = slim.conv2d(net, 32, 3, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                  scope=\\\\\\\\\\\\\\\'Conv2d_2a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Conv2d_2a_3x3\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                # 147 x 147 x 64\\\\\\\\n-                net = slim.conv2d(net, 64, 3, scope=\\\\\\\\\\\\\\\'Conv2d_2b_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Conv2d_2b_3x3\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                # 73 x 73 x 64\\\\\\\\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                      scope=\\\\\\\\\\\\\\\'MaxPool_3a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'MaxPool_3a_3x3\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                # 73 x 73 x 80\\\\\\\\n-                net = slim.conv2d(net, 80, 1, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                  scope=\\\\\\\\\\\\\\\'Conv2d_3b_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Conv2d_3b_1x1\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                # 71 x 71 x 192\\\\\\\\n-                net = slim.conv2d(net, 192, 3, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                  scope=\\\\\\\\\\\\\\\'Conv2d_4a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Conv2d_4a_3x3\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                # 35 x 35 x 256\\\\\\\\n-                net = slim.conv2d(net, 256, 3, stride=2, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                  scope=\\\\\\\\\\\\\\\'Conv2d_4b_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Conv2d_4b_3x3\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                \\\\\\\\n-                # 5 x Inception-resnet-A\\\\\\\\n-                net = slim.repeat(net, 5, block35, scale=0.17)\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Mixed_5a\\\\\\\\\\\\\\\'] = net\\\\\\\\n-        \\\\\\\\n-                # Reduction-A\\\\\\\\n-                with tf.variable_scope(\\\\\\\\\\\\\\\'Mixed_6a\\\\\\\\\\\\\\\'):\\\\\\\\n-                    net = reduction_a(net, 192, 192, 256, 384)\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Mixed_6a\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                \\\\\\\\n-                # 10 x Inception-Resnet-B\\\\\\\\n-                net = slim.repeat(net, 10, block17, scale=0.10)\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Mixed_6b\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                \\\\\\\\n-                # Reduction-B\\\\\\\\n-                with tf.variable_scope(\\\\\\\\\\\\\\\'Mixed_7a\\\\\\\\\\\\\\\'):\\\\\\\\n-                    net = reduction_b(net)\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Mixed_7a\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                \\\\\\\\n-                # 5 x Inception-Resnet-C\\\\\\\\n-                net = slim.repeat(net, 5, block8, scale=0.20)\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Mixed_8a\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                \\\\\\\\n-                net = block8(net, activation_fn=None)\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Mixed_8b\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                \\\\\\\\n-                with tf.variable_scope(\\\\\\\\\\\\\\\'Logits\\\\\\\\\\\\\\\'):\\\\\\\\n-                    end_points[\\\\\\\\\\\\\\\'PrePool\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                    #pylint: disable=no-member\\\\\\\\n-                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                          scope=\\\\\\\\\\\\\\\'AvgPool_1a_8x8\\\\\\\\\\\\\\\')\\\\\\\\n-                    net = slim.flatten(net)\\\\\\\\n-          \\\\\\\\n-                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\\\\\\\\n-                                       scope=\\\\\\\\\\\\\\\'Dropout\\\\\\\\\\\\\\\')\\\\\\\\n-          \\\\\\\\n-                    end_points[\\\\\\\\\\\\\\\'PreLogitsFlatten\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                \\\\\\\\n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \\\\\\\\n-                        scope=\\\\\\\\\\\\\\\'Bottleneck\\\\\\\\\\\\\\\', reuse=False)\\\\\\\\n-  \\\\\\\\n-    return net, end_points\\\\\\\\ndiff --git a/src/models/inception_resnet_v2.py b/src/models/inception_resnet_v2.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 0fb176f..0000000\\\\\\\\n--- a/src/models/inception_resnet_v2.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,255 +0,0 @@\\\\\\\\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\\\\\\\\n-#\\\\\\\\n-# Licensed under the Apache License, Version 2.0 (the "License");\\\\\\\\n-# you may not use this file except in compliance with the License.\\\\\\\\n-# You may obtain a copy of the License at\\\\\\\\n-#\\\\\\\\n-# http://www.apache.org/licenses/LICENSE-2.0\\\\\\\\n-#\\\\\\\\n-# Unless required by applicable law or agreed to in writing, software\\\\\\\\n-# distributed under the License is distributed on an "AS IS" BASIS,\\\\\\\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\\\\\\\n-# See the License for the specific language governing permissions and\\\\\\\\n-# limitations under the License.\\\\\\\\n-# ==============================================================================\\\\\\\\n-\\\\\\\\n-"""Contains the definition of the Inception Resnet V2 architecture.\\\\\\\\n-As described in http://arxiv.org/abs/1602.07261.\\\\\\\\n-  Inception-v4, Inception-ResNet and the Impact of Residual Connections\\\\\\\\n-    on Learning\\\\\\\\n-  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\\\\\\\\n-"""\\\\\\\\n-from __future__ import absolute_import\\\\\\\\n-from __future__ import division\\\\\\\\n-from __future__ import print_function\\\\\\\\n-\\\\\\\\n-import tensorflow as tf\\\\\\\\n-import tensorflow.contrib.slim as slim\\\\\\\\n-\\\\\\\\n-# Inception-Resnet-A\\\\\\\\n-def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\\\\\\\n-    """Builds the 35x35 resnet block."""\\\\\\\\n-    with tf.variable_scope(scope, \\\\\\\\\\\\\\\'Block35\\\\\\\\\\\\\\\', [net], reuse=reuse):\\\\\\\\n-        with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_0\\\\\\\\\\\\\\\'):\\\\\\\\n-            tower_conv = slim.conv2d(net, 32, 1, scope=\\\\\\\\\\\\\\\'Conv2d_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-        with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_1\\\\\\\\\\\\\\\'):\\\\\\\\n-            tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\\\\\\\\\\\\\\\'Conv2d_0b_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-        with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_2\\\\\\\\\\\\\\\'):\\\\\\\\n-            tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-            tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\\\\\\\\\\\\\\\'Conv2d_0b_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-            tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\\\\\\\\\\\\\\\'Conv2d_0c_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-        mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\\\\\\\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\\\\\\\n-                         activation_fn=None, scope=\\\\\\\\\\\\\\\'Conv2d_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-        net += scale * up\\\\\\\\n-        if activation_fn:\\\\\\\\n-            net = activation_fn(net)\\\\\\\\n-    return net\\\\\\\\n-\\\\\\\\n-# Inception-Resnet-B\\\\\\\\n-def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\\\\\\\n-    """Builds the 17x17 resnet block."""\\\\\\\\n-    with tf.variable_scope(scope, \\\\\\\\\\\\\\\'Block17\\\\\\\\\\\\\\\', [net], reuse=reuse):\\\\\\\\n-        with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_0\\\\\\\\\\\\\\\'):\\\\\\\\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\\\\\\\\\\\\\\\'Conv2d_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-        with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_1\\\\\\\\\\\\\\\'):\\\\\\\\n-            tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\\\\\\\\n-                                        scope=\\\\\\\\\\\\\\\'Conv2d_0b_1x7\\\\\\\\\\\\\\\')\\\\\\\\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\\\\\\\\n-                                        scope=\\\\\\\\\\\\\\\'Conv2d_0c_7x1\\\\\\\\\\\\\\\')\\\\\\\\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\\\\\\\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\\\\\\\n-                         activation_fn=None, scope=\\\\\\\\\\\\\\\'Conv2d_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-        net += scale * up\\\\\\\\n-        if activation_fn:\\\\\\\\n-            net = activation_fn(net)\\\\\\\\n-    return net\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-# Inception-Resnet-C\\\\\\\\n-def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\\\\\\\n-    """Builds the 8x8 resnet block."""\\\\\\\\n-    with tf.variable_scope(scope, \\\\\\\\\\\\\\\'Block8\\\\\\\\\\\\\\\', [net], reuse=reuse):\\\\\\\\n-        with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_0\\\\\\\\\\\\\\\'):\\\\\\\\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\\\\\\\\\\\\\\\'Conv2d_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-        with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_1\\\\\\\\\\\\\\\'):\\\\\\\\n-            tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\\\\\\\\n-                                        scope=\\\\\\\\\\\\\\\'Conv2d_0b_1x3\\\\\\\\\\\\\\\')\\\\\\\\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\\\\\\\\n-                                        scope=\\\\\\\\\\\\\\\'Conv2d_0c_3x1\\\\\\\\\\\\\\\')\\\\\\\\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\\\\\\\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\\\\\\\n-                         activation_fn=None, scope=\\\\\\\\\\\\\\\'Conv2d_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-        net += scale * up\\\\\\\\n-        if activation_fn:\\\\\\\\n-            net = activation_fn(net)\\\\\\\\n-    return net\\\\\\\\n-  \\\\\\\\n-def inference(images, keep_probability, phase_train=True, \\\\\\\\n-              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\\\\\\\\n-    batch_norm_params = {\\\\\\\\n-        # Decay for the moving averages.\\\\\\\\n-        \\\\\\\\\\\\\\\'decay\\\\\\\\\\\\\\\': 0.995,\\\\\\\\n-        # epsilon to prevent 0s in variance.\\\\\\\\n-        \\\\\\\\\\\\\\\'epsilon\\\\\\\\\\\\\\\': 0.001,\\\\\\\\n-        # force in-place updates of mean and variance estimates\\\\\\\\n-        \\\\\\\\\\\\\\\'updates_collections\\\\\\\\\\\\\\\': None,\\\\\\\\n-        # Moving averages ends up in the trainable variables collection\\\\\\\\n-        \\\\\\\\\\\\\\\'variables_collections\\\\\\\\\\\\\\\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\\\\\\\\n-}\\\\\\\\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\\\\\\\\n-                        weights_initializer=slim.initializers.xavier_initializer(), \\\\\\\\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\\\\\\\\n-                        normalizer_fn=slim.batch_norm,\\\\\\\\n-                        normalizer_params=batch_norm_params):\\\\\\\\n-        return inception_resnet_v2(images, is_training=phase_train,\\\\\\\\n-              dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size, reuse=reuse)\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-def inception_resnet_v2(inputs, is_training=True,\\\\\\\\n-                        dropout_keep_prob=0.8,\\\\\\\\n-                        bottleneck_layer_size=128,\\\\\\\\n-                        reuse=None,\\\\\\\\n-                        scope=\\\\\\\\\\\\\\\'InceptionResnetV2\\\\\\\\\\\\\\\'):\\\\\\\\n-    """Creates the Inception Resnet V2 model.\\\\\\\\n-    Args:\\\\\\\\n-      inputs: a 4-D tensor of size [batch_size, height, width, 3].\\\\\\\\n-      num_classes: number of predicted classes.\\\\\\\\n-      is_training: whether is training or not.\\\\\\\\n-      dropout_keep_prob: float, the fraction to keep before final layer.\\\\\\\\n-      reuse: whether or not the network and its variables should be reused. To be\\\\\\\\n-        able to reuse \\\\\\\\\\\\\\\'scope\\\\\\\\\\\\\\\' must be given.\\\\\\\\n-      scope: Optional variable_scope.\\\\\\\\n-    Returns:\\\\\\\\n-      logits: the logits outputs of the model.\\\\\\\\n-      end_points: the set of end_points from the inception model.\\\\\\\\n-    """\\\\\\\\n-    end_points = {}\\\\\\\\n-  \\\\\\\\n-    with tf.variable_scope(scope, \\\\\\\\\\\\\\\'InceptionResnetV2\\\\\\\\\\\\\\\', [inputs], reuse=reuse):\\\\\\\\n-        with slim.arg_scope([slim.batch_norm, slim.dropout],\\\\\\\\n-                            is_training=is_training):\\\\\\\\n-            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\\\\\\\\n-                                stride=1, padding=\\\\\\\\\\\\\\\'SAME\\\\\\\\\\\\\\\'):\\\\\\\\n-      \\\\\\\\n-                # 149 x 149 x 32\\\\\\\\n-                net = slim.conv2d(inputs, 32, 3, stride=2, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                  scope=\\\\\\\\\\\\\\\'Conv2d_1a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Conv2d_1a_3x3\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                # 147 x 147 x 32\\\\\\\\n-                net = slim.conv2d(net, 32, 3, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                  scope=\\\\\\\\\\\\\\\'Conv2d_2a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Conv2d_2a_3x3\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                # 147 x 147 x 64\\\\\\\\n-                net = slim.conv2d(net, 64, 3, scope=\\\\\\\\\\\\\\\'Conv2d_2b_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Conv2d_2b_3x3\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                # 73 x 73 x 64\\\\\\\\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                      scope=\\\\\\\\\\\\\\\'MaxPool_3a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'MaxPool_3a_3x3\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                # 73 x 73 x 80\\\\\\\\n-                net = slim.conv2d(net, 80, 1, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                  scope=\\\\\\\\\\\\\\\'Conv2d_3b_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Conv2d_3b_1x1\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                # 71 x 71 x 192\\\\\\\\n-                net = slim.conv2d(net, 192, 3, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                  scope=\\\\\\\\\\\\\\\'Conv2d_4a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Conv2d_4a_3x3\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                # 35 x 35 x 192\\\\\\\\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                      scope=\\\\\\\\\\\\\\\'MaxPool_5a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'MaxPool_5a_3x3\\\\\\\\\\\\\\\'] = net\\\\\\\\n-        \\\\\\\\n-                # 35 x 35 x 320\\\\\\\\n-                with tf.variable_scope(\\\\\\\\\\\\\\\'Mixed_5b\\\\\\\\\\\\\\\'):\\\\\\\\n-                    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_0\\\\\\\\\\\\\\\'):\\\\\\\\n-                        tower_conv = slim.conv2d(net, 96, 1, scope=\\\\\\\\\\\\\\\'Conv2d_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-                    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_1\\\\\\\\\\\\\\\'):\\\\\\\\n-                        tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-                        tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\\\\\\\\n-                                                    scope=\\\\\\\\\\\\\\\'Conv2d_0b_5x5\\\\\\\\\\\\\\\')\\\\\\\\n-                    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_2\\\\\\\\\\\\\\\'):\\\\\\\\n-                        tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-                        tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\\\\\\\\n-                                                    scope=\\\\\\\\\\\\\\\'Conv2d_0b_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                        tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\\\\\\\\n-                                                    scope=\\\\\\\\\\\\\\\'Conv2d_0c_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_3\\\\\\\\\\\\\\\'):\\\\\\\\n-                        tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\\\\\\\\\\\\\\\'SAME\\\\\\\\\\\\\\\',\\\\\\\\n-                                                     scope=\\\\\\\\\\\\\\\'AvgPool_0a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                        tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\\\\\\\\n-                                                   scope=\\\\\\\\\\\\\\\'Conv2d_0b_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-                    net = tf.concat([tower_conv, tower_conv1_1,\\\\\\\\n-                                        tower_conv2_2, tower_pool_1], 3)\\\\\\\\n-        \\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Mixed_5b\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                net = slim.repeat(net, 10, block35, scale=0.17)\\\\\\\\n-        \\\\\\\\n-                # 17 x 17 x 1024\\\\\\\\n-                with tf.variable_scope(\\\\\\\\\\\\\\\'Mixed_6a\\\\\\\\\\\\\\\'):\\\\\\\\n-                    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_0\\\\\\\\\\\\\\\'):\\\\\\\\n-                        tower_conv = slim.conv2d(net, 384, 3, stride=2, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                                 scope=\\\\\\\\\\\\\\\'Conv2d_1a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_1\\\\\\\\\\\\\\\'):\\\\\\\\n-                        tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-                        tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\\\\\\\\n-                                                    scope=\\\\\\\\\\\\\\\'Conv2d_0b_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                        tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\\\\\\\\n-                                                    stride=2, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                                    scope=\\\\\\\\\\\\\\\'Conv2d_1a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_2\\\\\\\\\\\\\\\'):\\\\\\\\n-                        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                                     scope=\\\\\\\\\\\\\\\'MaxPool_1a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                    net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\\\\\\\\n-        \\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Mixed_6a\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                net = slim.repeat(net, 20, block17, scale=0.10)\\\\\\\\n-        \\\\\\\\n-                with tf.variable_scope(\\\\\\\\\\\\\\\'Mixed_7a\\\\\\\\\\\\\\\'):\\\\\\\\n-                    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_0\\\\\\\\\\\\\\\'):\\\\\\\\n-                        tower_conv = slim.conv2d(net, 256, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-                        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\\\\\\\\n-                                                   padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\', scope=\\\\\\\\\\\\\\\'Conv2d_1a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_1\\\\\\\\\\\\\\\'):\\\\\\\\n-                        tower_conv1 = slim.conv2d(net, 256, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-                        tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\\\\\\\\n-                                                    padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\', scope=\\\\\\\\\\\\\\\'Conv2d_1a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_2\\\\\\\\\\\\\\\'):\\\\\\\\n-                        tower_conv2 = slim.conv2d(net, 256, 1, scope=\\\\\\\\\\\\\\\'Conv2d_0a_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-                        tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\\\\\\\\n-                                                    scope=\\\\\\\\\\\\\\\'Conv2d_0b_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                        tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\\\\\\\\n-                                                    padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\', scope=\\\\\\\\\\\\\\\'Conv2d_1a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                    with tf.variable_scope(\\\\\\\\\\\\\\\'Branch_3\\\\\\\\\\\\\\\'):\\\\\\\\n-                        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                                     scope=\\\\\\\\\\\\\\\'MaxPool_1a_3x3\\\\\\\\\\\\\\\')\\\\\\\\n-                    net = tf.concat([tower_conv_1, tower_conv1_1,\\\\\\\\n-                                        tower_conv2_2, tower_pool], 3)\\\\\\\\n-        \\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Mixed_7a\\\\\\\\\\\\\\\'] = net\\\\\\\\n-        \\\\\\\\n-                net = slim.repeat(net, 9, block8, scale=0.20)\\\\\\\\n-                net = block8(net, activation_fn=None)\\\\\\\\n-        \\\\\\\\n-                net = slim.conv2d(net, 1536, 1, scope=\\\\\\\\\\\\\\\'Conv2d_7b_1x1\\\\\\\\\\\\\\\')\\\\\\\\n-                end_points[\\\\\\\\\\\\\\\'Conv2d_7b_1x1\\\\\\\\\\\\\\\'] = net\\\\\\\\n-        \\\\\\\\n-                with tf.variable_scope(\\\\\\\\\\\\\\\'Logits\\\\\\\\\\\\\\\'):\\\\\\\\n-                    end_points[\\\\\\\\\\\\\\\'PrePool\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                    #pylint: disable=no-member\\\\\\\\n-                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\\\\\\\\\\\\\\\'VALID\\\\\\\\\\\\\\\',\\\\\\\\n-                                          scope=\\\\\\\\\\\\\\\'AvgPool_1a_8x8\\\\\\\\\\\\\\\')\\\\\\\\n-                    net = slim.flatten(net)\\\\\\\\n-          \\\\\\\\n-                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\\\\\\\\n-                                       scope=\\\\\\\\\\\\\\\'Dropout\\\\\\\\\\\\\\\')\\\\\\\\n-          \\\\\\\\n-                    end_points[\\\\\\\\\\\\\\\'PreLogitsFlatten\\\\\\\\\\\\\\\'] = net\\\\\\\\n-                \\\\\\\\n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \\\\\\\\n-                        scope=\\\\\\\\\\\\\\\'Bottleneck\\\\\\\\\\\\\\\', reuse=False)\\\\\\\\n-  \\\\\\\\n-    return net, end_points\\\\\\\\ndiff --git a/src/models/squeezenet.py b/src/models/squeezenet.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex ae117e1..0000000\\\\\\\\n--- a/src/models/squeezenet.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,67 +0,0 @@\\\\\\\\n-from __future__ import absolute_import\\\\\\\\n-from __future__ import division\\\\\\\\n-from __future__ import print_function\\\\\\\\n-\\\\\\\\n-import tensorflow as tf\\\\\\\\n-import tensorflow.contrib.slim as slim\\\\\\\\n-\\\\\\\\n-def fire_module(inputs,\\\\\\\\n-                squeeze_depth,\\\\\\\\n-                expand_depth,\\\\\\\\n-                reuse=None,\\\\\\\\n-                scope=None,\\\\\\\\n-                outputs_collections=None):\\\\\\\\n-    with tf.variable_scope(scope, \\\\\\\\\\\\\\\'fire\\\\\\\\\\\\\\\', [inputs], reuse=reuse):\\\\\\\\n-        with slim.arg_scope([slim.conv2d, slim.max_pool2d],\\\\\\\\n-                            outputs_collections=None):\\\\\\\\n-            net = squeeze(inputs, squeeze_depth)\\\\\\\\n-            outputs = expand(net, expand_depth)\\\\\\\\n-            return outputs\\\\\\\\n-\\\\\\\\n-def squeeze(inputs, num_outputs):\\\\\\\\n-    return slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope=\\\\\\\\\\\\\\\'squeeze\\\\\\\\\\\\\\\')\\\\\\\\n-\\\\\\\\n-def expand(inputs, num_outputs):\\\\\\\\n-    with tf.variable_scope(\\\\\\\\\\\\\\\'expand\\\\\\\\\\\\\\\'):\\\\\\\\n-        e1x1 = slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope=\\\\\\\\\\\\\\\'1x1\\\\\\\\\\\\\\\')\\\\\\\\n-        e3x3 = slim.conv2d(inputs, num_outputs, [3, 3], scope=\\\\\\\\\\\\\\\'3x3\\\\\\\\\\\\\\\')\\\\\\\\n-    return tf.concat([e1x1, e3x3], 3)\\\\\\\\n-\\\\\\\\n-def inference(images, keep_probability, phase_train=True, bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\\\\\\\\n-    batch_norm_params = {\\\\\\\\n-        # Decay for the moving averages.\\\\\\\\n-        \\\\\\\\\\\\\\\'decay\\\\\\\\\\\\\\\': 0.995,\\\\\\\\n-        # epsilon to prevent 0s in variance.\\\\\\\\n-        \\\\\\\\\\\\\\\'epsilon\\\\\\\\\\\\\\\': 0.001,\\\\\\\\n-        # force in-place updates of mean and variance estimates\\\\\\\\n-        \\\\\\\\\\\\\\\'updates_collections\\\\\\\\\\\\\\\': None,\\\\\\\\n-        # Moving averages ends up in the trainable variables collection\\\\\\\\n-        \\\\\\\\\\\\\\\'variables_collections\\\\\\\\\\\\\\\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\\\\\\\\n-    }\\\\\\\\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\\\\\\\\n-                        weights_initializer=slim.xavier_initializer_conv2d(uniform=True),\\\\\\\\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\\\\\\\\n-                        normalizer_fn=slim.batch_norm,\\\\\\\\n-                        normalizer_params=batch_norm_params):\\\\\\\\n-        with tf.variable_scope(\\\\\\\\\\\\\\\'squeezenet\\\\\\\\\\\\\\\', [images], reuse=reuse):\\\\\\\\n-            with slim.arg_scope([slim.batch_norm, slim.dropout],\\\\\\\\n-                                is_training=phase_train):\\\\\\\\n-                net = slim.conv2d(images, 96, [7, 7], stride=2, scope=\\\\\\\\\\\\\\\'conv1\\\\\\\\\\\\\\\')\\\\\\\\n-                net = slim.max_pool2d(net, [3, 3], stride=2, scope=\\\\\\\\\\\\\\\'maxpool1\\\\\\\\\\\\\\\')\\\\\\\\n-                net = fire_module(net, 16, 64, scope=\\\\\\\\\\\\\\\'fire2\\\\\\\\\\\\\\\')\\\\\\\\n-                net = fire_module(net, 16, 64, scope=\\\\\\\\\\\\\\\'fire3\\\\\\\\\\\\\\\')\\\\\\\\n-                net = fire_module(net, 32, 128, scope=\\\\\\\\\\\\\\\'fire4\\\\\\\\\\\\\\\')\\\\\\\\n-                net = slim.max_pool2d(net, [2, 2], stride=2, scope=\\\\\\\\\\\\\\\'maxpool4\\\\\\\\\\\\\\\')\\\\\\\\n-                net = fire_module(net, 32, 128, scope=\\\\\\\\\\\\\\\'fire5\\\\\\\\\\\\\\\')\\\\\\\\n-                net = fire_module(net, 48, 192, scope=\\\\\\\\\\\\\\\'fire6\\\\\\\\\\\\\\\')\\\\\\\\n-                net = fire_module(net, 48, 192, scope=\\\\\\\\\\\\\\\'fire7\\\\\\\\\\\\\\\')\\\\\\\\n-                net = fire_module(net, 64, 256, scope=\\\\\\\\\\\\\\\'fire8\\\\\\\\\\\\\\\')\\\\\\\\n-                net = slim.max_pool2d(net, [3, 3], stride=2, scope=\\\\\\\\\\\\\\\'maxpool8\\\\\\\\\\\\\\\')\\\\\\\\n-                net = fire_module(net, 64, 256, scope=\\\\\\\\\\\\\\\'fire9\\\\\\\\\\\\\\\')\\\\\\\\n-                net = slim.dropout(net, keep_probability)\\\\\\\\n-                net = slim.conv2d(net, 1000, [1, 1], activation_fn=None, normalizer_fn=None, scope=\\\\\\\\\\\\\\\'conv10\\\\\\\\\\\\\\\')\\\\\\\\n-                net = slim.avg_pool2d(net, net.get_shape()[1:3], scope=\\\\\\\\\\\\\\\'avgpool10\\\\\\\\\\\\\\\')\\\\\\\\n-                net = tf.squeeze(net, [1, 2], name=\\\\\\\\\\\\\\\'logits\\\\\\\\\\\\\\\')\\\\\\\\n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \\\\\\\\n-                        scope=\\\\\\\\\\\\\\\'Bottleneck\\\\\\\\\\\\\\\', reuse=False)\\\\\\\\n-    return net, None\\\\\\\'\\\\n\\\\\\\\ No newline at end of file\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_54_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_54_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex 132948b..0000000\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_54_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_55_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_55_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex d589b1a..0000000\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_55_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro (2).jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex d47e20d..0000000\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex fbe6de7..0000000\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro (2).jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex 3e5846d..0000000\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex b23bcfa..0000000\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro (2).jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex bb832e9..0000000\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex a0b4312..0000000\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_59_Pro (2).jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_59_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex 928f751..0000000\\\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_59_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/raw/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg b/Dataset/FaceData/raw/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg\\\\ndeleted file mode 100644\\\\nindex 6537490..0000000\\\\nBinary files a/Dataset/FaceData/raw/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/raw/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg b/Dataset/FaceData/raw/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg\\\\ndeleted file mode 100644\\\\nindex 8f6b0f9..0000000\\\\nBinary files a/Dataset/FaceData/raw/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/raw/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg b/Dataset/FaceData/raw/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg\\\\ndeleted file mode 100644\\\\nindex f74031f..0000000\\\\nBinary files a/Dataset/FaceData/raw/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.jpeg b/Dataset/FaceData/test/data/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.jpeg\\\\ndeleted file mode 100644\\\\nindex b5ebe28..0000000\\\\nBinary files a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.jpeg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.jpeg b/Dataset/FaceData/test/data/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.jpeg\\\\ndeleted file mode 100644\\\\nindex af9bfea..0000000\\\\nBinary files a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.jpeg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.jpeg b/Dataset/FaceData/test/data/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.jpeg\\\\ndeleted file mode 100644\\\\nindex 28c29dc..0000000\\\\nBinary files a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.jpeg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.jpeg b/Dataset/FaceData/test/data/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.jpeg\\\\ndeleted file mode 100644\\\\nindex 979d22b..0000000\\\\nBinary files a/Dataset/FaceData/test/data/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.jpeg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.jpeg b/Dataset/FaceData/test/data/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.jpeg\\\\ndeleted file mode 100644\\\\nindex 95a477c..0000000\\\\nBinary files a/Dataset/FaceData/test/data/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.jpeg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.jpeg b/Dataset/FaceData/test/data/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.jpeg\\\\ndeleted file mode 100644\\\\nindex 208ec5c..0000000\\\\nBinary files a/Dataset/FaceData/test/data/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.jpeg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/ChuQuangHuy/IMG_0349.HEIC b/Dataset/FaceData/test/data/ChuQuangHuy/IMG_0349.HEIC\\\\ndeleted file mode 100644\\\\nindex 345acbc..0000000\\\\nBinary files a/Dataset/FaceData/test/data/ChuQuangHuy/IMG_0349.HEIC and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.jpg b/Dataset/FaceData/test/data/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.jpg\\\\ndeleted file mode 100644\\\\nindex b038c0c..0000000\\\\nBinary files a/Dataset/FaceData/test/data/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.jpg b/Dataset/FaceData/test/data/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.jpg\\\\ndeleted file mode 100644\\\\nindex e3de1ee..0000000\\\\nBinary files a/Dataset/FaceData/test/data/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro (3).jpg\\\\ndeleted file mode 100644\\\\nindex 967a588..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro (3).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex c4e9ec9..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex ab17f1f..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex e9e3371..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex dcd5b17..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (3).jpg\\\\ndeleted file mode 100644\\\\nindex bdf69fe..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (3).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex e5e0d42..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex cf7525e..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (3).jpg\\\\ndeleted file mode 100644\\\\nindex f22e4b7..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (3).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex 56ac445..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex a72c6b8..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex e4e969a..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_36_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_36_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex 94d80b4..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_36_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex ef3d4f9..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex d7333e8..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex f8bec2a..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (3).jpg\\\\ndeleted file mode 100644\\\\nindex 42fdad1..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (3).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex 9851605..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex 63fd876..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (3).jpg\\\\ndeleted file mode 100644\\\\nindex 90c377d..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (3).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex f669175..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex 5ba014d..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (3).jpg\\\\ndeleted file mode 100644\\\\nindex 17c4b93..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (3).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex 7c91590..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex 16d41be..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex 4271bdd..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex 0646f0b..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (3).jpg\\\\ndeleted file mode 100644\\\\nindex eeb4397..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (3).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex dba3891..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex b2c8e83..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (3).jpg\\\\ndeleted file mode 100644\\\\nindex a6e8ebf..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (3).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex d163084..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex 38971bf..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex 5d9bd5c..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex 9903ab8..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (3).jpg\\\\ndeleted file mode 100644\\\\nindex 5f8c977..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (3).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex 6db75f8..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro (2).jpg\\\\ndeleted file mode 100644\\\\nindex 2daaf72..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex e84c735..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_22_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_22_Pro.jpg\\\\ndeleted file mode 100644\\\\nindex 7888261..0000000\\\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_22_Pro.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_0327.JPG b/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_0327.JPG\\\\ndeleted file mode 100644\\\\nindex 9f03d5d..0000000\\\\nBinary files a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_0327.JPG and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092236.jpg b/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092236.jpg\\\\ndeleted file mode 100644\\\\nindex e587b19..0000000\\\\nBinary files a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092236.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092237.jpg b/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092237.jpg\\\\ndeleted file mode 100644\\\\nindex cf24b36..0000000\\\\nBinary files a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092237.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092241.jpg b/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092241.jpg\\\\ndeleted file mode 100644\\\\nindex a505a8a..0000000\\\\nBinary files a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092241.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/NgocKem/IMG_1742972407559.jpg b/Dataset/FaceData/test/data/NgocKem/IMG_1742972407559.jpg\\\\ndeleted file mode 100644\\\\nindex 0483ff8..0000000\\\\nBinary files a/Dataset/FaceData/test/data/NgocKem/IMG_1742972407559.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/NgocKem/IMG_1742972428266.jpg b/Dataset/FaceData/test/data/NgocKem/IMG_1742972428266.jpg\\\\ndeleted file mode 100644\\\\nindex 96d77fc..0000000\\\\nBinary files a/Dataset/FaceData/test/data/NgocKem/IMG_1742972428266.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/NgocKem/IMG_1742972431298.jpg b/Dataset/FaceData/test/data/NgocKem/IMG_1742972431298.jpg\\\\ndeleted file mode 100644\\\\nindex bd4c93c..0000000\\\\nBinary files a/Dataset/FaceData/test/data/NgocKem/IMG_1742972431298.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.jpeg b/Dataset/FaceData/test/data/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.jpeg\\\\ndeleted file mode 100644\\\\nindex 6aba153..0000000\\\\nBinary files a/Dataset/FaceData/test/data/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.jpeg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/Phuong/Messenger_creation_7343AEA7-71DD-40DB-B8C3-9D6F0CED60F9.jpeg b/Dataset/FaceData/test/data/Phuong/Messenger_creation_7343AEA7-71DD-40DB-B8C3-9D6F0CED60F9.jpeg\\\\ndeleted file mode 100644\\\\nindex d41ca36..0000000\\\\nBinary files a/Dataset/FaceData/test/data/Phuong/Messenger_creation_7343AEA7-71DD-40DB-B8C3-9D6F0CED60F9.jpeg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.jpeg b/Dataset/FaceData/test/data/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.jpeg\\\\ndeleted file mode 100644\\\\nindex 161aa4b..0000000\\\\nBinary files a/Dataset/FaceData/test/data/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.jpeg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173023.jpg b/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173023.jpg\\\\ndeleted file mode 100644\\\\nindex 4982c4f..0000000\\\\nBinary files a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173023.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173025.jpg b/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173025.jpg\\\\ndeleted file mode 100644\\\\nindex 85c8a30..0000000\\\\nBinary files a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173025.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173026.jpg b/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173026.jpg\\\\ndeleted file mode 100644\\\\nindex a18bfd6..0000000\\\\nBinary files a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173026.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173028.jpg b/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173028.jpg\\\\ndeleted file mode 100644\\\\nindex 5b6d155..0000000\\\\nBinary files a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173028.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.jpg b/Dataset/FaceData/test/data/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.jpg\\\\ndeleted file mode 100644\\\\nindex c688251..0000000\\\\nBinary files a/Dataset/FaceData/test/data/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/TranDangHieu/download (1).jpg b/Dataset/FaceData/test/data/TranDangHieu/download (1).jpg\\\\ndeleted file mode 100644\\\\nindex e0c8116..0000000\\\\nBinary files a/Dataset/FaceData/test/data/TranDangHieu/download (1).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/TranDangHieu/download (2).jpg b/Dataset/FaceData/test/data/TranDangHieu/download (2).jpg\\\\ndeleted file mode 100644\\\\nindex ef5f43d..0000000\\\\nBinary files a/Dataset/FaceData/test/data/TranDangHieu/download (2).jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/TranDangHieu/download.jpg b/Dataset/FaceData/test/data/TranDangHieu/download.jpg\\\\ndeleted file mode 100644\\\\nindex 3d6e0fb..0000000\\\\nBinary files a/Dataset/FaceData/test/data/TranDangHieu/download.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134208.jpg b/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134208.jpg\\\\ndeleted file mode 100644\\\\nindex 104e453..0000000\\\\nBinary files a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134208.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134210.jpg b/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134210.jpg\\\\ndeleted file mode 100644\\\\nindex c3ee227..0000000\\\\nBinary files a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134210.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134211.jpg b/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134211.jpg\\\\ndeleted file mode 100644\\\\nindex 77cc3f0..0000000\\\\nBinary files a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134211.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png b/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png\\\\ndeleted file mode 100644\\\\nindex 68c44d6..0000000\\\\nBinary files a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png b/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png\\\\ndeleted file mode 100644\\\\nindex f2dc8a1..0000000\\\\nBinary files a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png b/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png\\\\ndeleted file mode 100644\\\\nindex 9b27a7d..0000000\\\\nBinary files a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135142.jpg b/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135142.jpg\\\\ndeleted file mode 100644\\\\nindex 5b6bf55..0000000\\\\nBinary files a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135142.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135145.jpg b/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135145.jpg\\\\ndeleted file mode 100644\\\\nindex 46005cc..0000000\\\\nBinary files a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135145.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135147.jpg b/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135147.jpg\\\\ndeleted file mode 100644\\\\nindex 0c45781..0000000\\\\nBinary files a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135147.jpg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg b/Dataset/FaceData/test/data/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg\\\\ndeleted file mode 100644\\\\nindex 6537490..0000000\\\\nBinary files a/Dataset/FaceData/test/data/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg b/Dataset/FaceData/test/data/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg\\\\ndeleted file mode 100644\\\\nindex 8f6b0f9..0000000\\\\nBinary files a/Dataset/FaceData/test/data/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg b/Dataset/FaceData/test/data/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg\\\\ndeleted file mode 100644\\\\nindex f74031f..0000000\\\\nBinary files a/Dataset/FaceData/test/data/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.jpeg b/Dataset/FaceData/test/data/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.jpeg\\\\ndeleted file mode 100644\\\\nindex 7b62bc3..0000000\\\\nBinary files a/Dataset/FaceData/test/data/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.jpeg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.jpeg b/Dataset/FaceData/test/data/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.jpeg\\\\ndeleted file mode 100644\\\\nindex 1be1f19..0000000\\\\nBinary files a/Dataset/FaceData/test/data/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.jpeg and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.png b/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.png\\\\ndeleted file mode 100644\\\\nindex 87978ce..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.png b/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.png\\\\ndeleted file mode 100644\\\\nindex 5269259..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.png b/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.png\\\\ndeleted file mode 100644\\\\nindex cd8c24c..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.png b/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.png\\\\ndeleted file mode 100644\\\\nindex 0650f16..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.png b/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.png\\\\ndeleted file mode 100644\\\\nindex 714bc33..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.png b/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.png\\\\ndeleted file mode 100644\\\\nindex 9b3ac40..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/485290655_1069958524968534_6128511440722701069_n.png b/Dataset/FaceData/test/data/txl/BuiCongSon/485290655_1069958524968534_6128511440722701069_n.png\\\\ndeleted file mode 100644\\\\nindex cd86dc1..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/485290655_1069958524968534_6128511440722701069_n.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/485424995_1386922852752466_129752981138586263_n.png b/Dataset/FaceData/test/data/txl/BuiCongSon/485424995_1386922852752466_129752981138586263_n.png\\\\ndeleted file mode 100644\\\\nindex 90013f6..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/485424995_1386922852752466_129752981138586263_n.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/485464001_1219012669643907_2610181536763391909_n.png b/Dataset/FaceData/test/data/txl/BuiCongSon/485464001_1219012669643907_2610181536763391909_n.png\\\\ndeleted file mode 100644\\\\nindex 31c9d0c..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/485464001_1219012669643907_2610181536763391909_n.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/485512577_2113779945729669_160018376255303225_n.png b/Dataset/FaceData/test/data/txl/BuiCongSon/485512577_2113779945729669_160018376255303225_n.png\\\\ndeleted file mode 100644\\\\nindex efbe736..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/485512577_2113779945729669_160018376255303225_n.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/z6377532752047_4b7b3c7d460239f92fa3cec037738eca.png b/Dataset/FaceData/test/data/txl/BuiCongSon/z6377532752047_4b7b3c7d460239f92fa3cec037738eca.png\\\\ndeleted file mode 100644\\\\nindex 09d25b3..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/z6377532752047_4b7b3c7d460239f92fa3cec037738eca.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151626.png b/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151626.png\\\\ndeleted file mode 100644\\\\nindex 66a927d..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151626.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151720.png b/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151720.png\\\\ndeleted file mode 100644\\\\nindex 5554b2f..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151720.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151731.png b/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151731.png\\\\ndeleted file mode 100644\\\\nindex b92b066..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151731.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151738.png b/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151738.png\\\\ndeleted file mode 100644\\\\nindex 852ab42..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151738.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.png b/Dataset/FaceData/test/data/txl/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.png\\\\ndeleted file mode 100644\\\\nindex f6ad820..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.png b/Dataset/FaceData/test/data/txl/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.png\\\\ndeleted file mode 100644\\\\nindex 8021733..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_1742972296094.png b/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_1742972296094.png\\\\ndeleted file mode 100644\\\\nindex e7677d6..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_1742972296094.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_20250326_134739.png b/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_20250326_134739.png\\\\ndeleted file mode 100644\\\\nindex d8efdad..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_20250326_134739.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_0327.png b/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_0327.png\\\\ndeleted file mode 100644\\\\nindex 1037e8e..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_0327.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092236.png b/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092236.png\\\\ndeleted file mode 100644\\\\nindex 9be2f92..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092236.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092237.png b/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092237.png\\\\ndeleted file mode 100644\\\\nindex 17b5d34..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092237.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092241.png b/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092241.png\\\\ndeleted file mode 100644\\\\nindex 97fcfdb..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092241.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972407559.png b/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972407559.png\\\\ndeleted file mode 100644\\\\nindex 66c7877..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972407559.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972428266.png b/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972428266.png\\\\ndeleted file mode 100644\\\\nindex 47db00e..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972428266.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972431298.png b/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972431298.png\\\\ndeleted file mode 100644\\\\nindex 1bcc4bf..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972431298.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.png b/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.png\\\\ndeleted file mode 100644\\\\nindex 6d2cfbd..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.png b/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.png\\\\ndeleted file mode 100644\\\\nindex d1d90a5..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Thao/851D4C65-07BE-4220-8488-84C40C1A79D6.png b/Dataset/FaceData/test/data/txl/Thao/851D4C65-07BE-4220-8488-84C40C1A79D6.png\\\\ndeleted file mode 100644\\\\nindex d2170a7..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/Thao/851D4C65-07BE-4220-8488-84C40C1A79D6.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Thao/98E5A551-3DC2-4A85-9497-AFA4A845325A.png b/Dataset/FaceData/test/data/txl/Thao/98E5A551-3DC2-4A85-9497-AFA4A845325A.png\\\\ndeleted file mode 100644\\\\nindex 055d27a..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/Thao/98E5A551-3DC2-4A85-9497-AFA4A845325A.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173023.png b/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173023.png\\\\ndeleted file mode 100644\\\\nindex f95fdd9..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173023.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173025.png b/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173025.png\\\\ndeleted file mode 100644\\\\nindex f2c1813..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173025.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173026.png b/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173026.png\\\\ndeleted file mode 100644\\\\nindex 2541428..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173026.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173028.png b/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173028.png\\\\ndeleted file mode 100644\\\\nindex 1eae995..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173028.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.png b/Dataset/FaceData/test/data/txl/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.png\\\\ndeleted file mode 100644\\\\nindex 0678197..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TranDangHieu/download (1).png b/Dataset/FaceData/test/data/txl/TranDangHieu/download (1).png\\\\ndeleted file mode 100644\\\\nindex d7a2377..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/TranDangHieu/download (1).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TranDangHieu/download (2).png b/Dataset/FaceData/test/data/txl/TranDangHieu/download (2).png\\\\ndeleted file mode 100644\\\\nindex da1b935..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/TranDangHieu/download (2).png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TranDangHieu/download.png b/Dataset/FaceData/test/data/txl/TranDangHieu/download.png\\\\ndeleted file mode 100644\\\\nindex 51eee7f..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/TranDangHieu/download.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134208.png b/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134208.png\\\\ndeleted file mode 100644\\\\nindex a3ffc1f..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134208.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134210.png b/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134210.png\\\\ndeleted file mode 100644\\\\nindex 49694a3..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134210.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134211.png b/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134211.png\\\\ndeleted file mode 100644\\\\nindex 7af9479..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134211.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG1.png b/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG1.png\\\\ndeleted file mode 100644\\\\nindex 4e78809..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG1.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG2.png b/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG2.png\\\\ndeleted file mode 100644\\\\nindex 21b37f7..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG2.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png b/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png\\\\ndeleted file mode 100644\\\\nindex 971746e..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png b/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png\\\\ndeleted file mode 100644\\\\nindex 275fcb7..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png b/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png\\\\ndeleted file mode 100644\\\\nindex 29af910..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135142.png b/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135142.png\\\\ndeleted file mode 100644\\\\nindex 0149809..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135142.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135145.png b/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135145.png\\\\ndeleted file mode 100644\\\\nindex 2a6a9b4..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135145.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135147.png b/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135147.png\\\\ndeleted file mode 100644\\\\nindex 83a6450..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135147.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png b/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png\\\\ndeleted file mode 100644\\\\nindex 8257ab5..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png b/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png\\\\ndeleted file mode 100644\\\\nindex 45870ff..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png b/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png\\\\ndeleted file mode 100644\\\\nindex f4593ba..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.png b/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.png\\\\ndeleted file mode 100644\\\\nindex 942d65d..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.png b/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.png\\\\ndeleted file mode 100644\\\\nindex 6872f3e..0000000\\\\nBinary files a/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.png and /dev/null differ\\\\ndiff --git a/Dataset/FaceData/test/data/txl/bounding_boxes_47191.txt b/Dataset/FaceData/test/data/txl/bounding_boxes_47191.txt\\\\ndeleted file mode 100644\\\\nindex d314c07..0000000\\\\n--- a/Dataset/FaceData/test/data/txl/bounding_boxes_47191.txt\\\\n+++ /dev/null\\\\n@@ -1,55 +0,0 @@\\\\n-Dataset/FaceData/processed\\\\\\\\TrangNhung\\\\\\\\IMG_20250326_134210.png 895 349 1544 1272\\\\n-Dataset/FaceData/processed\\\\\\\\TrangNhung\\\\\\\\IMG_20250326_134208.png 535 336 1175 1157\\\\n-Dataset/FaceData/processed\\\\\\\\TrangNhung\\\\\\\\IMG_20250326_134211.png 586 360 1301 1293\\\\n-Dataset/FaceData/processed\\\\\\\\Xemesis\\\\\\\\Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.png 348 648 886 1345\\\\n-Dataset/FaceData/processed\\\\\\\\Xemesis\\\\\\\\Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.png 0 293 772 1203\\\\n-Dataset/FaceData/processed\\\\\\\\Emma\\\\\\\\IMG_20250326_151626.png 967 170 1656 1050\\\\n-Dataset/FaceData/processed\\\\\\\\Emma\\\\\\\\IMG_20250326_151731.png 362 264 1458 1704\\\\n-Dataset/FaceData/processed\\\\\\\\Emma\\\\\\\\IMG_20250326_151738.png 380 538 1544 1985\\\\n-Dataset/FaceData/processed\\\\\\\\Emma\\\\\\\\IMG_20250326_151720.png 417 326 1549 1712\\\\n-Dataset/FaceData/processed\\\\\\\\TrinhHuuTuan\\\\\\\\IMG1.png 199 545 810 1319\\\\n-Dataset/FaceData/processed\\\\\\\\TrinhHuuTuan\\\\\\\\IMG2.png 298 659 907 1438\\\\n-Dataset/FaceData/processed\\\\\\\\BotIu\\\\\\\\Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.png 589 739 1206 1280\\\\n-Dataset/FaceData/processed\\\\\\\\BotIu\\\\\\\\Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.png 338 362 954 1198\\\\n-Dataset/FaceData/processed\\\\\\\\BotIu\\\\\\\\Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.png 527 904 1096 1457\\\\n-Dataset/FaceData/processed\\\\\\\\TrinhTranPhuongTuan\\\\\\\\Screenshot 2025-03-26 152756.png 73 208 429 642\\\\n-Dataset/FaceData/processed\\\\\\\\TrinhTranPhuongTuan\\\\\\\\Screenshot 2025-03-26 152811.png 20 150 367 604\\\\n-Dataset/FaceData/processed\\\\\\\\TrinhTranPhuongTuan\\\\\\\\Screenshot 2025-03-26 152730.png 73 210 394 633\\\\n-Dataset/FaceData/processed\\\\\\\\Viruss\\\\\\\\Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png 154 108 1417 1799\\\\n-Dataset/FaceData/processed\\\\\\\\Viruss\\\\\\\\Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png 364 442 910 1148\\\\n-Dataset/FaceData/processed\\\\\\\\Viruss\\\\\\\\Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png 50 121 926 1264\\\\n-Dataset/FaceData/processed\\\\\\\\NgocKem\\\\\\\\IMG_1742972407559.png 304 247 820 976\\\\n-Dataset/FaceData/processed\\\\\\\\NgocKem\\\\\\\\IMG_1742972428266.png 300 233 870 981\\\\n-Dataset/FaceData/processed\\\\\\\\NgocKem\\\\\\\\IMG_1742972431298.png 180 168 796 998\\\\n-Dataset/FaceData/processed\\\\\\\\ThaoNguyen\\\\\\\\IMG_20250325_173023.png 389 530 1184 1500\\\\n-Dataset/FaceData/processed\\\\\\\\ThaoNguyen\\\\\\\\IMG_20250325_173028.png 490 449 1140 1231\\\\n-Dataset/FaceData/processed\\\\\\\\ThaoNguyen\\\\\\\\IMG_20250325_173025.png 513 544 1284 1399\\\\n-Dataset/FaceData/processed\\\\\\\\ThaoNguyen\\\\\\\\IMG_20250325_173026.png 489 328 1176 1142\\\\n-Dataset/FaceData/processed\\\\\\\\AnhTu\\\\\\\\Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.png 321 361 1163 1438\\\\n-Dataset/FaceData/processed\\\\\\\\AnhTu\\\\\\\\Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.png 217 635 1053 1802\\\\n-Dataset/FaceData/processed\\\\\\\\AnhTu\\\\\\\\Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.png 474 420 1348 1596\\\\n-Dataset/FaceData/processed\\\\\\\\Thao\\\\\\\\98E5A551-3DC2-4A85-9497-AFA4A845325A.png 287 854 841 1536\\\\n-Dataset/FaceData/processed\\\\\\\\Thao\\\\\\\\851D4C65-07BE-4220-8488-84C40C1A79D6.png 163 460 841 1386\\\\n-Dataset/FaceData/processed\\\\\\\\Phuong\\\\\\\\Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.png 383 266 1204 1442\\\\n-Dataset/FaceData/processed\\\\\\\\Phuong\\\\\\\\Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.png 398 511 855 1063\\\\n-Dataset/FaceData/processed\\\\\\\\Phuong\\\\\\\\Messenger_creation_7343AEA7-71DD-40DB-B8C3-9D6F0CED60F9.png\\\\n-Dataset/FaceData/processed\\\\\\\\ThienAn\\\\\\\\att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.png 357 494 1083 1463\\\\n-Dataset/FaceData/processed\\\\\\\\LeAnhTrang\\\\\\\\IMG_1742972296094.png 436 138 965 850\\\\n-Dataset/FaceData/processed\\\\\\\\LeAnhTrang\\\\\\\\IMG_20250326_134739.png 417 628 1522 2056\\\\n-Dataset/FaceData/processed\\\\\\\\Jack\\\\\\\\A3FED369-0E1F-4064-830E-FBFD69D77C85.png 111 709 831 1654\\\\n-Dataset/FaceData/processed\\\\\\\\Jack\\\\\\\\2209C349-3B30-47AC-89C1-1152E1A42FFC.png 224 606 982 1624\\\\n-Dataset/FaceData/processed\\\\\\\\NghiemAnhHieu\\\\\\\\IMG_20250327_092236.png 274 800 1031 1820\\\\n-Dataset/FaceData/processed\\\\\\\\NghiemAnhHieu\\\\\\\\IMG_20250327_092241.png 486 1016 840 1487\\\\n-Dataset/FaceData/processed\\\\\\\\NghiemAnhHieu\\\\\\\\IMG_0327.png 83 74 764 794\\\\n-Dataset/FaceData/processed\\\\\\\\NghiemAnhHieu\\\\\\\\IMG_20250327_092237.png 566 1129 1142 1953\\\\n-Dataset/FaceData/processed\\\\\\\\TranDangHieu\\\\\\\\download (2).png 152 762 696 1478\\\\n-Dataset/FaceData/processed\\\\\\\\TranDangHieu\\\\\\\\download (1).png 262 672 767 1357\\\\n-Dataset/FaceData/processed\\\\\\\\TranDangHieu\\\\\\\\download.png 202 705 750 1444\\\\n-Dataset/FaceData/processed\\\\\\\\BuiCongSon\\\\\\\\485424995_1386922852752466_129752981138586263_n.png 320 554 1058 1544\\\\n-Dataset/FaceData/processed\\\\\\\\BuiCongSon\\\\\\\\485512577_2113779945729669_160018376255303225_n.png 318 390 1184 1528\\\\n-Dataset/FaceData/processed\\\\\\\\BuiCongSon\\\\\\\\485464001_1219012669643907_2610181536763391909_n.png 461 308 1375 1574\\\\n-Dataset/FaceData/processed\\\\\\\\BuiCongSon\\\\\\\\z6377532752047_4b7b3c7d460239f92fa3cec037738eca.png 462 527 1017 1272\\\\n-Dataset/FaceData/processed\\\\\\\\BuiCongSon\\\\\\\\485290655_1069958524968534_6128511440722701069_n.png 376 472 1198 1582\\\\n-Dataset/FaceData/processed\\\\\\\\TuSenna\\\\\\\\IMG_20250326_135145.png 80 165 468 685\\\\n-Dataset/FaceData/processed\\\\\\\\TuSenna\\\\\\\\IMG_20250326_135147.png 225 199 579 683\\\\n-Dataset/FaceData/processed\\\\\\\\TuSenna\\\\\\\\IMG_20250326_135142.png 171 209 549 722\\\\ndiff --git a/Dataset/FaceData/test/data/txl/revision_info.txt b/Dataset/FaceData/test/data/txl/revision_info.txt\\\\ndeleted file mode 100644\\\\nindex 33dea13..0000000\\\\n--- a/Dataset/FaceData/test/data/txl/revision_info.txt\\\\n+++ /dev/null\\\\n@@ -1,7 +0,0 @@\\\\n-arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25\\\\n---------------------\\\\n-tensorflow version: 2.19.0\\\\n---------------------\\\\n-git hash: b\\\\\\\'69ff1e149c0d84a123d6516ddd82970e65392608\\\\\\\'\\\\n---------------------\\\\n-b\\\\\\\'\\\\\\\'\\\\n\\\\\\\\ No newline at end of file\\\\ndiff --git a/cmd.txt b/cmd.txt\\\\ndeleted file mode 100644\\\\nindex a3d455b..0000000\\\\n--- a/cmd.txt\\\\n+++ /dev/null\\\\n@@ -1,6 +0,0 @@\\\\n-python src/align_dataset_mtcnn.py  Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32  --random_order --gpu_memory_fraction 0.25\\\\n-\\\\n-\\\\n-python src/classifier.py TRAIN Dataset/FaceData/processed Models/20180402-114759.pb Models/facemodel.pkl --batch_size 1000\\\\n-\\\\n-python src/face_rec_cam.py \\\\n\\\\\\\\ No newline at end of file\\\\ndiff --git a/src/compare.py b/src/compare.py\\\\ndeleted file mode 100644\\\\nindex bc53cc4..0000000\\\\n--- a/src/compare.py\\\\n+++ /dev/null\\\\n@@ -1,130 +0,0 @@\\\\n-"""Performs face alignment and calculates L2 distance between the embeddings of images."""\\\\n-\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-from scipy import misc\\\\n-import tensorflow as tf\\\\n-import numpy as np\\\\n-import sys\\\\n-import os\\\\n-import copy\\\\n-import argparse\\\\n-import facenet\\\\n-import align.detect_face\\\\n-\\\\n-def main(args):\\\\n-\\\\n-    images = load_and_align_data(args.image_files, args.image_size, args.margin, args.gpu_memory_fraction)\\\\n-    with tf.Graph().as_default():\\\\n-\\\\n-        with tf.Session() as sess:\\\\n-      \\\\n-            # Load the model\\\\n-            facenet.load_model(args.model)\\\\n-    \\\\n-            # Get input and output tensors\\\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\n-\\\\n-            # Run forward pass to calculate embeddings\\\\n-            feed_dict = { images_placeholder: images, phase_train_placeholder:False }\\\\n-            emb = sess.run(embeddings, feed_dict=feed_dict)\\\\n-            \\\\n-            nrof_images = len(args.image_files)\\\\n-\\\\n-            print(\\\\\\\'Images:\\\\\\\')\\\\n-            for i in range(nrof_images):\\\\n-                print(\\\\\\\'%1d: %s\\\\\\\' % (i, args.image_files[i]))\\\\n-            print(\\\\\\\'\\\\\\\')\\\\n-            \\\\n-            # Print distance matrix\\\\n-            print(\\\\\\\'Distance matrix\\\\\\\')\\\\n-            print(\\\\\\\'    \\\\\\\', end=\\\\\\\'\\\\\\\')\\\\n-            for i in range(nrof_images):\\\\n-                print(\\\\\\\'    %1d     \\\\\\\' % i, end=\\\\\\\'\\\\\\\')\\\\n-            print(\\\\\\\'\\\\\\\')\\\\n-            for i in range(nrof_images):\\\\n-                print(\\\\\\\'%1d  \\\\\\\' % i, end=\\\\\\\'\\\\\\\')\\\\n-                for j in range(nrof_images):\\\\n-                    dist = np.sqrt(np.sum(np.square(np.subtract(emb[i,:], emb[j,:]))))\\\\n-                    print(\\\\\\\'  %1.4f  \\\\\\\' % dist, end=\\\\\\\'\\\\\\\')\\\\n-                print(\\\\\\\'\\\\\\\')\\\\n-            \\\\n-            \\\\n-def load_and_align_data(image_paths, image_size, margin, gpu_memory_fraction):\\\\n-\\\\n-    minsize = 20 # minimum size of face\\\\n-    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\\\\\\\'s threshold\\\\n-    factor = 0.709 # scale factor\\\\n-    \\\\n-    print(\\\\\\\'Creating networks and loading parameters\\\\\\\')\\\\n-    with tf.Graph().as_default():\\\\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\\\\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\n-        with sess.as_default():\\\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\\\\n-  \\\\n-    tmp_image_paths=copy.copy(image_paths)\\\\n-    img_list = []\\\\n-    for image in tmp_image_paths:\\\\n-        img = misc.imread(os.path.expanduser(image), mode=\\\\\\\'RGB\\\\\\\')\\\\n-        img_size = np.asarray(img.shape)[0:2]\\\\n-        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\\\\n-        if len(bounding_boxes) < 1:\\\\n-          image_paths.remove(image)\\\\n-          print("can\\\\\\\'t detect face, remove ", image)\\\\n-          continue\\\\n-        det = np.squeeze(bounding_boxes[0,0:4])\\\\n-        bb = np.zeros(4, dtype=np.int32)\\\\n-        bb[0] = np.maximum(det[0]-margin/2, 0)\\\\n-        bb[1] = np.maximum(det[1]-margin/2, 0)\\\\n-        bb[2] = np.minimum(det[2]+margin/2, img_size[1])\\\\n-        bb[3] = np.minimum(det[3]+margin/2, img_size[0])\\\\n-        cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\\\\n-        aligned = misc.imresize(cropped, (image_size, image_size), interp=\\\\\\\'bilinear\\\\\\\')\\\\n-        prewhitened = facenet.prewhiten(aligned)\\\\n-        img_list.append(prewhitened)\\\\n-    images = np.stack(img_list)\\\\n-    return images\\\\n-\\\\n-def parse_arguments(argv):\\\\n-    parser = argparse.ArgumentParser()\\\\n-    \\\\n-    parser.add_argument(\\\\\\\'model\\\\\\\', type=str, \\\\n-        help=\\\\\\\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'image_files\\\\\\\', type=str, nargs=\\\\\\\'+\\\\\\\', help=\\\\\\\'Images to compare\\\\\\\')\\\\n-    parser.add_argument(\\\\\\\'--image_size\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Image size (height, width) in pixels.\\\\\\\', default=160)\\\\n-    parser.add_argument(\\\\\\\'--margin\\\\\\\', type=int,\\\\n-        help=\\\\\\\'Margin for the crop around the bounding box (height, width) in pixels.\\\\\\\', default=44)\\\\n-    parser.add_argument(\\\\\\\'--gpu_memory_fraction\\\\\\\', type=float,\\\\n-        help=\\\\\\\'Upper bound on the amount of GPU memory that will be used by the process.\\\\\\\', default=1.0)\\\\n-    return parser.parse_args(argv)\\\\n-\\\\n-if __name__ == \\\\\\\'__main__\\\\\\\':\\\\n-    main(parse_arguments(sys.argv[1:]))\\\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\\\ndeleted file mode 100644\\\\nindex 455f67a..0000000\\\\n--- a/src/face_rec_cam.py\\\\n+++ /dev/null\\\\n@@ -1,135 +0,0 @@\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-import tensorflow as tf\\\\n-from imutils.video import VideoStream\\\\n-\\\\n-\\\\n-import argparse\\\\n-import facenet\\\\n-import imutils\\\\n-import os\\\\n-import sys\\\\n-import math\\\\n-import pickle\\\\n-import align.detect_face\\\\n-import numpy as np\\\\n-import cv2\\\\n-import collections\\\\n-from sklearn.svm import SVC\\\\n-\\\\n-\\\\n-def main():\\\\n-    parser = argparse.ArgumentParser()\\\\n-    parser.add_argument(\\\\\\\'--path\\\\\\\', help=\\\\\\\'Path of the video you want to test on.\\\\\\\', default=0)\\\\n-    args = parser.parse_args()\\\\n-\\\\n-    MINSIZE = 20\\\\n-    THRESHOLD = [0.6, 0.7, 0.7]\\\\n-    FACTOR = 0.709\\\\n-    IMAGE_SIZE = 182\\\\n-    INPUT_IMAGE_SIZE = 160\\\\n-    CLASSIFIER_PATH = \\\\\\\'Models/facemodel.pkl\\\\\\\'\\\\n-    VIDEO_PATH = args.path\\\\n-    FACENET_MODEL_PATH = \\\\\\\'Models/20180402-114759.pb\\\\\\\'\\\\n-\\\\n-    # Load The Custom Classifier\\\\n-    with open(CLASSIFIER_PATH, \\\\\\\'rb\\\\\\\') as file:\\\\n-        model, class_names = pickle.load(file)\\\\n-    print("Custom Classifier, Successfully loaded")\\\\n-\\\\n-    with tf.Graph().as_default():\\\\n-\\\\n-        # Cai dat GPU neu co\\\\n-        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\n-\\\\n-        with sess.as_default():\\\\n-\\\\n-            # Load the model\\\\n-            print(\\\\\\\'Loading feature extraction model\\\\\\\')\\\\n-            facenet.load_model(FACENET_MODEL_PATH)\\\\n-\\\\n-            # Get input and output tensors\\\\n-            graph = tf.compat.v1.get_default_graph()\\\\n-            images_placeholder = graph.get_tensor_by_name("input:0")\\\\n-            embeddings = graph.get_tensor_by_name("embeddings:0")\\\\n-            phase_train_placeholder = graph.get_tensor_by_name("phase_train:0")\\\\n-            embedding_size = embeddings.get_shape()[1]\\\\n-\\\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\n-\\\\n-            people_detected = set()\\\\n-            person_detected = collections.Counter()\\\\n-\\\\n-            cap  = VideoStream(src=0).start()\\\\n-\\\\n-            while (True):\\\\n-                frame = cap.read()\\\\n-                frame = imutils.resize(frame, width=600)\\\\n-                frame = cv2.flip(frame, 1)\\\\n-\\\\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\n-\\\\n-                faces_found = bounding_boxes.shape[0]\\\\n-                try:\\\\n-                    if faces_found > 1:\\\\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\n-                                    1, (255, 255, 255), thickness=1, lineType=2)\\\\n-                    elif faces_found > 0:\\\\n-                        det = bounding_boxes[:, 0:4]\\\\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\n-                        for i in range(faces_found):\\\\n-                            bb[i][0] = det[i][0]\\\\n-                            bb[i][1] = det[i][1]\\\\n-                            bb[i][2] = det[i][2]\\\\n-                            bb[i][3] = det[i][3]\\\\n-                            print(bb[i][3]-bb[i][1])\\\\n-                            print(frame.shape[0])\\\\n-                            print((bb[i][3]-bb[i][1])/frame.shape[0])\\\\n-                            if (bb[i][3]-bb[i][1])/frame.shape[0]>0.25:\\\\n-                                cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\n-                                                    interpolation=cv2.INTER_CUBIC)\\\\n-                                scaled = facenet.prewhiten(scaled)\\\\n-                                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\n-                                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\n-                                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\n-\\\\n-                                predictions = model.predict_proba(emb_array)\\\\n-                                best_class_indices = np.argmax(predictions, axis=1)\\\\n-                                best_class_probabilities = predictions[\\\\n-                                    np.arange(len(best_class_indices)), best_class_indices]\\\\n-                                best_name = class_names[best_class_indices[0]]\\\\n-                                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\n-\\\\n-\\\\n-\\\\n-                                if best_class_probabilities > 0.8:\\\\n-                                    cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\\\n-                                    text_x = bb[i][0]\\\\n-                                    text_y = bb[i][3] + 20\\\\n-\\\\n-                                    name = class_names[best_class_indices[0]]\\\\n-                                    cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\\\n-                                    cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\\\\n-                                                cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\\\n-                                    person_detected[best_name] += 1\\\\n-                                else:\\\\n-                                    name = "Unknown"\\\\n-\\\\n-                except:\\\\n-                    pass\\\\n-\\\\n-                cv2.imshow(\\\\\\\'Face Recognition\\\\\\\', frame)\\\\n-                if cv2.waitKey(1) & 0xFF == ord(\\\\\\\'q\\\\\\\'):\\\\n-                    break\\\\n-\\\\n-            cap.release()\\\\n-            cv2.destroyAllWindows()\\\\n-\\\\n-\\\\n-main()\\\\ndiff --git a/src/face_rec_flask.py b/src/face_rec_flask.py\\\\ndeleted file mode 100644\\\\nindex 3f7a27b..0000000\\\\n--- a/src/face_rec_flask.py\\\\n+++ /dev/null\\\\n@@ -1,118 +0,0 @@\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-from flask import Flask\\\\n-from flask import render_template , request\\\\n-from flask_cors import CORS, cross_origin\\\\n-import tensorflow as tf\\\\n-import argparse\\\\n-import facenet\\\\n-import os\\\\n-import sys\\\\n-import math\\\\n-import pickle\\\\n-import align.detect_face\\\\n-import numpy as np\\\\n-import cv2\\\\n-import collections\\\\n-from sklearn.svm import SVC\\\\n-import base64\\\\n-\\\\n-MINSIZE = 20\\\\n-THRESHOLD = [0.6, 0.7, 0.7]\\\\n-FACTOR = 0.709\\\\n-IMAGE_SIZE = 182\\\\n-INPUT_IMAGE_SIZE = 160\\\\n-CLASSIFIER_PATH = \\\\\\\'./Models/facemodel.pkl\\\\\\\'\\\\n-FACENET_MODEL_PATH = \\\\\\\'./Models/20180402-114759.pb\\\\\\\'\\\\n-\\\\n-# Load The Custom Classifier\\\\n-with open(CLASSIFIER_PATH, \\\\\\\'rb\\\\\\\') as file:\\\\n-    model, class_names = pickle.load(file)\\\\n-print("Custom Classifier, Successfully loaded")\\\\n-\\\\n-tf.Graph().as_default()\\\\n-\\\\n-# Cai dat GPU neu co\\\\n-gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\n-sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\n-\\\\n-\\\\n-# Load the model\\\\n-print(\\\\\\\'Loading feature extraction model\\\\\\\')\\\\n-facenet.load_model(FACENET_MODEL_PATH)\\\\n-\\\\n-# Get input and output tensors\\\\n-images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\n-embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\n-phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\n-embedding_size = embeddings.get_shape()[1]\\\\n-pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "align")\\\\n-\\\\n-\\\\n-\\\\n-app = Flask(__name__)\\\\n-CORS(app)\\\\n-\\\\n-\\\\n-\\\\n-@app.route(\\\\\\\'/\\\\\\\')\\\\n-@cross_origin()\\\\n-def index():\\\\n-    return "OK!";\\\\n-\\\\n-@app.route(\\\\\\\'/recog\\\\\\\', methods=[\\\\\\\'POST\\\\\\\'])\\\\n-@cross_origin()\\\\n-def upload_img_file():\\\\n-    if request.method == \\\\\\\'POST\\\\\\\':\\\\n-        # base 64\\\\n-        name="Unknown"\\\\n-        f = request.form.get(\\\\\\\'image\\\\\\\')\\\\n-        w = int(request.form.get(\\\\\\\'w\\\\\\\'))\\\\n-        h = int(request.form.get(\\\\\\\'h\\\\\\\'))\\\\n-\\\\n-        decoded_string = base64.b64decode(f)\\\\n-        frame = np.fromstring(decoded_string, dtype=np.uint8)\\\\n-        #frame = frame.reshape(w,h,3)\\\\n-        frame = cv2.imdecode(frame, cv2.IMREAD_ANYCOLOR)  # cv2.IMREAD_COLOR in OpenCV 3.1\\\\n-\\\\n-        bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\n-\\\\n-        faces_found = bounding_boxes.shape[0]\\\\n-\\\\n-        if faces_found > 0:\\\\n-            det = bounding_boxes[:, 0:4]\\\\n-            bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\n-            for i in range(faces_found):\\\\n-                bb[i][0] = det[i][0]\\\\n-                bb[i][1] = det[i][1]\\\\n-                bb[i][2] = det[i][2]\\\\n-                bb[i][3] = det[i][3]\\\\n-                cropped = frame\\\\n-                #cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\n-                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\n-                                    interpolation=cv2.INTER_CUBIC)\\\\n-                scaled = facenet.prewhiten(scaled)\\\\n-                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\n-                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\n-                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\n-                predictions = model.predict_proba(emb_array)\\\\n-                best_class_indices = np.argmax(predictions, axis=1)\\\\n-                best_class_probabilities = predictions[\\\\n-                    np.arange(len(best_class_indices)), best_class_indices]\\\\n-                best_name = class_names[best_class_indices[0]]\\\\n-                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\n-\\\\n-                if best_class_probabilities > 0.8:\\\\n-                    name = class_names[best_class_indices[0]]\\\\n-                else:\\\\n-                    name = "Unknown"\\\\n-\\\\n-\\\\n-        return name;\\\\n-\\\\n-\\\\n-if __name__ == \\\\\\\'__main__\\\\\\\':\\\\n-    app.run(debug=True, host=\\\\\\\'0.0.0.0\\\\\\\',port=\\\\\\\'8000\\\\\\\')\\\\n-\\\\ndiff --git a/src/models/__init__.py b/src/models/__init__.py\\\\ndeleted file mode 100644\\\\nindex efa6252..0000000\\\\n--- a/src/models/__init__.py\\\\n+++ /dev/null\\\\n@@ -1,2 +0,0 @@\\\\n-# flake8: noqa\\\\n-\\\\ndiff --git a/src/models/dummy.py b/src/models/dummy.py\\\\ndeleted file mode 100644\\\\nindex 7afe1ef..0000000\\\\n--- a/src/models/dummy.py\\\\n+++ /dev/null\\\\n@@ -1,54 +0,0 @@\\\\n-"""Dummy model used only for testing\\\\n-"""\\\\n-# MIT License\\\\n-# \\\\n-# Copyright (c) 2016 David Sandberg\\\\n-# \\\\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\\\n-# of this software and associated documentation files (the "Software"), to deal\\\\n-# in the Software without restriction, including without limitation the rights\\\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\\\n-# copies of the Software, and to permit persons to whom the Software is\\\\n-# furnished to do so, subject to the following conditions:\\\\n-# \\\\n-# The above copyright notice and this permission notice shall be included in all\\\\n-# copies or substantial portions of the Software.\\\\n-# \\\\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\\\n-# SOFTWARE.\\\\n-\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-import tensorflow as tf\\\\n-import tensorflow.contrib.slim as slim\\\\n-import numpy as np\\\\n-  \\\\n-def inference(images, keep_probability, phase_train=True,  # @UnusedVariable\\\\n-              bottleneck_layer_size=128, bottleneck_layer_activation=None, weight_decay=0.0, reuse=None):  # @UnusedVariable\\\\n-    batch_norm_params = {\\\\n-        # Decay for the moving averages.\\\\n-        \\\\\\\'decay\\\\\\\': 0.995,\\\\n-        # epsilon to prevent 0s in variance.\\\\n-        \\\\\\\'epsilon\\\\\\\': 0.001,\\\\n-        # force in-place updates of mean and variance estimates\\\\n-        \\\\\\\'updates_collections\\\\\\\': None,\\\\n-        # Moving averages ends up in the trainable variables collection\\\\n-        \\\\\\\'variables_collections\\\\\\\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\\\\n-    }\\\\n-    \\\\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\\\\n-                        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\\\\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\\\\n-                        normalizer_fn=slim.batch_norm,\\\\n-                        normalizer_params=batch_norm_params):\\\\n-        size = np.prod(images.get_shape()[1:].as_list())\\\\n-        net = slim.fully_connected(tf.reshape(images, (-1,size)), bottleneck_layer_size, activation_fn=None, \\\\n-                scope=\\\\\\\'Bottleneck\\\\\\\', reuse=False)\\\\n-        return net, None\\\\ndiff --git a/src/models/inception_resnet_v1.py b/src/models/inception_resnet_v1.py\\\\ndeleted file mode 100644\\\\nindex 475e81b..0000000\\\\n--- a/src/models/inception_resnet_v1.py\\\\n+++ /dev/null\\\\n@@ -1,246 +0,0 @@\\\\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\\\\n-#\\\\n-# Licensed under the Apache License, Version 2.0 (the "License");\\\\n-# you may not use this file except in compliance with the License.\\\\n-# You may obtain a copy of the License at\\\\n-#\\\\n-# http://www.apache.org/licenses/LICENSE-2.0\\\\n-#\\\\n-# Unless required by applicable law or agreed to in writing, software\\\\n-# distributed under the License is distributed on an "AS IS" BASIS,\\\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\\\n-# See the License for the specific language governing permissions and\\\\n-# limitations under the License.\\\\n-# ==============================================================================\\\\n-\\\\n-"""Contains the definition of the Inception Resnet V1 architecture.\\\\n-As described in http://arxiv.org/abs/1602.07261.\\\\n-  Inception-v4, Inception-ResNet and the Impact of Residual Connections\\\\n-    on Learning\\\\n-  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\\\\n-"""\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-import tensorflow as tf\\\\n-import tensorflow.contrib.slim as slim\\\\n-\\\\n-# Inception-Resnet-A\\\\n-def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\\\n-    """Builds the 35x35 resnet block."""\\\\n-    with tf.variable_scope(scope, \\\\\\\'Block35\\\\\\\', [net], reuse=reuse):\\\\n-        with tf.variable_scope(\\\\\\\'Branch_0\\\\\\\'):\\\\n-            tower_conv = slim.conv2d(net, 32, 1, scope=\\\\\\\'Conv2d_1x1\\\\\\\')\\\\n-        with tf.variable_scope(\\\\\\\'Branch_1\\\\\\\'):\\\\n-            tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\\\\\\\'Conv2d_0b_3x3\\\\\\\')\\\\n-        with tf.variable_scope(\\\\\\\'Branch_2\\\\\\\'):\\\\n-            tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-            tower_conv2_1 = slim.conv2d(tower_conv2_0, 32, 3, scope=\\\\\\\'Conv2d_0b_3x3\\\\\\\')\\\\n-            tower_conv2_2 = slim.conv2d(tower_conv2_1, 32, 3, scope=\\\\\\\'Conv2d_0c_3x3\\\\\\\')\\\\n-        mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\\\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\\\n-                         activation_fn=None, scope=\\\\\\\'Conv2d_1x1\\\\\\\')\\\\n-        net += scale * up\\\\n-        if activation_fn:\\\\n-            net = activation_fn(net)\\\\n-    return net\\\\n-\\\\n-# Inception-Resnet-B\\\\n-def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\\\n-    """Builds the 17x17 resnet block."""\\\\n-    with tf.variable_scope(scope, \\\\\\\'Block17\\\\\\\', [net], reuse=reuse):\\\\n-        with tf.variable_scope(\\\\\\\'Branch_0\\\\\\\'):\\\\n-            tower_conv = slim.conv2d(net, 128, 1, scope=\\\\\\\'Conv2d_1x1\\\\\\\')\\\\n-        with tf.variable_scope(\\\\\\\'Branch_1\\\\\\\'):\\\\n-            tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 128, [1, 7],\\\\n-                                        scope=\\\\\\\'Conv2d_0b_1x7\\\\\\\')\\\\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 128, [7, 1],\\\\n-                                        scope=\\\\\\\'Conv2d_0c_7x1\\\\\\\')\\\\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\\\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\\\n-                         activation_fn=None, scope=\\\\\\\'Conv2d_1x1\\\\\\\')\\\\n-        net += scale * up\\\\n-        if activation_fn:\\\\n-            net = activation_fn(net)\\\\n-    return net\\\\n-\\\\n-\\\\n-# Inception-Resnet-C\\\\n-def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\\\n-    """Builds the 8x8 resnet block."""\\\\n-    with tf.variable_scope(scope, \\\\\\\'Block8\\\\\\\', [net], reuse=reuse):\\\\n-        with tf.variable_scope(\\\\\\\'Branch_0\\\\\\\'):\\\\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\\\\\\\'Conv2d_1x1\\\\\\\')\\\\n-        with tf.variable_scope(\\\\\\\'Branch_1\\\\\\\'):\\\\n-            tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 192, [1, 3],\\\\n-                                        scope=\\\\\\\'Conv2d_0b_1x3\\\\\\\')\\\\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [3, 1],\\\\n-                                        scope=\\\\\\\'Conv2d_0c_3x1\\\\\\\')\\\\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\\\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\\\n-                         activation_fn=None, scope=\\\\\\\'Conv2d_1x1\\\\\\\')\\\\n-        net += scale * up\\\\n-        if activation_fn:\\\\n-            net = activation_fn(net)\\\\n-    return net\\\\n-  \\\\n-def reduction_a(net, k, l, m, n):\\\\n-    with tf.variable_scope(\\\\\\\'Branch_0\\\\\\\'):\\\\n-        tower_conv = slim.conv2d(net, n, 3, stride=2, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                 scope=\\\\\\\'Conv2d_1a_3x3\\\\\\\')\\\\n-    with tf.variable_scope(\\\\\\\'Branch_1\\\\\\\'):\\\\n-        tower_conv1_0 = slim.conv2d(net, k, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-        tower_conv1_1 = slim.conv2d(tower_conv1_0, l, 3,\\\\n-                                    scope=\\\\\\\'Conv2d_0b_3x3\\\\\\\')\\\\n-        tower_conv1_2 = slim.conv2d(tower_conv1_1, m, 3,\\\\n-                                    stride=2, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                    scope=\\\\\\\'Conv2d_1a_3x3\\\\\\\')\\\\n-    with tf.variable_scope(\\\\\\\'Branch_2\\\\\\\'):\\\\n-        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                     scope=\\\\\\\'MaxPool_1a_3x3\\\\\\\')\\\\n-    net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\\\\n-    return net\\\\n-\\\\n-def reduction_b(net):\\\\n-    with tf.variable_scope(\\\\\\\'Branch_0\\\\\\\'):\\\\n-        tower_conv = slim.conv2d(net, 256, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\\\\n-                                   padding=\\\\\\\'VALID\\\\\\\', scope=\\\\\\\'Conv2d_1a_3x3\\\\\\\')\\\\n-    with tf.variable_scope(\\\\\\\'Branch_1\\\\\\\'):\\\\n-        tower_conv1 = slim.conv2d(net, 256, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-        tower_conv1_1 = slim.conv2d(tower_conv1, 256, 3, stride=2,\\\\n-                                    padding=\\\\\\\'VALID\\\\\\\', scope=\\\\\\\'Conv2d_1a_3x3\\\\\\\')\\\\n-    with tf.variable_scope(\\\\\\\'Branch_2\\\\\\\'):\\\\n-        tower_conv2 = slim.conv2d(net, 256, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-        tower_conv2_1 = slim.conv2d(tower_conv2, 256, 3,\\\\n-                                    scope=\\\\\\\'Conv2d_0b_3x3\\\\\\\')\\\\n-        tower_conv2_2 = slim.conv2d(tower_conv2_1, 256, 3, stride=2,\\\\n-                                    padding=\\\\\\\'VALID\\\\\\\', scope=\\\\\\\'Conv2d_1a_3x3\\\\\\\')\\\\n-    with tf.variable_scope(\\\\\\\'Branch_3\\\\\\\'):\\\\n-        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                     scope=\\\\\\\'MaxPool_1a_3x3\\\\\\\')\\\\n-    net = tf.concat([tower_conv_1, tower_conv1_1,\\\\n-                        tower_conv2_2, tower_pool], 3)\\\\n-    return net\\\\n-  \\\\n-def inference(images, keep_probability, phase_train=True, \\\\n-              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\\\\n-    batch_norm_params = {\\\\n-        # Decay for the moving averages.\\\\n-        \\\\\\\'decay\\\\\\\': 0.995,\\\\n-        # epsilon to prevent 0s in variance.\\\\n-        \\\\\\\'epsilon\\\\\\\': 0.001,\\\\n-        # force in-place updates of mean and variance estimates\\\\n-        \\\\\\\'updates_collections\\\\\\\': None,\\\\n-        # Moving averages ends up in the trainable variables collection\\\\n-        \\\\\\\'variables_collections\\\\\\\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\\\\n-    }\\\\n-    \\\\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\\\\n-                        weights_initializer=slim.initializers.xavier_initializer(), \\\\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\\\\n-                        normalizer_fn=slim.batch_norm,\\\\n-                        normalizer_params=batch_norm_params):\\\\n-        return inception_resnet_v1(images, is_training=phase_train,\\\\n-              dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size, reuse=reuse)\\\\n-\\\\n-\\\\n-def inception_resnet_v1(inputs, is_training=True,\\\\n-                        dropout_keep_prob=0.8,\\\\n-                        bottleneck_layer_size=128,\\\\n-                        reuse=None, \\\\n-                        scope=\\\\\\\'InceptionResnetV1\\\\\\\'):\\\\n-    """Creates the Inception Resnet V1 model.\\\\n-    Args:\\\\n-      inputs: a 4-D tensor of size [batch_size, height, width, 3].\\\\n-      num_classes: number of predicted classes.\\\\n-      is_training: whether is training or not.\\\\n-      dropout_keep_prob: float, the fraction to keep before final layer.\\\\n-      reuse: whether or not the network and its variables should be reused. To be\\\\n-        able to reuse \\\\\\\'scope\\\\\\\' must be given.\\\\n-      scope: Optional variable_scope.\\\\n-    Returns:\\\\n-      logits: the logits outputs of the model.\\\\n-      end_points: the set of end_points from the inception model.\\\\n-    """\\\\n-    end_points = {}\\\\n-  \\\\n-    with tf.variable_scope(scope, \\\\\\\'InceptionResnetV1\\\\\\\', [inputs], reuse=reuse):\\\\n-        with slim.arg_scope([slim.batch_norm, slim.dropout],\\\\n-                            is_training=is_training):\\\\n-            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\\\\n-                                stride=1, padding=\\\\\\\'SAME\\\\\\\'):\\\\n-      \\\\n-                # 149 x 149 x 32\\\\n-                net = slim.conv2d(inputs, 32, 3, stride=2, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                  scope=\\\\\\\'Conv2d_1a_3x3\\\\\\\')\\\\n-                end_points[\\\\\\\'Conv2d_1a_3x3\\\\\\\'] = net\\\\n-                # 147 x 147 x 32\\\\n-                net = slim.conv2d(net, 32, 3, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                  scope=\\\\\\\'Conv2d_2a_3x3\\\\\\\')\\\\n-                end_points[\\\\\\\'Conv2d_2a_3x3\\\\\\\'] = net\\\\n-                # 147 x 147 x 64\\\\n-                net = slim.conv2d(net, 64, 3, scope=\\\\\\\'Conv2d_2b_3x3\\\\\\\')\\\\n-                end_points[\\\\\\\'Conv2d_2b_3x3\\\\\\\'] = net\\\\n-                # 73 x 73 x 64\\\\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                      scope=\\\\\\\'MaxPool_3a_3x3\\\\\\\')\\\\n-                end_points[\\\\\\\'MaxPool_3a_3x3\\\\\\\'] = net\\\\n-                # 73 x 73 x 80\\\\n-                net = slim.conv2d(net, 80, 1, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                  scope=\\\\\\\'Conv2d_3b_1x1\\\\\\\')\\\\n-                end_points[\\\\\\\'Conv2d_3b_1x1\\\\\\\'] = net\\\\n-                # 71 x 71 x 192\\\\n-                net = slim.conv2d(net, 192, 3, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                  scope=\\\\\\\'Conv2d_4a_3x3\\\\\\\')\\\\n-                end_points[\\\\\\\'Conv2d_4a_3x3\\\\\\\'] = net\\\\n-                # 35 x 35 x 256\\\\n-                net = slim.conv2d(net, 256, 3, stride=2, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                  scope=\\\\\\\'Conv2d_4b_3x3\\\\\\\')\\\\n-                end_points[\\\\\\\'Conv2d_4b_3x3\\\\\\\'] = net\\\\n-                \\\\n-                # 5 x Inception-resnet-A\\\\n-                net = slim.repeat(net, 5, block35, scale=0.17)\\\\n-                end_points[\\\\\\\'Mixed_5a\\\\\\\'] = net\\\\n-        \\\\n-                # Reduction-A\\\\n-                with tf.variable_scope(\\\\\\\'Mixed_6a\\\\\\\'):\\\\n-                    net = reduction_a(net, 192, 192, 256, 384)\\\\n-                end_points[\\\\\\\'Mixed_6a\\\\\\\'] = net\\\\n-                \\\\n-                # 10 x Inception-Resnet-B\\\\n-                net = slim.repeat(net, 10, block17, scale=0.10)\\\\n-                end_points[\\\\\\\'Mixed_6b\\\\\\\'] = net\\\\n-                \\\\n-                # Reduction-B\\\\n-                with tf.variable_scope(\\\\\\\'Mixed_7a\\\\\\\'):\\\\n-                    net = reduction_b(net)\\\\n-                end_points[\\\\\\\'Mixed_7a\\\\\\\'] = net\\\\n-                \\\\n-                # 5 x Inception-Resnet-C\\\\n-                net = slim.repeat(net, 5, block8, scale=0.20)\\\\n-                end_points[\\\\\\\'Mixed_8a\\\\\\\'] = net\\\\n-                \\\\n-                net = block8(net, activation_fn=None)\\\\n-                end_points[\\\\\\\'Mixed_8b\\\\\\\'] = net\\\\n-                \\\\n-                with tf.variable_scope(\\\\\\\'Logits\\\\\\\'):\\\\n-                    end_points[\\\\\\\'PrePool\\\\\\\'] = net\\\\n-                    #pylint: disable=no-member\\\\n-                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                          scope=\\\\\\\'AvgPool_1a_8x8\\\\\\\')\\\\n-                    net = slim.flatten(net)\\\\n-          \\\\n-                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\\\\n-                                       scope=\\\\\\\'Dropout\\\\\\\')\\\\n-          \\\\n-                    end_points[\\\\\\\'PreLogitsFlatten\\\\\\\'] = net\\\\n-                \\\\n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \\\\n-                        scope=\\\\\\\'Bottleneck\\\\\\\', reuse=False)\\\\n-  \\\\n-    return net, end_points\\\\ndiff --git a/src/models/inception_resnet_v2.py b/src/models/inception_resnet_v2.py\\\\ndeleted file mode 100644\\\\nindex 0fb176f..0000000\\\\n--- a/src/models/inception_resnet_v2.py\\\\n+++ /dev/null\\\\n@@ -1,255 +0,0 @@\\\\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\\\\n-#\\\\n-# Licensed under the Apache License, Version 2.0 (the "License");\\\\n-# you may not use this file except in compliance with the License.\\\\n-# You may obtain a copy of the License at\\\\n-#\\\\n-# http://www.apache.org/licenses/LICENSE-2.0\\\\n-#\\\\n-# Unless required by applicable law or agreed to in writing, software\\\\n-# distributed under the License is distributed on an "AS IS" BASIS,\\\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\\\n-# See the License for the specific language governing permissions and\\\\n-# limitations under the License.\\\\n-# ==============================================================================\\\\n-\\\\n-"""Contains the definition of the Inception Resnet V2 architecture.\\\\n-As described in http://arxiv.org/abs/1602.07261.\\\\n-  Inception-v4, Inception-ResNet and the Impact of Residual Connections\\\\n-    on Learning\\\\n-  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\\\\n-"""\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-import tensorflow as tf\\\\n-import tensorflow.contrib.slim as slim\\\\n-\\\\n-# Inception-Resnet-A\\\\n-def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\\\n-    """Builds the 35x35 resnet block."""\\\\n-    with tf.variable_scope(scope, \\\\\\\'Block35\\\\\\\', [net], reuse=reuse):\\\\n-        with tf.variable_scope(\\\\\\\'Branch_0\\\\\\\'):\\\\n-            tower_conv = slim.conv2d(net, 32, 1, scope=\\\\\\\'Conv2d_1x1\\\\\\\')\\\\n-        with tf.variable_scope(\\\\\\\'Branch_1\\\\\\\'):\\\\n-            tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\\\\\\\'Conv2d_0b_3x3\\\\\\\')\\\\n-        with tf.variable_scope(\\\\\\\'Branch_2\\\\\\\'):\\\\n-            tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-            tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\\\\\\\'Conv2d_0b_3x3\\\\\\\')\\\\n-            tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\\\\\\\'Conv2d_0c_3x3\\\\\\\')\\\\n-        mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\\\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\\\n-                         activation_fn=None, scope=\\\\\\\'Conv2d_1x1\\\\\\\')\\\\n-        net += scale * up\\\\n-        if activation_fn:\\\\n-            net = activation_fn(net)\\\\n-    return net\\\\n-\\\\n-# Inception-Resnet-B\\\\n-def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\\\n-    """Builds the 17x17 resnet block."""\\\\n-    with tf.variable_scope(scope, \\\\\\\'Block17\\\\\\\', [net], reuse=reuse):\\\\n-        with tf.variable_scope(\\\\\\\'Branch_0\\\\\\\'):\\\\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\\\\\\\'Conv2d_1x1\\\\\\\')\\\\n-        with tf.variable_scope(\\\\\\\'Branch_1\\\\\\\'):\\\\n-            tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\\\\n-                                        scope=\\\\\\\'Conv2d_0b_1x7\\\\\\\')\\\\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\\\\n-                                        scope=\\\\\\\'Conv2d_0c_7x1\\\\\\\')\\\\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\\\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\\\n-                         activation_fn=None, scope=\\\\\\\'Conv2d_1x1\\\\\\\')\\\\n-        net += scale * up\\\\n-        if activation_fn:\\\\n-            net = activation_fn(net)\\\\n-    return net\\\\n-\\\\n-\\\\n-# Inception-Resnet-C\\\\n-def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\\\n-    """Builds the 8x8 resnet block."""\\\\n-    with tf.variable_scope(scope, \\\\\\\'Block8\\\\\\\', [net], reuse=reuse):\\\\n-        with tf.variable_scope(\\\\\\\'Branch_0\\\\\\\'):\\\\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\\\\\\\'Conv2d_1x1\\\\\\\')\\\\n-        with tf.variable_scope(\\\\\\\'Branch_1\\\\\\\'):\\\\n-            tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\\\\n-                                        scope=\\\\\\\'Conv2d_0b_1x3\\\\\\\')\\\\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\\\\n-                                        scope=\\\\\\\'Conv2d_0c_3x1\\\\\\\')\\\\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\\\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\\\n-                         activation_fn=None, scope=\\\\\\\'Conv2d_1x1\\\\\\\')\\\\n-        net += scale * up\\\\n-        if activation_fn:\\\\n-            net = activation_fn(net)\\\\n-    return net\\\\n-  \\\\n-def inference(images, keep_probability, phase_train=True, \\\\n-              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\\\\n-    batch_norm_params = {\\\\n-        # Decay for the moving averages.\\\\n-        \\\\\\\'decay\\\\\\\': 0.995,\\\\n-        # epsilon to prevent 0s in variance.\\\\n-        \\\\\\\'epsilon\\\\\\\': 0.001,\\\\n-        # force in-place updates of mean and variance estimates\\\\n-        \\\\\\\'updates_collections\\\\\\\': None,\\\\n-        # Moving averages ends up in the trainable variables collection\\\\n-        \\\\\\\'variables_collections\\\\\\\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\\\\n-}\\\\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\\\\n-                        weights_initializer=slim.initializers.xavier_initializer(), \\\\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\\\\n-                        normalizer_fn=slim.batch_norm,\\\\n-                        normalizer_params=batch_norm_params):\\\\n-        return inception_resnet_v2(images, is_training=phase_train,\\\\n-              dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size, reuse=reuse)\\\\n-\\\\n-\\\\n-def inception_resnet_v2(inputs, is_training=True,\\\\n-                        dropout_keep_prob=0.8,\\\\n-                        bottleneck_layer_size=128,\\\\n-                        reuse=None,\\\\n-                        scope=\\\\\\\'InceptionResnetV2\\\\\\\'):\\\\n-    """Creates the Inception Resnet V2 model.\\\\n-    Args:\\\\n-      inputs: a 4-D tensor of size [batch_size, height, width, 3].\\\\n-      num_classes: number of predicted classes.\\\\n-      is_training: whether is training or not.\\\\n-      dropout_keep_prob: float, the fraction to keep before final layer.\\\\n-      reuse: whether or not the network and its variables should be reused. To be\\\\n-        able to reuse \\\\\\\'scope\\\\\\\' must be given.\\\\n-      scope: Optional variable_scope.\\\\n-    Returns:\\\\n-      logits: the logits outputs of the model.\\\\n-      end_points: the set of end_points from the inception model.\\\\n-    """\\\\n-    end_points = {}\\\\n-  \\\\n-    with tf.variable_scope(scope, \\\\\\\'InceptionResnetV2\\\\\\\', [inputs], reuse=reuse):\\\\n-        with slim.arg_scope([slim.batch_norm, slim.dropout],\\\\n-                            is_training=is_training):\\\\n-            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\\\\n-                                stride=1, padding=\\\\\\\'SAME\\\\\\\'):\\\\n-      \\\\n-                # 149 x 149 x 32\\\\n-                net = slim.conv2d(inputs, 32, 3, stride=2, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                  scope=\\\\\\\'Conv2d_1a_3x3\\\\\\\')\\\\n-                end_points[\\\\\\\'Conv2d_1a_3x3\\\\\\\'] = net\\\\n-                # 147 x 147 x 32\\\\n-                net = slim.conv2d(net, 32, 3, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                  scope=\\\\\\\'Conv2d_2a_3x3\\\\\\\')\\\\n-                end_points[\\\\\\\'Conv2d_2a_3x3\\\\\\\'] = net\\\\n-                # 147 x 147 x 64\\\\n-                net = slim.conv2d(net, 64, 3, scope=\\\\\\\'Conv2d_2b_3x3\\\\\\\')\\\\n-                end_points[\\\\\\\'Conv2d_2b_3x3\\\\\\\'] = net\\\\n-                # 73 x 73 x 64\\\\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                      scope=\\\\\\\'MaxPool_3a_3x3\\\\\\\')\\\\n-                end_points[\\\\\\\'MaxPool_3a_3x3\\\\\\\'] = net\\\\n-                # 73 x 73 x 80\\\\n-                net = slim.conv2d(net, 80, 1, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                  scope=\\\\\\\'Conv2d_3b_1x1\\\\\\\')\\\\n-                end_points[\\\\\\\'Conv2d_3b_1x1\\\\\\\'] = net\\\\n-                # 71 x 71 x 192\\\\n-                net = slim.conv2d(net, 192, 3, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                  scope=\\\\\\\'Conv2d_4a_3x3\\\\\\\')\\\\n-                end_points[\\\\\\\'Conv2d_4a_3x3\\\\\\\'] = net\\\\n-                # 35 x 35 x 192\\\\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                      scope=\\\\\\\'MaxPool_5a_3x3\\\\\\\')\\\\n-                end_points[\\\\\\\'MaxPool_5a_3x3\\\\\\\'] = net\\\\n-        \\\\n-                # 35 x 35 x 320\\\\n-                with tf.variable_scope(\\\\\\\'Mixed_5b\\\\\\\'):\\\\n-                    with tf.variable_scope(\\\\\\\'Branch_0\\\\\\\'):\\\\n-                        tower_conv = slim.conv2d(net, 96, 1, scope=\\\\\\\'Conv2d_1x1\\\\\\\')\\\\n-                    with tf.variable_scope(\\\\\\\'Branch_1\\\\\\\'):\\\\n-                        tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-                        tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\\\\n-                                                    scope=\\\\\\\'Conv2d_0b_5x5\\\\\\\')\\\\n-                    with tf.variable_scope(\\\\\\\'Branch_2\\\\\\\'):\\\\n-                        tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-                        tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\\\\n-                                                    scope=\\\\\\\'Conv2d_0b_3x3\\\\\\\')\\\\n-                        tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\\\\n-                                                    scope=\\\\\\\'Conv2d_0c_3x3\\\\\\\')\\\\n-                    with tf.variable_scope(\\\\\\\'Branch_3\\\\\\\'):\\\\n-                        tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\\\\\\\'SAME\\\\\\\',\\\\n-                                                     scope=\\\\\\\'AvgPool_0a_3x3\\\\\\\')\\\\n-                        tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\\\\n-                                                   scope=\\\\\\\'Conv2d_0b_1x1\\\\\\\')\\\\n-                    net = tf.concat([tower_conv, tower_conv1_1,\\\\n-                                        tower_conv2_2, tower_pool_1], 3)\\\\n-        \\\\n-                end_points[\\\\\\\'Mixed_5b\\\\\\\'] = net\\\\n-                net = slim.repeat(net, 10, block35, scale=0.17)\\\\n-        \\\\n-                # 17 x 17 x 1024\\\\n-                with tf.variable_scope(\\\\\\\'Mixed_6a\\\\\\\'):\\\\n-                    with tf.variable_scope(\\\\\\\'Branch_0\\\\\\\'):\\\\n-                        tower_conv = slim.conv2d(net, 384, 3, stride=2, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                                 scope=\\\\\\\'Conv2d_1a_3x3\\\\\\\')\\\\n-                    with tf.variable_scope(\\\\\\\'Branch_1\\\\\\\'):\\\\n-                        tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-                        tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\\\\n-                                                    scope=\\\\\\\'Conv2d_0b_3x3\\\\\\\')\\\\n-                        tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\\\\n-                                                    stride=2, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                                    scope=\\\\\\\'Conv2d_1a_3x3\\\\\\\')\\\\n-                    with tf.variable_scope(\\\\\\\'Branch_2\\\\\\\'):\\\\n-                        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                                     scope=\\\\\\\'MaxPool_1a_3x3\\\\\\\')\\\\n-                    net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\\\\n-        \\\\n-                end_points[\\\\\\\'Mixed_6a\\\\\\\'] = net\\\\n-                net = slim.repeat(net, 20, block17, scale=0.10)\\\\n-        \\\\n-                with tf.variable_scope(\\\\\\\'Mixed_7a\\\\\\\'):\\\\n-                    with tf.variable_scope(\\\\\\\'Branch_0\\\\\\\'):\\\\n-                        tower_conv = slim.conv2d(net, 256, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-                        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\\\\n-                                                   padding=\\\\\\\'VALID\\\\\\\', scope=\\\\\\\'Conv2d_1a_3x3\\\\\\\')\\\\n-                    with tf.variable_scope(\\\\\\\'Branch_1\\\\\\\'):\\\\n-                        tower_conv1 = slim.conv2d(net, 256, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-                        tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\\\\n-                                                    padding=\\\\\\\'VALID\\\\\\\', scope=\\\\\\\'Conv2d_1a_3x3\\\\\\\')\\\\n-                    with tf.variable_scope(\\\\\\\'Branch_2\\\\\\\'):\\\\n-                        tower_conv2 = slim.conv2d(net, 256, 1, scope=\\\\\\\'Conv2d_0a_1x1\\\\\\\')\\\\n-                        tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\\\\n-                                                    scope=\\\\\\\'Conv2d_0b_3x3\\\\\\\')\\\\n-                        tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\\\\n-                                                    padding=\\\\\\\'VALID\\\\\\\', scope=\\\\\\\'Conv2d_1a_3x3\\\\\\\')\\\\n-                    with tf.variable_scope(\\\\\\\'Branch_3\\\\\\\'):\\\\n-                        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                                     scope=\\\\\\\'MaxPool_1a_3x3\\\\\\\')\\\\n-                    net = tf.concat([tower_conv_1, tower_conv1_1,\\\\n-                                        tower_conv2_2, tower_pool], 3)\\\\n-        \\\\n-                end_points[\\\\\\\'Mixed_7a\\\\\\\'] = net\\\\n-        \\\\n-                net = slim.repeat(net, 9, block8, scale=0.20)\\\\n-                net = block8(net, activation_fn=None)\\\\n-        \\\\n-                net = slim.conv2d(net, 1536, 1, scope=\\\\\\\'Conv2d_7b_1x1\\\\\\\')\\\\n-                end_points[\\\\\\\'Conv2d_7b_1x1\\\\\\\'] = net\\\\n-        \\\\n-                with tf.variable_scope(\\\\\\\'Logits\\\\\\\'):\\\\n-                    end_points[\\\\\\\'PrePool\\\\\\\'] = net\\\\n-                    #pylint: disable=no-member\\\\n-                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\\\\\\\'VALID\\\\\\\',\\\\n-                                          scope=\\\\\\\'AvgPool_1a_8x8\\\\\\\')\\\\n-                    net = slim.flatten(net)\\\\n-          \\\\n-                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\\\\n-                                       scope=\\\\\\\'Dropout\\\\\\\')\\\\n-          \\\\n-                    end_points[\\\\\\\'PreLogitsFlatten\\\\\\\'] = net\\\\n-                \\\\n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \\\\n-                        scope=\\\\\\\'Bottleneck\\\\\\\', reuse=False)\\\\n-  \\\\n-    return net, end_points\\\\ndiff --git a/src/models/squeezenet.py b/src/models/squeezenet.py\\\\ndeleted file mode 100644\\\\nindex ae117e1..0000000\\\\n--- a/src/models/squeezenet.py\\\\n+++ /dev/null\\\\n@@ -1,67 +0,0 @@\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-import tensorflow as tf\\\\n-import tensorflow.contrib.slim as slim\\\\n-\\\\n-def fire_module(inputs,\\\\n-                squeeze_depth,\\\\n-                expand_depth,\\\\n-                reuse=None,\\\\n-                scope=None,\\\\n-                outputs_collections=None):\\\\n-    with tf.variable_scope(scope, \\\\\\\'fire\\\\\\\', [inputs], reuse=reuse):\\\\n-        with slim.arg_scope([slim.conv2d, slim.max_pool2d],\\\\n-                            outputs_collections=None):\\\\n-            net = squeeze(inputs, squeeze_depth)\\\\n-            outputs = expand(net, expand_depth)\\\\n-            return outputs\\\\n-\\\\n-def squeeze(inputs, num_outputs):\\\\n-    return slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope=\\\\\\\'squeeze\\\\\\\')\\\\n-\\\\n-def expand(inputs, num_outputs):\\\\n-    with tf.variable_scope(\\\\\\\'expand\\\\\\\'):\\\\n-        e1x1 = slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope=\\\\\\\'1x1\\\\\\\')\\\\n-        e3x3 = slim.conv2d(inputs, num_outputs, [3, 3], scope=\\\\\\\'3x3\\\\\\\')\\\\n-    return tf.concat([e1x1, e3x3], 3)\\\\n-\\\\n-def inference(images, keep_probability, phase_train=True, bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\\\\n-    batch_norm_params = {\\\\n-        # Decay for the moving averages.\\\\n-        \\\\\\\'decay\\\\\\\': 0.995,\\\\n-        # epsilon to prevent 0s in variance.\\\\n-        \\\\\\\'epsilon\\\\\\\': 0.001,\\\\n-        # force in-place updates of mean and variance estimates\\\\n-        \\\\\\\'updates_collections\\\\\\\': None,\\\\n-        # Moving averages ends up in the trainable variables collection\\\\n-        \\\\\\\'variables_collections\\\\\\\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\\\\n-    }\\\\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\\\\n-                        weights_initializer=slim.xavier_initializer_conv2d(uniform=True),\\\\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\\\\n-                        normalizer_fn=slim.batch_norm,\\\\n-                        normalizer_params=batch_norm_params):\\\\n-        with tf.variable_scope(\\\\\\\'squeezenet\\\\\\\', [images], reuse=reuse):\\\\n-            with slim.arg_scope([slim.batch_norm, slim.dropout],\\\\n-                                is_training=phase_train):\\\\n-                net = slim.conv2d(images, 96, [7, 7], stride=2, scope=\\\\\\\'conv1\\\\\\\')\\\\n-                net = slim.max_pool2d(net, [3, 3], stride=2, scope=\\\\\\\'maxpool1\\\\\\\')\\\\n-                net = fire_module(net, 16, 64, scope=\\\\\\\'fire2\\\\\\\')\\\\n-                net = fire_module(net, 16, 64, scope=\\\\\\\'fire3\\\\\\\')\\\\n-                net = fire_module(net, 32, 128, scope=\\\\\\\'fire4\\\\\\\')\\\\n-                net = slim.max_pool2d(net, [2, 2], stride=2, scope=\\\\\\\'maxpool4\\\\\\\')\\\\n-                net = fire_module(net, 32, 128, scope=\\\\\\\'fire5\\\\\\\')\\\\n-                net = fire_module(net, 48, 192, scope=\\\\\\\'fire6\\\\\\\')\\\\n-                net = fire_module(net, 48, 192, scope=\\\\\\\'fire7\\\\\\\')\\\\n-                net = fire_module(net, 64, 256, scope=\\\\\\\'fire8\\\\\\\')\\\\n-                net = slim.max_pool2d(net, [3, 3], stride=2, scope=\\\\\\\'maxpool8\\\\\\\')\\\\n-                net = fire_module(net, 64, 256, scope=\\\\\\\'fire9\\\\\\\')\\\\n-                net = slim.dropout(net, keep_probability)\\\\n-                net = slim.conv2d(net, 1000, [1, 1], activation_fn=None, normalizer_fn=None, scope=\\\\\\\'conv10\\\\\\\')\\\\n-                net = slim.avg_pool2d(net, net.get_shape()[1:3], scope=\\\\\\\'avgpool10\\\\\\\')\\\\n-                net = tf.squeeze(net, [1, 2], name=\\\\\\\'logits\\\\\\\')\\\\n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \\\\n-                        scope=\\\\\\\'Bottleneck\\\\\\\', reuse=False)\\\\n-    return net, None\\\'\\n\\\\ No newline at end of file\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_54_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_54_Pro.jpg\\ndeleted file mode 100644\\nindex 132948b..0000000\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_54_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_55_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_55_Pro.jpg\\ndeleted file mode 100644\\nindex d589b1a..0000000\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_55_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro (2).jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro (2).jpg\\ndeleted file mode 100644\\nindex d47e20d..0000000\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro.jpg\\ndeleted file mode 100644\\nindex fbe6de7..0000000\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro (2).jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro (2).jpg\\ndeleted file mode 100644\\nindex 3e5846d..0000000\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro.jpg\\ndeleted file mode 100644\\nindex b23bcfa..0000000\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro (2).jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro (2).jpg\\ndeleted file mode 100644\\nindex bb832e9..0000000\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro.jpg\\ndeleted file mode 100644\\nindex a0b4312..0000000\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_59_Pro (2).jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_59_Pro (2).jpg\\ndeleted file mode 100644\\nindex 928f751..0000000\\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_59_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/raw/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg b/Dataset/FaceData/raw/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg\\ndeleted file mode 100644\\nindex 6537490..0000000\\nBinary files a/Dataset/FaceData/raw/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg and /dev/null differ\\ndiff --git a/Dataset/FaceData/raw/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg b/Dataset/FaceData/raw/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg\\ndeleted file mode 100644\\nindex 8f6b0f9..0000000\\nBinary files a/Dataset/FaceData/raw/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg and /dev/null differ\\ndiff --git a/Dataset/FaceData/raw/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg b/Dataset/FaceData/raw/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg\\ndeleted file mode 100644\\nindex f74031f..0000000\\nBinary files a/Dataset/FaceData/raw/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.jpeg b/Dataset/FaceData/test/data/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.jpeg\\ndeleted file mode 100644\\nindex b5ebe28..0000000\\nBinary files a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.jpeg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.jpeg b/Dataset/FaceData/test/data/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.jpeg\\ndeleted file mode 100644\\nindex af9bfea..0000000\\nBinary files a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.jpeg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.jpeg b/Dataset/FaceData/test/data/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.jpeg\\ndeleted file mode 100644\\nindex 28c29dc..0000000\\nBinary files a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.jpeg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.jpeg b/Dataset/FaceData/test/data/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.jpeg\\ndeleted file mode 100644\\nindex 979d22b..0000000\\nBinary files a/Dataset/FaceData/test/data/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.jpeg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.jpeg b/Dataset/FaceData/test/data/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.jpeg\\ndeleted file mode 100644\\nindex 95a477c..0000000\\nBinary files a/Dataset/FaceData/test/data/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.jpeg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.jpeg b/Dataset/FaceData/test/data/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.jpeg\\ndeleted file mode 100644\\nindex 208ec5c..0000000\\nBinary files a/Dataset/FaceData/test/data/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.jpeg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/ChuQuangHuy/IMG_0349.HEIC b/Dataset/FaceData/test/data/ChuQuangHuy/IMG_0349.HEIC\\ndeleted file mode 100644\\nindex 345acbc..0000000\\nBinary files a/Dataset/FaceData/test/data/ChuQuangHuy/IMG_0349.HEIC and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.jpg b/Dataset/FaceData/test/data/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.jpg\\ndeleted file mode 100644\\nindex b038c0c..0000000\\nBinary files a/Dataset/FaceData/test/data/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.jpg b/Dataset/FaceData/test/data/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.jpg\\ndeleted file mode 100644\\nindex e3de1ee..0000000\\nBinary files a/Dataset/FaceData/test/data/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro (3).jpg\\ndeleted file mode 100644\\nindex 967a588..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro (3).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro.jpg\\ndeleted file mode 100644\\nindex c4e9ec9..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro (2).jpg\\ndeleted file mode 100644\\nindex ab17f1f..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro.jpg\\ndeleted file mode 100644\\nindex e9e3371..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (2).jpg\\ndeleted file mode 100644\\nindex dcd5b17..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (3).jpg\\ndeleted file mode 100644\\nindex bdf69fe..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (3).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro.jpg\\ndeleted file mode 100644\\nindex e5e0d42..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (2).jpg\\ndeleted file mode 100644\\nindex cf7525e..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (3).jpg\\ndeleted file mode 100644\\nindex f22e4b7..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (3).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro.jpg\\ndeleted file mode 100644\\nindex 56ac445..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro (2).jpg\\ndeleted file mode 100644\\nindex a72c6b8..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro.jpg\\ndeleted file mode 100644\\nindex e4e969a..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_36_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_36_Pro.jpg\\ndeleted file mode 100644\\nindex 94d80b4..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_36_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro (2).jpg\\ndeleted file mode 100644\\nindex ef3d4f9..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro.jpg\\ndeleted file mode 100644\\nindex d7333e8..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (2).jpg\\ndeleted file mode 100644\\nindex f8bec2a..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (3).jpg\\ndeleted file mode 100644\\nindex 42fdad1..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (3).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro.jpg\\ndeleted file mode 100644\\nindex 9851605..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (2).jpg\\ndeleted file mode 100644\\nindex 63fd876..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (3).jpg\\ndeleted file mode 100644\\nindex 90c377d..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (3).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro.jpg\\ndeleted file mode 100644\\nindex f669175..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (2).jpg\\ndeleted file mode 100644\\nindex 5ba014d..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (3).jpg\\ndeleted file mode 100644\\nindex 17c4b93..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (3).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro.jpg\\ndeleted file mode 100644\\nindex 7c91590..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro (2).jpg\\ndeleted file mode 100644\\nindex 16d41be..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro.jpg\\ndeleted file mode 100644\\nindex 4271bdd..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (2).jpg\\ndeleted file mode 100644\\nindex 0646f0b..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (3).jpg\\ndeleted file mode 100644\\nindex eeb4397..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (3).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro.jpg\\ndeleted file mode 100644\\nindex dba3891..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (2).jpg\\ndeleted file mode 100644\\nindex b2c8e83..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (3).jpg\\ndeleted file mode 100644\\nindex a6e8ebf..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (3).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro.jpg\\ndeleted file mode 100644\\nindex d163084..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro (2).jpg\\ndeleted file mode 100644\\nindex 38971bf..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro.jpg\\ndeleted file mode 100644\\nindex 5d9bd5c..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (2).jpg\\ndeleted file mode 100644\\nindex 9903ab8..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (3).jpg\\ndeleted file mode 100644\\nindex 5f8c977..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (3).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro.jpg\\ndeleted file mode 100644\\nindex 6db75f8..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro (2).jpg\\ndeleted file mode 100644\\nindex 2daaf72..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro.jpg\\ndeleted file mode 100644\\nindex e84c735..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_22_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_22_Pro.jpg\\ndeleted file mode 100644\\nindex 7888261..0000000\\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_22_Pro.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_0327.JPG b/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_0327.JPG\\ndeleted file mode 100644\\nindex 9f03d5d..0000000\\nBinary files a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_0327.JPG and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092236.jpg b/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092236.jpg\\ndeleted file mode 100644\\nindex e587b19..0000000\\nBinary files a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092236.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092237.jpg b/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092237.jpg\\ndeleted file mode 100644\\nindex cf24b36..0000000\\nBinary files a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092237.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092241.jpg b/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092241.jpg\\ndeleted file mode 100644\\nindex a505a8a..0000000\\nBinary files a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092241.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/NgocKem/IMG_1742972407559.jpg b/Dataset/FaceData/test/data/NgocKem/IMG_1742972407559.jpg\\ndeleted file mode 100644\\nindex 0483ff8..0000000\\nBinary files a/Dataset/FaceData/test/data/NgocKem/IMG_1742972407559.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/NgocKem/IMG_1742972428266.jpg b/Dataset/FaceData/test/data/NgocKem/IMG_1742972428266.jpg\\ndeleted file mode 100644\\nindex 96d77fc..0000000\\nBinary files a/Dataset/FaceData/test/data/NgocKem/IMG_1742972428266.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/NgocKem/IMG_1742972431298.jpg b/Dataset/FaceData/test/data/NgocKem/IMG_1742972431298.jpg\\ndeleted file mode 100644\\nindex bd4c93c..0000000\\nBinary files a/Dataset/FaceData/test/data/NgocKem/IMG_1742972431298.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.jpeg b/Dataset/FaceData/test/data/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.jpeg\\ndeleted file mode 100644\\nindex 6aba153..0000000\\nBinary files a/Dataset/FaceData/test/data/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.jpeg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/Phuong/Messenger_creation_7343AEA7-71DD-40DB-B8C3-9D6F0CED60F9.jpeg b/Dataset/FaceData/test/data/Phuong/Messenger_creation_7343AEA7-71DD-40DB-B8C3-9D6F0CED60F9.jpeg\\ndeleted file mode 100644\\nindex d41ca36..0000000\\nBinary files a/Dataset/FaceData/test/data/Phuong/Messenger_creation_7343AEA7-71DD-40DB-B8C3-9D6F0CED60F9.jpeg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.jpeg b/Dataset/FaceData/test/data/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.jpeg\\ndeleted file mode 100644\\nindex 161aa4b..0000000\\nBinary files a/Dataset/FaceData/test/data/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.jpeg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173023.jpg b/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173023.jpg\\ndeleted file mode 100644\\nindex 4982c4f..0000000\\nBinary files a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173023.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173025.jpg b/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173025.jpg\\ndeleted file mode 100644\\nindex 85c8a30..0000000\\nBinary files a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173025.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173026.jpg b/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173026.jpg\\ndeleted file mode 100644\\nindex a18bfd6..0000000\\nBinary files a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173026.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173028.jpg b/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173028.jpg\\ndeleted file mode 100644\\nindex 5b6d155..0000000\\nBinary files a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173028.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.jpg b/Dataset/FaceData/test/data/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.jpg\\ndeleted file mode 100644\\nindex c688251..0000000\\nBinary files a/Dataset/FaceData/test/data/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/TranDangHieu/download (1).jpg b/Dataset/FaceData/test/data/TranDangHieu/download (1).jpg\\ndeleted file mode 100644\\nindex e0c8116..0000000\\nBinary files a/Dataset/FaceData/test/data/TranDangHieu/download (1).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/TranDangHieu/download (2).jpg b/Dataset/FaceData/test/data/TranDangHieu/download (2).jpg\\ndeleted file mode 100644\\nindex ef5f43d..0000000\\nBinary files a/Dataset/FaceData/test/data/TranDangHieu/download (2).jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/TranDangHieu/download.jpg b/Dataset/FaceData/test/data/TranDangHieu/download.jpg\\ndeleted file mode 100644\\nindex 3d6e0fb..0000000\\nBinary files a/Dataset/FaceData/test/data/TranDangHieu/download.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134208.jpg b/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134208.jpg\\ndeleted file mode 100644\\nindex 104e453..0000000\\nBinary files a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134208.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134210.jpg b/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134210.jpg\\ndeleted file mode 100644\\nindex c3ee227..0000000\\nBinary files a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134210.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134211.jpg b/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134211.jpg\\ndeleted file mode 100644\\nindex 77cc3f0..0000000\\nBinary files a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134211.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png b/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png\\ndeleted file mode 100644\\nindex 68c44d6..0000000\\nBinary files a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png b/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png\\ndeleted file mode 100644\\nindex f2dc8a1..0000000\\nBinary files a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png b/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png\\ndeleted file mode 100644\\nindex 9b27a7d..0000000\\nBinary files a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135142.jpg b/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135142.jpg\\ndeleted file mode 100644\\nindex 5b6bf55..0000000\\nBinary files a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135142.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135145.jpg b/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135145.jpg\\ndeleted file mode 100644\\nindex 46005cc..0000000\\nBinary files a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135145.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135147.jpg b/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135147.jpg\\ndeleted file mode 100644\\nindex 0c45781..0000000\\nBinary files a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135147.jpg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg b/Dataset/FaceData/test/data/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg\\ndeleted file mode 100644\\nindex 6537490..0000000\\nBinary files a/Dataset/FaceData/test/data/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg b/Dataset/FaceData/test/data/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg\\ndeleted file mode 100644\\nindex 8f6b0f9..0000000\\nBinary files a/Dataset/FaceData/test/data/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg b/Dataset/FaceData/test/data/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg\\ndeleted file mode 100644\\nindex f74031f..0000000\\nBinary files a/Dataset/FaceData/test/data/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.jpeg b/Dataset/FaceData/test/data/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.jpeg\\ndeleted file mode 100644\\nindex 7b62bc3..0000000\\nBinary files a/Dataset/FaceData/test/data/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.jpeg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.jpeg b/Dataset/FaceData/test/data/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.jpeg\\ndeleted file mode 100644\\nindex 1be1f19..0000000\\nBinary files a/Dataset/FaceData/test/data/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.jpeg and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.png b/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.png\\ndeleted file mode 100644\\nindex 87978ce..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.png b/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.png\\ndeleted file mode 100644\\nindex 5269259..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.png b/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.png\\ndeleted file mode 100644\\nindex cd8c24c..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.png b/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.png\\ndeleted file mode 100644\\nindex 0650f16..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.png b/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.png\\ndeleted file mode 100644\\nindex 714bc33..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.png b/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.png\\ndeleted file mode 100644\\nindex 9b3ac40..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/485290655_1069958524968534_6128511440722701069_n.png b/Dataset/FaceData/test/data/txl/BuiCongSon/485290655_1069958524968534_6128511440722701069_n.png\\ndeleted file mode 100644\\nindex cd86dc1..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/485290655_1069958524968534_6128511440722701069_n.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/485424995_1386922852752466_129752981138586263_n.png b/Dataset/FaceData/test/data/txl/BuiCongSon/485424995_1386922852752466_129752981138586263_n.png\\ndeleted file mode 100644\\nindex 90013f6..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/485424995_1386922852752466_129752981138586263_n.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/485464001_1219012669643907_2610181536763391909_n.png b/Dataset/FaceData/test/data/txl/BuiCongSon/485464001_1219012669643907_2610181536763391909_n.png\\ndeleted file mode 100644\\nindex 31c9d0c..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/485464001_1219012669643907_2610181536763391909_n.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/485512577_2113779945729669_160018376255303225_n.png b/Dataset/FaceData/test/data/txl/BuiCongSon/485512577_2113779945729669_160018376255303225_n.png\\ndeleted file mode 100644\\nindex efbe736..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/485512577_2113779945729669_160018376255303225_n.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/z6377532752047_4b7b3c7d460239f92fa3cec037738eca.png b/Dataset/FaceData/test/data/txl/BuiCongSon/z6377532752047_4b7b3c7d460239f92fa3cec037738eca.png\\ndeleted file mode 100644\\nindex 09d25b3..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/z6377532752047_4b7b3c7d460239f92fa3cec037738eca.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151626.png b/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151626.png\\ndeleted file mode 100644\\nindex 66a927d..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151626.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151720.png b/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151720.png\\ndeleted file mode 100644\\nindex 5554b2f..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151720.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151731.png b/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151731.png\\ndeleted file mode 100644\\nindex b92b066..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151731.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151738.png b/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151738.png\\ndeleted file mode 100644\\nindex 852ab42..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151738.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.png b/Dataset/FaceData/test/data/txl/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.png\\ndeleted file mode 100644\\nindex f6ad820..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.png b/Dataset/FaceData/test/data/txl/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.png\\ndeleted file mode 100644\\nindex 8021733..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_1742972296094.png b/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_1742972296094.png\\ndeleted file mode 100644\\nindex e7677d6..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_1742972296094.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_20250326_134739.png b/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_20250326_134739.png\\ndeleted file mode 100644\\nindex d8efdad..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_20250326_134739.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_0327.png b/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_0327.png\\ndeleted file mode 100644\\nindex 1037e8e..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_0327.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092236.png b/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092236.png\\ndeleted file mode 100644\\nindex 9be2f92..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092236.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092237.png b/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092237.png\\ndeleted file mode 100644\\nindex 17b5d34..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092237.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092241.png b/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092241.png\\ndeleted file mode 100644\\nindex 97fcfdb..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092241.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972407559.png b/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972407559.png\\ndeleted file mode 100644\\nindex 66c7877..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972407559.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972428266.png b/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972428266.png\\ndeleted file mode 100644\\nindex 47db00e..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972428266.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972431298.png b/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972431298.png\\ndeleted file mode 100644\\nindex 1bcc4bf..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972431298.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.png b/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.png\\ndeleted file mode 100644\\nindex 6d2cfbd..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.png b/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.png\\ndeleted file mode 100644\\nindex d1d90a5..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/Thao/851D4C65-07BE-4220-8488-84C40C1A79D6.png b/Dataset/FaceData/test/data/txl/Thao/851D4C65-07BE-4220-8488-84C40C1A79D6.png\\ndeleted file mode 100644\\nindex d2170a7..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/Thao/851D4C65-07BE-4220-8488-84C40C1A79D6.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/Thao/98E5A551-3DC2-4A85-9497-AFA4A845325A.png b/Dataset/FaceData/test/data/txl/Thao/98E5A551-3DC2-4A85-9497-AFA4A845325A.png\\ndeleted file mode 100644\\nindex 055d27a..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/Thao/98E5A551-3DC2-4A85-9497-AFA4A845325A.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173023.png b/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173023.png\\ndeleted file mode 100644\\nindex f95fdd9..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173023.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173025.png b/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173025.png\\ndeleted file mode 100644\\nindex f2c1813..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173025.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173026.png b/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173026.png\\ndeleted file mode 100644\\nindex 2541428..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173026.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173028.png b/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173028.png\\ndeleted file mode 100644\\nindex 1eae995..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173028.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.png b/Dataset/FaceData/test/data/txl/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.png\\ndeleted file mode 100644\\nindex 0678197..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/TranDangHieu/download (1).png b/Dataset/FaceData/test/data/txl/TranDangHieu/download (1).png\\ndeleted file mode 100644\\nindex d7a2377..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/TranDangHieu/download (1).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/TranDangHieu/download (2).png b/Dataset/FaceData/test/data/txl/TranDangHieu/download (2).png\\ndeleted file mode 100644\\nindex da1b935..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/TranDangHieu/download (2).png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/TranDangHieu/download.png b/Dataset/FaceData/test/data/txl/TranDangHieu/download.png\\ndeleted file mode 100644\\nindex 51eee7f..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/TranDangHieu/download.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134208.png b/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134208.png\\ndeleted file mode 100644\\nindex a3ffc1f..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134208.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134210.png b/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134210.png\\ndeleted file mode 100644\\nindex 49694a3..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134210.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134211.png b/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134211.png\\ndeleted file mode 100644\\nindex 7af9479..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134211.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG1.png b/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG1.png\\ndeleted file mode 100644\\nindex 4e78809..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG1.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG2.png b/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG2.png\\ndeleted file mode 100644\\nindex 21b37f7..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG2.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png b/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png\\ndeleted file mode 100644\\nindex 971746e..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png b/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png\\ndeleted file mode 100644\\nindex 275fcb7..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png b/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png\\ndeleted file mode 100644\\nindex 29af910..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135142.png b/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135142.png\\ndeleted file mode 100644\\nindex 0149809..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135142.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135145.png b/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135145.png\\ndeleted file mode 100644\\nindex 2a6a9b4..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135145.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135147.png b/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135147.png\\ndeleted file mode 100644\\nindex 83a6450..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135147.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png b/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png\\ndeleted file mode 100644\\nindex 8257ab5..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png b/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png\\ndeleted file mode 100644\\nindex 45870ff..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png b/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png\\ndeleted file mode 100644\\nindex f4593ba..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.png b/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.png\\ndeleted file mode 100644\\nindex 942d65d..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.png b/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.png\\ndeleted file mode 100644\\nindex 6872f3e..0000000\\nBinary files a/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.png and /dev/null differ\\ndiff --git a/Dataset/FaceData/test/data/txl/bounding_boxes_47191.txt b/Dataset/FaceData/test/data/txl/bounding_boxes_47191.txt\\ndeleted file mode 100644\\nindex d314c07..0000000\\n--- a/Dataset/FaceData/test/data/txl/bounding_boxes_47191.txt\\n+++ /dev/null\\n@@ -1,55 +0,0 @@\\n-Dataset/FaceData/processed\\\\TrangNhung\\\\IMG_20250326_134210.png 895 349 1544 1272\\n-Dataset/FaceData/processed\\\\TrangNhung\\\\IMG_20250326_134208.png 535 336 1175 1157\\n-Dataset/FaceData/processed\\\\TrangNhung\\\\IMG_20250326_134211.png 586 360 1301 1293\\n-Dataset/FaceData/processed\\\\Xemesis\\\\Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.png 348 648 886 1345\\n-Dataset/FaceData/processed\\\\Xemesis\\\\Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.png 0 293 772 1203\\n-Dataset/FaceData/processed\\\\Emma\\\\IMG_20250326_151626.png 967 170 1656 1050\\n-Dataset/FaceData/processed\\\\Emma\\\\IMG_20250326_151731.png 362 264 1458 1704\\n-Dataset/FaceData/processed\\\\Emma\\\\IMG_20250326_151738.png 380 538 1544 1985\\n-Dataset/FaceData/processed\\\\Emma\\\\IMG_20250326_151720.png 417 326 1549 1712\\n-Dataset/FaceData/processed\\\\TrinhHuuTuan\\\\IMG1.png 199 545 810 1319\\n-Dataset/FaceData/processed\\\\TrinhHuuTuan\\\\IMG2.png 298 659 907 1438\\n-Dataset/FaceData/processed\\\\BotIu\\\\Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.png 589 739 1206 1280\\n-Dataset/FaceData/processed\\\\BotIu\\\\Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.png 338 362 954 1198\\n-Dataset/FaceData/processed\\\\BotIu\\\\Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.png 527 904 1096 1457\\n-Dataset/FaceData/processed\\\\TrinhTranPhuongTuan\\\\Screenshot 2025-03-26 152756.png 73 208 429 642\\n-Dataset/FaceData/processed\\\\TrinhTranPhuongTuan\\\\Screenshot 2025-03-26 152811.png 20 150 367 604\\n-Dataset/FaceData/processed\\\\TrinhTranPhuongTuan\\\\Screenshot 2025-03-26 152730.png 73 210 394 633\\n-Dataset/FaceData/processed\\\\Viruss\\\\Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png 154 108 1417 1799\\n-Dataset/FaceData/processed\\\\Viruss\\\\Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png 364 442 910 1148\\n-Dataset/FaceData/processed\\\\Viruss\\\\Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png 50 121 926 1264\\n-Dataset/FaceData/processed\\\\NgocKem\\\\IMG_1742972407559.png 304 247 820 976\\n-Dataset/FaceData/processed\\\\NgocKem\\\\IMG_1742972428266.png 300 233 870 981\\n-Dataset/FaceData/processed\\\\NgocKem\\\\IMG_1742972431298.png 180 168 796 998\\n-Dataset/FaceData/processed\\\\ThaoNguyen\\\\IMG_20250325_173023.png 389 530 1184 1500\\n-Dataset/FaceData/processed\\\\ThaoNguyen\\\\IMG_20250325_173028.png 490 449 1140 1231\\n-Dataset/FaceData/processed\\\\ThaoNguyen\\\\IMG_20250325_173025.png 513 544 1284 1399\\n-Dataset/FaceData/processed\\\\ThaoNguyen\\\\IMG_20250325_173026.png 489 328 1176 1142\\n-Dataset/FaceData/processed\\\\AnhTu\\\\Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.png 321 361 1163 1438\\n-Dataset/FaceData/processed\\\\AnhTu\\\\Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.png 217 635 1053 1802\\n-Dataset/FaceData/processed\\\\AnhTu\\\\Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.png 474 420 1348 1596\\n-Dataset/FaceData/processed\\\\Thao\\\\98E5A551-3DC2-4A85-9497-AFA4A845325A.png 287 854 841 1536\\n-Dataset/FaceData/processed\\\\Thao\\\\851D4C65-07BE-4220-8488-84C40C1A79D6.png 163 460 841 1386\\n-Dataset/FaceData/processed\\\\Phuong\\\\Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.png 383 266 1204 1442\\n-Dataset/FaceData/processed\\\\Phuong\\\\Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.png 398 511 855 1063\\n-Dataset/FaceData/processed\\\\Phuong\\\\Messenger_creation_7343AEA7-71DD-40DB-B8C3-9D6F0CED60F9.png\\n-Dataset/FaceData/processed\\\\ThienAn\\\\att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.png 357 494 1083 1463\\n-Dataset/FaceData/processed\\\\LeAnhTrang\\\\IMG_1742972296094.png 436 138 965 850\\n-Dataset/FaceData/processed\\\\LeAnhTrang\\\\IMG_20250326_134739.png 417 628 1522 2056\\n-Dataset/FaceData/processed\\\\Jack\\\\A3FED369-0E1F-4064-830E-FBFD69D77C85.png 111 709 831 1654\\n-Dataset/FaceData/processed\\\\Jack\\\\2209C349-3B30-47AC-89C1-1152E1A42FFC.png 224 606 982 1624\\n-Dataset/FaceData/processed\\\\NghiemAnhHieu\\\\IMG_20250327_092236.png 274 800 1031 1820\\n-Dataset/FaceData/processed\\\\NghiemAnhHieu\\\\IMG_20250327_092241.png 486 1016 840 1487\\n-Dataset/FaceData/processed\\\\NghiemAnhHieu\\\\IMG_0327.png 83 74 764 794\\n-Dataset/FaceData/processed\\\\NghiemAnhHieu\\\\IMG_20250327_092237.png 566 1129 1142 1953\\n-Dataset/FaceData/processed\\\\TranDangHieu\\\\download (2).png 152 762 696 1478\\n-Dataset/FaceData/processed\\\\TranDangHieu\\\\download (1).png 262 672 767 1357\\n-Dataset/FaceData/processed\\\\TranDangHieu\\\\download.png 202 705 750 1444\\n-Dataset/FaceData/processed\\\\BuiCongSon\\\\485424995_1386922852752466_129752981138586263_n.png 320 554 1058 1544\\n-Dataset/FaceData/processed\\\\BuiCongSon\\\\485512577_2113779945729669_160018376255303225_n.png 318 390 1184 1528\\n-Dataset/FaceData/processed\\\\BuiCongSon\\\\485464001_1219012669643907_2610181536763391909_n.png 461 308 1375 1574\\n-Dataset/FaceData/processed\\\\BuiCongSon\\\\z6377532752047_4b7b3c7d460239f92fa3cec037738eca.png 462 527 1017 1272\\n-Dataset/FaceData/processed\\\\BuiCongSon\\\\485290655_1069958524968534_6128511440722701069_n.png 376 472 1198 1582\\n-Dataset/FaceData/processed\\\\TuSenna\\\\IMG_20250326_135145.png 80 165 468 685\\n-Dataset/FaceData/processed\\\\TuSenna\\\\IMG_20250326_135147.png 225 199 579 683\\n-Dataset/FaceData/processed\\\\TuSenna\\\\IMG_20250326_135142.png 171 209 549 722\\ndiff --git a/Dataset/FaceData/test/data/txl/revision_info.txt b/Dataset/FaceData/test/data/txl/revision_info.txt\\ndeleted file mode 100644\\nindex 33dea13..0000000\\n--- a/Dataset/FaceData/test/data/txl/revision_info.txt\\n+++ /dev/null\\n@@ -1,7 +0,0 @@\\n-arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25\\n---------------------\\n-tensorflow version: 2.19.0\\n---------------------\\n-git hash: b\\\'69ff1e149c0d84a123d6516ddd82970e65392608\\\'\\n---------------------\\n-b\\\'\\\'\\n\\\\ No newline at end of file\\ndiff --git a/cmd.txt b/cmd.txt\\ndeleted file mode 100644\\nindex a3d455b..0000000\\n--- a/cmd.txt\\n+++ /dev/null\\n@@ -1,6 +0,0 @@\\n-python src/align_dataset_mtcnn.py  Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32  --random_order --gpu_memory_fraction 0.25\\n-\\n-\\n-python src/classifier.py TRAIN Dataset/FaceData/processed Models/20180402-114759.pb Models/facemodel.pkl --batch_size 1000\\n-\\n-python src/face_rec_cam.py \\n\\\\ No newline at end of file\\ndiff --git a/src/compare.py b/src/compare.py\\ndeleted file mode 100644\\nindex bc53cc4..0000000\\n--- a/src/compare.py\\n+++ /dev/null\\n@@ -1,130 +0,0 @@\\n-"""Performs face alignment and calculates L2 distance between the embeddings of images."""\\n-\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-from scipy import misc\\n-import tensorflow as tf\\n-import numpy as np\\n-import sys\\n-import os\\n-import copy\\n-import argparse\\n-import facenet\\n-import align.detect_face\\n-\\n-def main(args):\\n-\\n-    images = load_and_align_data(args.image_files, args.image_size, args.margin, args.gpu_memory_fraction)\\n-    with tf.Graph().as_default():\\n-\\n-        with tf.Session() as sess:\\n-      \\n-            # Load the model\\n-            facenet.load_model(args.model)\\n-    \\n-            # Get input and output tensors\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\n-\\n-            # Run forward pass to calculate embeddings\\n-            feed_dict = { images_placeholder: images, phase_train_placeholder:False }\\n-            emb = sess.run(embeddings, feed_dict=feed_dict)\\n-            \\n-            nrof_images = len(args.image_files)\\n-\\n-            print(\\\'Images:\\\')\\n-            for i in range(nrof_images):\\n-                print(\\\'%1d: %s\\\' % (i, args.image_files[i]))\\n-            print(\\\'\\\')\\n-            \\n-            # Print distance matrix\\n-            print(\\\'Distance matrix\\\')\\n-            print(\\\'    \\\', end=\\\'\\\')\\n-            for i in range(nrof_images):\\n-                print(\\\'    %1d     \\\' % i, end=\\\'\\\')\\n-            print(\\\'\\\')\\n-            for i in range(nrof_images):\\n-                print(\\\'%1d  \\\' % i, end=\\\'\\\')\\n-                for j in range(nrof_images):\\n-                    dist = np.sqrt(np.sum(np.square(np.subtract(emb[i,:], emb[j,:]))))\\n-                    print(\\\'  %1.4f  \\\' % dist, end=\\\'\\\')\\n-                print(\\\'\\\')\\n-            \\n-            \\n-def load_and_align_data(image_paths, image_size, margin, gpu_memory_fraction):\\n-\\n-    minsize = 20 # minimum size of face\\n-    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\\\'s threshold\\n-    factor = 0.709 # scale factor\\n-    \\n-    print(\\\'Creating networks and loading parameters\\\')\\n-    with tf.Graph().as_default():\\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\n-        with sess.as_default():\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\\n-  \\n-    tmp_image_paths=copy.copy(image_paths)\\n-    img_list = []\\n-    for image in tmp_image_paths:\\n-        img = misc.imread(os.path.expanduser(image), mode=\\\'RGB\\\')\\n-        img_size = np.asarray(img.shape)[0:2]\\n-        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\\n-        if len(bounding_boxes) < 1:\\n-          image_paths.remove(image)\\n-          print("can\\\'t detect face, remove ", image)\\n-          continue\\n-        det = np.squeeze(bounding_boxes[0,0:4])\\n-        bb = np.zeros(4, dtype=np.int32)\\n-        bb[0] = np.maximum(det[0]-margin/2, 0)\\n-        bb[1] = np.maximum(det[1]-margin/2, 0)\\n-        bb[2] = np.minimum(det[2]+margin/2, img_size[1])\\n-        bb[3] = np.minimum(det[3]+margin/2, img_size[0])\\n-        cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\\n-        aligned = misc.imresize(cropped, (image_size, image_size), interp=\\\'bilinear\\\')\\n-        prewhitened = facenet.prewhiten(aligned)\\n-        img_list.append(prewhitened)\\n-    images = np.stack(img_list)\\n-    return images\\n-\\n-def parse_arguments(argv):\\n-    parser = argparse.ArgumentParser()\\n-    \\n-    parser.add_argument(\\\'model\\\', type=str, \\n-        help=\\\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\\\')\\n-    parser.add_argument(\\\'image_files\\\', type=str, nargs=\\\'+\\\', help=\\\'Images to compare\\\')\\n-    parser.add_argument(\\\'--image_size\\\', type=int,\\n-        help=\\\'Image size (height, width) in pixels.\\\', default=160)\\n-    parser.add_argument(\\\'--margin\\\', type=int,\\n-        help=\\\'Margin for the crop around the bounding box (height, width) in pixels.\\\', default=44)\\n-    parser.add_argument(\\\'--gpu_memory_fraction\\\', type=float,\\n-        help=\\\'Upper bound on the amount of GPU memory that will be used by the process.\\\', default=1.0)\\n-    return parser.parse_args(argv)\\n-\\n-if __name__ == \\\'__main__\\\':\\n-    main(parse_arguments(sys.argv[1:]))\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\ndeleted file mode 100644\\nindex 455f67a..0000000\\n--- a/src/face_rec_cam.py\\n+++ /dev/null\\n@@ -1,135 +0,0 @@\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-import tensorflow as tf\\n-from imutils.video import VideoStream\\n-\\n-\\n-import argparse\\n-import facenet\\n-import imutils\\n-import os\\n-import sys\\n-import math\\n-import pickle\\n-import align.detect_face\\n-import numpy as np\\n-import cv2\\n-import collections\\n-from sklearn.svm import SVC\\n-\\n-\\n-def main():\\n-    parser = argparse.ArgumentParser()\\n-    parser.add_argument(\\\'--path\\\', help=\\\'Path of the video you want to test on.\\\', default=0)\\n-    args = parser.parse_args()\\n-\\n-    MINSIZE = 20\\n-    THRESHOLD = [0.6, 0.7, 0.7]\\n-    FACTOR = 0.709\\n-    IMAGE_SIZE = 182\\n-    INPUT_IMAGE_SIZE = 160\\n-    CLASSIFIER_PATH = \\\'Models/facemodel.pkl\\\'\\n-    VIDEO_PATH = args.path\\n-    FACENET_MODEL_PATH = \\\'Models/20180402-114759.pb\\\'\\n-\\n-    # Load The Custom Classifier\\n-    with open(CLASSIFIER_PATH, \\\'rb\\\') as file:\\n-        model, class_names = pickle.load(file)\\n-    print("Custom Classifier, Successfully loaded")\\n-\\n-    with tf.Graph().as_default():\\n-\\n-        # Cai dat GPU neu co\\n-        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\n-\\n-        with sess.as_default():\\n-\\n-            # Load the model\\n-            print(\\\'Loading feature extraction model\\\')\\n-            facenet.load_model(FACENET_MODEL_PATH)\\n-\\n-            # Get input and output tensors\\n-            graph = tf.compat.v1.get_default_graph()\\n-            images_placeholder = graph.get_tensor_by_name("input:0")\\n-            embeddings = graph.get_tensor_by_name("embeddings:0")\\n-            phase_train_placeholder = graph.get_tensor_by_name("phase_train:0")\\n-            embedding_size = embeddings.get_shape()[1]\\n-\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\n-\\n-            people_detected = set()\\n-            person_detected = collections.Counter()\\n-\\n-            cap  = VideoStream(src=0).start()\\n-\\n-            while (True):\\n-                frame = cap.read()\\n-                frame = imutils.resize(frame, width=600)\\n-                frame = cv2.flip(frame, 1)\\n-\\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\n-\\n-                faces_found = bounding_boxes.shape[0]\\n-                try:\\n-                    if faces_found > 1:\\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\n-                                    1, (255, 255, 255), thickness=1, lineType=2)\\n-                    elif faces_found > 0:\\n-                        det = bounding_boxes[:, 0:4]\\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\\n-                        for i in range(faces_found):\\n-                            bb[i][0] = det[i][0]\\n-                            bb[i][1] = det[i][1]\\n-                            bb[i][2] = det[i][2]\\n-                            bb[i][3] = det[i][3]\\n-                            print(bb[i][3]-bb[i][1])\\n-                            print(frame.shape[0])\\n-                            print((bb[i][3]-bb[i][1])/frame.shape[0])\\n-                            if (bb[i][3]-bb[i][1])/frame.shape[0]>0.25:\\n-                                cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\n-                                                    interpolation=cv2.INTER_CUBIC)\\n-                                scaled = facenet.prewhiten(scaled)\\n-                                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\n-                                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\n-                                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\n-\\n-                                predictions = model.predict_proba(emb_array)\\n-                                best_class_indices = np.argmax(predictions, axis=1)\\n-                                best_class_probabilities = predictions[\\n-                                    np.arange(len(best_class_indices)), best_class_indices]\\n-                                best_name = class_names[best_class_indices[0]]\\n-                                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\n-\\n-\\n-\\n-                                if best_class_probabilities > 0.8:\\n-                                    cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\n-                                    text_x = bb[i][0]\\n-                                    text_y = bb[i][3] + 20\\n-\\n-                                    name = class_names[best_class_indices[0]]\\n-                                    cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\n-                                    cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\\n-                                                cv2.FONT_HERSHEY_COMPLEX_SMALL,\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\n-                                    person_detected[best_name] += 1\\n-                                else:\\n-                                    name = "Unknown"\\n-\\n-                except:\\n-                    pass\\n-\\n-                cv2.imshow(\\\'Face Recognition\\\', frame)\\n-                if cv2.waitKey(1) & 0xFF == ord(\\\'q\\\'):\\n-                    break\\n-\\n-            cap.release()\\n-            cv2.destroyAllWindows()\\n-\\n-\\n-main()\\ndiff --git a/src/face_rec_flask.py b/src/face_rec_flask.py\\ndeleted file mode 100644\\nindex 3f7a27b..0000000\\n--- a/src/face_rec_flask.py\\n+++ /dev/null\\n@@ -1,118 +0,0 @@\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-from flask import Flask\\n-from flask import render_template , request\\n-from flask_cors import CORS, cross_origin\\n-import tensorflow as tf\\n-import argparse\\n-import facenet\\n-import os\\n-import sys\\n-import math\\n-import pickle\\n-import align.detect_face\\n-import numpy as np\\n-import cv2\\n-import collections\\n-from sklearn.svm import SVC\\n-import base64\\n-\\n-MINSIZE = 20\\n-THRESHOLD = [0.6, 0.7, 0.7]\\n-FACTOR = 0.709\\n-IMAGE_SIZE = 182\\n-INPUT_IMAGE_SIZE = 160\\n-CLASSIFIER_PATH = \\\'./Models/facemodel.pkl\\\'\\n-FACENET_MODEL_PATH = \\\'./Models/20180402-114759.pb\\\'\\n-\\n-# Load The Custom Classifier\\n-with open(CLASSIFIER_PATH, \\\'rb\\\') as file:\\n-    model, class_names = pickle.load(file)\\n-print("Custom Classifier, Successfully loaded")\\n-\\n-tf.Graph().as_default()\\n-\\n-# Cai dat GPU neu co\\n-gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\n-sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\n-\\n-\\n-# Load the model\\n-print(\\\'Loading feature extraction model\\\')\\n-facenet.load_model(FACENET_MODEL_PATH)\\n-\\n-# Get input and output tensors\\n-images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\n-embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\n-phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\n-embedding_size = embeddings.get_shape()[1]\\n-pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "align")\\n-\\n-\\n-\\n-app = Flask(__name__)\\n-CORS(app)\\n-\\n-\\n-\\n-@app.route(\\\'/\\\')\\n-@cross_origin()\\n-def index():\\n-    return "OK!";\\n-\\n-@app.route(\\\'/recog\\\', methods=[\\\'POST\\\'])\\n-@cross_origin()\\n-def upload_img_file():\\n-    if request.method == \\\'POST\\\':\\n-        # base 64\\n-        name="Unknown"\\n-        f = request.form.get(\\\'image\\\')\\n-        w = int(request.form.get(\\\'w\\\'))\\n-        h = int(request.form.get(\\\'h\\\'))\\n-\\n-        decoded_string = base64.b64decode(f)\\n-        frame = np.fromstring(decoded_string, dtype=np.uint8)\\n-        #frame = frame.reshape(w,h,3)\\n-        frame = cv2.imdecode(frame, cv2.IMREAD_ANYCOLOR)  # cv2.IMREAD_COLOR in OpenCV 3.1\\n-\\n-        bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\n-\\n-        faces_found = bounding_boxes.shape[0]\\n-\\n-        if faces_found > 0:\\n-            det = bounding_boxes[:, 0:4]\\n-            bb = np.zeros((faces_found, 4), dtype=np.int32)\\n-            for i in range(faces_found):\\n-                bb[i][0] = det[i][0]\\n-                bb[i][1] = det[i][1]\\n-                bb[i][2] = det[i][2]\\n-                bb[i][3] = det[i][3]\\n-                cropped = frame\\n-                #cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\n-                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\n-                                    interpolation=cv2.INTER_CUBIC)\\n-                scaled = facenet.prewhiten(scaled)\\n-                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\n-                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\n-                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\n-                predictions = model.predict_proba(emb_array)\\n-                best_class_indices = np.argmax(predictions, axis=1)\\n-                best_class_probabilities = predictions[\\n-                    np.arange(len(best_class_indices)), best_class_indices]\\n-                best_name = class_names[best_class_indices[0]]\\n-                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\n-\\n-                if best_class_probabilities > 0.8:\\n-                    name = class_names[best_class_indices[0]]\\n-                else:\\n-                    name = "Unknown"\\n-\\n-\\n-        return name;\\n-\\n-\\n-if __name__ == \\\'__main__\\\':\\n-    app.run(debug=True, host=\\\'0.0.0.0\\\',port=\\\'8000\\\')\\n-\\ndiff --git a/src/models/__init__.py b/src/models/__init__.py\\ndeleted file mode 100644\\nindex efa6252..0000000\\n--- a/src/models/__init__.py\\n+++ /dev/null\\n@@ -1,2 +0,0 @@\\n-# flake8: noqa\\n-\\ndiff --git a/src/models/dummy.py b/src/models/dummy.py\\ndeleted file mode 100644\\nindex 7afe1ef..0000000\\n--- a/src/models/dummy.py\\n+++ /dev/null\\n@@ -1,54 +0,0 @@\\n-"""Dummy model used only for testing\\n-"""\\n-# MIT License\\n-# \\n-# Copyright (c) 2016 David Sandberg\\n-# \\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\\n-# of this software and associated documentation files (the "Software"), to deal\\n-# in the Software without restriction, including without limitation the rights\\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\n-# copies of the Software, and to permit persons to whom the Software is\\n-# furnished to do so, subject to the following conditions:\\n-# \\n-# The above copyright notice and this permission notice shall be included in all\\n-# copies or substantial portions of the Software.\\n-# \\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n-# SOFTWARE.\\n-\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-import tensorflow as tf\\n-import tensorflow.contrib.slim as slim\\n-import numpy as np\\n-  \\n-def inference(images, keep_probability, phase_train=True,  # @UnusedVariable\\n-              bottleneck_layer_size=128, bottleneck_layer_activation=None, weight_decay=0.0, reuse=None):  # @UnusedVariable\\n-    batch_norm_params = {\\n-        # Decay for the moving averages.\\n-        \\\'decay\\\': 0.995,\\n-        # epsilon to prevent 0s in variance.\\n-        \\\'epsilon\\\': 0.001,\\n-        # force in-place updates of mean and variance estimates\\n-        \\\'updates_collections\\\': None,\\n-        # Moving averages ends up in the trainable variables collection\\n-        \\\'variables_collections\\\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\\n-    }\\n-    \\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\\n-                        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\\n-                        normalizer_fn=slim.batch_norm,\\n-                        normalizer_params=batch_norm_params):\\n-        size = np.prod(images.get_shape()[1:].as_list())\\n-        net = slim.fully_connected(tf.reshape(images, (-1,size)), bottleneck_layer_size, activation_fn=None, \\n-                scope=\\\'Bottleneck\\\', reuse=False)\\n-        return net, None\\ndiff --git a/src/models/inception_resnet_v1.py b/src/models/inception_resnet_v1.py\\ndeleted file mode 100644\\nindex 475e81b..0000000\\n--- a/src/models/inception_resnet_v1.py\\n+++ /dev/null\\n@@ -1,246 +0,0 @@\\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\\n-#\\n-# Licensed under the Apache License, Version 2.0 (the "License");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-# http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an "AS IS" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-# ==============================================================================\\n-\\n-"""Contains the definition of the Inception Resnet V1 architecture.\\n-As described in http://arxiv.org/abs/1602.07261.\\n-  Inception-v4, Inception-ResNet and the Impact of Residual Connections\\n-    on Learning\\n-  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\\n-"""\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-import tensorflow as tf\\n-import tensorflow.contrib.slim as slim\\n-\\n-# Inception-Resnet-A\\n-def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\n-    """Builds the 35x35 resnet block."""\\n-    with tf.variable_scope(scope, \\\'Block35\\\', [net], reuse=reuse):\\n-        with tf.variable_scope(\\\'Branch_0\\\'):\\n-            tower_conv = slim.conv2d(net, 32, 1, scope=\\\'Conv2d_1x1\\\')\\n-        with tf.variable_scope(\\\'Branch_1\\\'):\\n-            tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\\\'Conv2d_0b_3x3\\\')\\n-        with tf.variable_scope(\\\'Branch_2\\\'):\\n-            tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-            tower_conv2_1 = slim.conv2d(tower_conv2_0, 32, 3, scope=\\\'Conv2d_0b_3x3\\\')\\n-            tower_conv2_2 = slim.conv2d(tower_conv2_1, 32, 3, scope=\\\'Conv2d_0c_3x3\\\')\\n-        mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\n-                         activation_fn=None, scope=\\\'Conv2d_1x1\\\')\\n-        net += scale * up\\n-        if activation_fn:\\n-            net = activation_fn(net)\\n-    return net\\n-\\n-# Inception-Resnet-B\\n-def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\n-    """Builds the 17x17 resnet block."""\\n-    with tf.variable_scope(scope, \\\'Block17\\\', [net], reuse=reuse):\\n-        with tf.variable_scope(\\\'Branch_0\\\'):\\n-            tower_conv = slim.conv2d(net, 128, 1, scope=\\\'Conv2d_1x1\\\')\\n-        with tf.variable_scope(\\\'Branch_1\\\'):\\n-            tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 128, [1, 7],\\n-                                        scope=\\\'Conv2d_0b_1x7\\\')\\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 128, [7, 1],\\n-                                        scope=\\\'Conv2d_0c_7x1\\\')\\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\n-                         activation_fn=None, scope=\\\'Conv2d_1x1\\\')\\n-        net += scale * up\\n-        if activation_fn:\\n-            net = activation_fn(net)\\n-    return net\\n-\\n-\\n-# Inception-Resnet-C\\n-def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\n-    """Builds the 8x8 resnet block."""\\n-    with tf.variable_scope(scope, \\\'Block8\\\', [net], reuse=reuse):\\n-        with tf.variable_scope(\\\'Branch_0\\\'):\\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\\\'Conv2d_1x1\\\')\\n-        with tf.variable_scope(\\\'Branch_1\\\'):\\n-            tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 192, [1, 3],\\n-                                        scope=\\\'Conv2d_0b_1x3\\\')\\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [3, 1],\\n-                                        scope=\\\'Conv2d_0c_3x1\\\')\\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\n-                         activation_fn=None, scope=\\\'Conv2d_1x1\\\')\\n-        net += scale * up\\n-        if activation_fn:\\n-            net = activation_fn(net)\\n-    return net\\n-  \\n-def reduction_a(net, k, l, m, n):\\n-    with tf.variable_scope(\\\'Branch_0\\\'):\\n-        tower_conv = slim.conv2d(net, n, 3, stride=2, padding=\\\'VALID\\\',\\n-                                 scope=\\\'Conv2d_1a_3x3\\\')\\n-    with tf.variable_scope(\\\'Branch_1\\\'):\\n-        tower_conv1_0 = slim.conv2d(net, k, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-        tower_conv1_1 = slim.conv2d(tower_conv1_0, l, 3,\\n-                                    scope=\\\'Conv2d_0b_3x3\\\')\\n-        tower_conv1_2 = slim.conv2d(tower_conv1_1, m, 3,\\n-                                    stride=2, padding=\\\'VALID\\\',\\n-                                    scope=\\\'Conv2d_1a_3x3\\\')\\n-    with tf.variable_scope(\\\'Branch_2\\\'):\\n-        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\\\'VALID\\\',\\n-                                     scope=\\\'MaxPool_1a_3x3\\\')\\n-    net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\\n-    return net\\n-\\n-def reduction_b(net):\\n-    with tf.variable_scope(\\\'Branch_0\\\'):\\n-        tower_conv = slim.conv2d(net, 256, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\\n-                                   padding=\\\'VALID\\\', scope=\\\'Conv2d_1a_3x3\\\')\\n-    with tf.variable_scope(\\\'Branch_1\\\'):\\n-        tower_conv1 = slim.conv2d(net, 256, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-        tower_conv1_1 = slim.conv2d(tower_conv1, 256, 3, stride=2,\\n-                                    padding=\\\'VALID\\\', scope=\\\'Conv2d_1a_3x3\\\')\\n-    with tf.variable_scope(\\\'Branch_2\\\'):\\n-        tower_conv2 = slim.conv2d(net, 256, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-        tower_conv2_1 = slim.conv2d(tower_conv2, 256, 3,\\n-                                    scope=\\\'Conv2d_0b_3x3\\\')\\n-        tower_conv2_2 = slim.conv2d(tower_conv2_1, 256, 3, stride=2,\\n-                                    padding=\\\'VALID\\\', scope=\\\'Conv2d_1a_3x3\\\')\\n-    with tf.variable_scope(\\\'Branch_3\\\'):\\n-        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\\\'VALID\\\',\\n-                                     scope=\\\'MaxPool_1a_3x3\\\')\\n-    net = tf.concat([tower_conv_1, tower_conv1_1,\\n-                        tower_conv2_2, tower_pool], 3)\\n-    return net\\n-  \\n-def inference(images, keep_probability, phase_train=True, \\n-              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\\n-    batch_norm_params = {\\n-        # Decay for the moving averages.\\n-        \\\'decay\\\': 0.995,\\n-        # epsilon to prevent 0s in variance.\\n-        \\\'epsilon\\\': 0.001,\\n-        # force in-place updates of mean and variance estimates\\n-        \\\'updates_collections\\\': None,\\n-        # Moving averages ends up in the trainable variables collection\\n-        \\\'variables_collections\\\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\\n-    }\\n-    \\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\\n-                        weights_initializer=slim.initializers.xavier_initializer(), \\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\\n-                        normalizer_fn=slim.batch_norm,\\n-                        normalizer_params=batch_norm_params):\\n-        return inception_resnet_v1(images, is_training=phase_train,\\n-              dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size, reuse=reuse)\\n-\\n-\\n-def inception_resnet_v1(inputs, is_training=True,\\n-                        dropout_keep_prob=0.8,\\n-                        bottleneck_layer_size=128,\\n-                        reuse=None, \\n-                        scope=\\\'InceptionResnetV1\\\'):\\n-    """Creates the Inception Resnet V1 model.\\n-    Args:\\n-      inputs: a 4-D tensor of size [batch_size, height, width, 3].\\n-      num_classes: number of predicted classes.\\n-      is_training: whether is training or not.\\n-      dropout_keep_prob: float, the fraction to keep before final layer.\\n-      reuse: whether or not the network and its variables should be reused. To be\\n-        able to reuse \\\'scope\\\' must be given.\\n-      scope: Optional variable_scope.\\n-    Returns:\\n-      logits: the logits outputs of the model.\\n-      end_points: the set of end_points from the inception model.\\n-    """\\n-    end_points = {}\\n-  \\n-    with tf.variable_scope(scope, \\\'InceptionResnetV1\\\', [inputs], reuse=reuse):\\n-        with slim.arg_scope([slim.batch_norm, slim.dropout],\\n-                            is_training=is_training):\\n-            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\\n-                                stride=1, padding=\\\'SAME\\\'):\\n-      \\n-                # 149 x 149 x 32\\n-                net = slim.conv2d(inputs, 32, 3, stride=2, padding=\\\'VALID\\\',\\n-                                  scope=\\\'Conv2d_1a_3x3\\\')\\n-                end_points[\\\'Conv2d_1a_3x3\\\'] = net\\n-                # 147 x 147 x 32\\n-                net = slim.conv2d(net, 32, 3, padding=\\\'VALID\\\',\\n-                                  scope=\\\'Conv2d_2a_3x3\\\')\\n-                end_points[\\\'Conv2d_2a_3x3\\\'] = net\\n-                # 147 x 147 x 64\\n-                net = slim.conv2d(net, 64, 3, scope=\\\'Conv2d_2b_3x3\\\')\\n-                end_points[\\\'Conv2d_2b_3x3\\\'] = net\\n-                # 73 x 73 x 64\\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\\\'VALID\\\',\\n-                                      scope=\\\'MaxPool_3a_3x3\\\')\\n-                end_points[\\\'MaxPool_3a_3x3\\\'] = net\\n-                # 73 x 73 x 80\\n-                net = slim.conv2d(net, 80, 1, padding=\\\'VALID\\\',\\n-                                  scope=\\\'Conv2d_3b_1x1\\\')\\n-                end_points[\\\'Conv2d_3b_1x1\\\'] = net\\n-                # 71 x 71 x 192\\n-                net = slim.conv2d(net, 192, 3, padding=\\\'VALID\\\',\\n-                                  scope=\\\'Conv2d_4a_3x3\\\')\\n-                end_points[\\\'Conv2d_4a_3x3\\\'] = net\\n-                # 35 x 35 x 256\\n-                net = slim.conv2d(net, 256, 3, stride=2, padding=\\\'VALID\\\',\\n-                                  scope=\\\'Conv2d_4b_3x3\\\')\\n-                end_points[\\\'Conv2d_4b_3x3\\\'] = net\\n-                \\n-                # 5 x Inception-resnet-A\\n-                net = slim.repeat(net, 5, block35, scale=0.17)\\n-                end_points[\\\'Mixed_5a\\\'] = net\\n-        \\n-                # Reduction-A\\n-                with tf.variable_scope(\\\'Mixed_6a\\\'):\\n-                    net = reduction_a(net, 192, 192, 256, 384)\\n-                end_points[\\\'Mixed_6a\\\'] = net\\n-                \\n-                # 10 x Inception-Resnet-B\\n-                net = slim.repeat(net, 10, block17, scale=0.10)\\n-                end_points[\\\'Mixed_6b\\\'] = net\\n-                \\n-                # Reduction-B\\n-                with tf.variable_scope(\\\'Mixed_7a\\\'):\\n-                    net = reduction_b(net)\\n-                end_points[\\\'Mixed_7a\\\'] = net\\n-                \\n-                # 5 x Inception-Resnet-C\\n-                net = slim.repeat(net, 5, block8, scale=0.20)\\n-                end_points[\\\'Mixed_8a\\\'] = net\\n-                \\n-                net = block8(net, activation_fn=None)\\n-                end_points[\\\'Mixed_8b\\\'] = net\\n-                \\n-                with tf.variable_scope(\\\'Logits\\\'):\\n-                    end_points[\\\'PrePool\\\'] = net\\n-                    #pylint: disable=no-member\\n-                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\\\'VALID\\\',\\n-                                          scope=\\\'AvgPool_1a_8x8\\\')\\n-                    net = slim.flatten(net)\\n-          \\n-                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\\n-                                       scope=\\\'Dropout\\\')\\n-          \\n-                    end_points[\\\'PreLogitsFlatten\\\'] = net\\n-                \\n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \\n-                        scope=\\\'Bottleneck\\\', reuse=False)\\n-  \\n-    return net, end_points\\ndiff --git a/src/models/inception_resnet_v2.py b/src/models/inception_resnet_v2.py\\ndeleted file mode 100644\\nindex 0fb176f..0000000\\n--- a/src/models/inception_resnet_v2.py\\n+++ /dev/null\\n@@ -1,255 +0,0 @@\\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\\n-#\\n-# Licensed under the Apache License, Version 2.0 (the "License");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-# http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an "AS IS" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-# ==============================================================================\\n-\\n-"""Contains the definition of the Inception Resnet V2 architecture.\\n-As described in http://arxiv.org/abs/1602.07261.\\n-  Inception-v4, Inception-ResNet and the Impact of Residual Connections\\n-    on Learning\\n-  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\\n-"""\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-import tensorflow as tf\\n-import tensorflow.contrib.slim as slim\\n-\\n-# Inception-Resnet-A\\n-def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\n-    """Builds the 35x35 resnet block."""\\n-    with tf.variable_scope(scope, \\\'Block35\\\', [net], reuse=reuse):\\n-        with tf.variable_scope(\\\'Branch_0\\\'):\\n-            tower_conv = slim.conv2d(net, 32, 1, scope=\\\'Conv2d_1x1\\\')\\n-        with tf.variable_scope(\\\'Branch_1\\\'):\\n-            tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\\\'Conv2d_0b_3x3\\\')\\n-        with tf.variable_scope(\\\'Branch_2\\\'):\\n-            tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-            tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\\\'Conv2d_0b_3x3\\\')\\n-            tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\\\'Conv2d_0c_3x3\\\')\\n-        mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\n-                         activation_fn=None, scope=\\\'Conv2d_1x1\\\')\\n-        net += scale * up\\n-        if activation_fn:\\n-            net = activation_fn(net)\\n-    return net\\n-\\n-# Inception-Resnet-B\\n-def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\n-    """Builds the 17x17 resnet block."""\\n-    with tf.variable_scope(scope, \\\'Block17\\\', [net], reuse=reuse):\\n-        with tf.variable_scope(\\\'Branch_0\\\'):\\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\\\'Conv2d_1x1\\\')\\n-        with tf.variable_scope(\\\'Branch_1\\\'):\\n-            tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\\n-                                        scope=\\\'Conv2d_0b_1x7\\\')\\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\\n-                                        scope=\\\'Conv2d_0c_7x1\\\')\\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\n-                         activation_fn=None, scope=\\\'Conv2d_1x1\\\')\\n-        net += scale * up\\n-        if activation_fn:\\n-            net = activation_fn(net)\\n-    return net\\n-\\n-\\n-# Inception-Resnet-C\\n-def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\\n-    """Builds the 8x8 resnet block."""\\n-    with tf.variable_scope(scope, \\\'Block8\\\', [net], reuse=reuse):\\n-        with tf.variable_scope(\\\'Branch_0\\\'):\\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\\\'Conv2d_1x1\\\')\\n-        with tf.variable_scope(\\\'Branch_1\\\'):\\n-            tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\\n-                                        scope=\\\'Conv2d_0b_1x3\\\')\\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\\n-                                        scope=\\\'Conv2d_0c_3x1\\\')\\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\\n-                         activation_fn=None, scope=\\\'Conv2d_1x1\\\')\\n-        net += scale * up\\n-        if activation_fn:\\n-            net = activation_fn(net)\\n-    return net\\n-  \\n-def inference(images, keep_probability, phase_train=True, \\n-              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\\n-    batch_norm_params = {\\n-        # Decay for the moving averages.\\n-        \\\'decay\\\': 0.995,\\n-        # epsilon to prevent 0s in variance.\\n-        \\\'epsilon\\\': 0.001,\\n-        # force in-place updates of mean and variance estimates\\n-        \\\'updates_collections\\\': None,\\n-        # Moving averages ends up in the trainable variables collection\\n-        \\\'variables_collections\\\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\\n-}\\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\\n-                        weights_initializer=slim.initializers.xavier_initializer(), \\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\\n-                        normalizer_fn=slim.batch_norm,\\n-                        normalizer_params=batch_norm_params):\\n-        return inception_resnet_v2(images, is_training=phase_train,\\n-              dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size, reuse=reuse)\\n-\\n-\\n-def inception_resnet_v2(inputs, is_training=True,\\n-                        dropout_keep_prob=0.8,\\n-                        bottleneck_layer_size=128,\\n-                        reuse=None,\\n-                        scope=\\\'InceptionResnetV2\\\'):\\n-    """Creates the Inception Resnet V2 model.\\n-    Args:\\n-      inputs: a 4-D tensor of size [batch_size, height, width, 3].\\n-      num_classes: number of predicted classes.\\n-      is_training: whether is training or not.\\n-      dropout_keep_prob: float, the fraction to keep before final layer.\\n-      reuse: whether or not the network and its variables should be reused. To be\\n-        able to reuse \\\'scope\\\' must be given.\\n-      scope: Optional variable_scope.\\n-    Returns:\\n-      logits: the logits outputs of the model.\\n-      end_points: the set of end_points from the inception model.\\n-    """\\n-    end_points = {}\\n-  \\n-    with tf.variable_scope(scope, \\\'InceptionResnetV2\\\', [inputs], reuse=reuse):\\n-        with slim.arg_scope([slim.batch_norm, slim.dropout],\\n-                            is_training=is_training):\\n-            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\\n-                                stride=1, padding=\\\'SAME\\\'):\\n-      \\n-                # 149 x 149 x 32\\n-                net = slim.conv2d(inputs, 32, 3, stride=2, padding=\\\'VALID\\\',\\n-                                  scope=\\\'Conv2d_1a_3x3\\\')\\n-                end_points[\\\'Conv2d_1a_3x3\\\'] = net\\n-                # 147 x 147 x 32\\n-                net = slim.conv2d(net, 32, 3, padding=\\\'VALID\\\',\\n-                                  scope=\\\'Conv2d_2a_3x3\\\')\\n-                end_points[\\\'Conv2d_2a_3x3\\\'] = net\\n-                # 147 x 147 x 64\\n-                net = slim.conv2d(net, 64, 3, scope=\\\'Conv2d_2b_3x3\\\')\\n-                end_points[\\\'Conv2d_2b_3x3\\\'] = net\\n-                # 73 x 73 x 64\\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\\\'VALID\\\',\\n-                                      scope=\\\'MaxPool_3a_3x3\\\')\\n-                end_points[\\\'MaxPool_3a_3x3\\\'] = net\\n-                # 73 x 73 x 80\\n-                net = slim.conv2d(net, 80, 1, padding=\\\'VALID\\\',\\n-                                  scope=\\\'Conv2d_3b_1x1\\\')\\n-                end_points[\\\'Conv2d_3b_1x1\\\'] = net\\n-                # 71 x 71 x 192\\n-                net = slim.conv2d(net, 192, 3, padding=\\\'VALID\\\',\\n-                                  scope=\\\'Conv2d_4a_3x3\\\')\\n-                end_points[\\\'Conv2d_4a_3x3\\\'] = net\\n-                # 35 x 35 x 192\\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\\\'VALID\\\',\\n-                                      scope=\\\'MaxPool_5a_3x3\\\')\\n-                end_points[\\\'MaxPool_5a_3x3\\\'] = net\\n-        \\n-                # 35 x 35 x 320\\n-                with tf.variable_scope(\\\'Mixed_5b\\\'):\\n-                    with tf.variable_scope(\\\'Branch_0\\\'):\\n-                        tower_conv = slim.conv2d(net, 96, 1, scope=\\\'Conv2d_1x1\\\')\\n-                    with tf.variable_scope(\\\'Branch_1\\\'):\\n-                        tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-                        tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\\n-                                                    scope=\\\'Conv2d_0b_5x5\\\')\\n-                    with tf.variable_scope(\\\'Branch_2\\\'):\\n-                        tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-                        tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\\n-                                                    scope=\\\'Conv2d_0b_3x3\\\')\\n-                        tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\\n-                                                    scope=\\\'Conv2d_0c_3x3\\\')\\n-                    with tf.variable_scope(\\\'Branch_3\\\'):\\n-                        tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\\\'SAME\\\',\\n-                                                     scope=\\\'AvgPool_0a_3x3\\\')\\n-                        tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\\n-                                                   scope=\\\'Conv2d_0b_1x1\\\')\\n-                    net = tf.concat([tower_conv, tower_conv1_1,\\n-                                        tower_conv2_2, tower_pool_1], 3)\\n-        \\n-                end_points[\\\'Mixed_5b\\\'] = net\\n-                net = slim.repeat(net, 10, block35, scale=0.17)\\n-        \\n-                # 17 x 17 x 1024\\n-                with tf.variable_scope(\\\'Mixed_6a\\\'):\\n-                    with tf.variable_scope(\\\'Branch_0\\\'):\\n-                        tower_conv = slim.conv2d(net, 384, 3, stride=2, padding=\\\'VALID\\\',\\n-                                                 scope=\\\'Conv2d_1a_3x3\\\')\\n-                    with tf.variable_scope(\\\'Branch_1\\\'):\\n-                        tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-                        tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\\n-                                                    scope=\\\'Conv2d_0b_3x3\\\')\\n-                        tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\\n-                                                    stride=2, padding=\\\'VALID\\\',\\n-                                                    scope=\\\'Conv2d_1a_3x3\\\')\\n-                    with tf.variable_scope(\\\'Branch_2\\\'):\\n-                        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\\\'VALID\\\',\\n-                                                     scope=\\\'MaxPool_1a_3x3\\\')\\n-                    net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\\n-        \\n-                end_points[\\\'Mixed_6a\\\'] = net\\n-                net = slim.repeat(net, 20, block17, scale=0.10)\\n-        \\n-                with tf.variable_scope(\\\'Mixed_7a\\\'):\\n-                    with tf.variable_scope(\\\'Branch_0\\\'):\\n-                        tower_conv = slim.conv2d(net, 256, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-                        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\\n-                                                   padding=\\\'VALID\\\', scope=\\\'Conv2d_1a_3x3\\\')\\n-                    with tf.variable_scope(\\\'Branch_1\\\'):\\n-                        tower_conv1 = slim.conv2d(net, 256, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-                        tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\\n-                                                    padding=\\\'VALID\\\', scope=\\\'Conv2d_1a_3x3\\\')\\n-                    with tf.variable_scope(\\\'Branch_2\\\'):\\n-                        tower_conv2 = slim.conv2d(net, 256, 1, scope=\\\'Conv2d_0a_1x1\\\')\\n-                        tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\\n-                                                    scope=\\\'Conv2d_0b_3x3\\\')\\n-                        tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\\n-                                                    padding=\\\'VALID\\\', scope=\\\'Conv2d_1a_3x3\\\')\\n-                    with tf.variable_scope(\\\'Branch_3\\\'):\\n-                        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\\\'VALID\\\',\\n-                                                     scope=\\\'MaxPool_1a_3x3\\\')\\n-                    net = tf.concat([tower_conv_1, tower_conv1_1,\\n-                                        tower_conv2_2, tower_pool], 3)\\n-        \\n-                end_points[\\\'Mixed_7a\\\'] = net\\n-        \\n-                net = slim.repeat(net, 9, block8, scale=0.20)\\n-                net = block8(net, activation_fn=None)\\n-        \\n-                net = slim.conv2d(net, 1536, 1, scope=\\\'Conv2d_7b_1x1\\\')\\n-                end_points[\\\'Conv2d_7b_1x1\\\'] = net\\n-        \\n-                with tf.variable_scope(\\\'Logits\\\'):\\n-                    end_points[\\\'PrePool\\\'] = net\\n-                    #pylint: disable=no-member\\n-                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\\\'VALID\\\',\\n-                                          scope=\\\'AvgPool_1a_8x8\\\')\\n-                    net = slim.flatten(net)\\n-          \\n-                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\\n-                                       scope=\\\'Dropout\\\')\\n-          \\n-                    end_points[\\\'PreLogitsFlatten\\\'] = net\\n-                \\n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \\n-                        scope=\\\'Bottleneck\\\', reuse=False)\\n-  \\n-    return net, end_points\\ndiff --git a/src/models/squeezenet.py b/src/models/squeezenet.py\\ndeleted file mode 100644\\nindex ae117e1..0000000\\n--- a/src/models/squeezenet.py\\n+++ /dev/null\\n@@ -1,67 +0,0 @@\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-import tensorflow as tf\\n-import tensorflow.contrib.slim as slim\\n-\\n-def fire_module(inputs,\\n-                squeeze_depth,\\n-                expand_depth,\\n-                reuse=None,\\n-                scope=None,\\n-                outputs_collections=None):\\n-    with tf.variable_scope(scope, \\\'fire\\\', [inputs], reuse=reuse):\\n-        with slim.arg_scope([slim.conv2d, slim.max_pool2d],\\n-                            outputs_collections=None):\\n-            net = squeeze(inputs, squeeze_depth)\\n-            outputs = expand(net, expand_depth)\\n-            return outputs\\n-\\n-def squeeze(inputs, num_outputs):\\n-    return slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope=\\\'squeeze\\\')\\n-\\n-def expand(inputs, num_outputs):\\n-    with tf.variable_scope(\\\'expand\\\'):\\n-        e1x1 = slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope=\\\'1x1\\\')\\n-        e3x3 = slim.conv2d(inputs, num_outputs, [3, 3], scope=\\\'3x3\\\')\\n-    return tf.concat([e1x1, e3x3], 3)\\n-\\n-def inference(images, keep_probability, phase_train=True, bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\\n-    batch_norm_params = {\\n-        # Decay for the moving averages.\\n-        \\\'decay\\\': 0.995,\\n-        # epsilon to prevent 0s in variance.\\n-        \\\'epsilon\\\': 0.001,\\n-        # force in-place updates of mean and variance estimates\\n-        \\\'updates_collections\\\': None,\\n-        # Moving averages ends up in the trainable variables collection\\n-        \\\'variables_collections\\\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\\n-    }\\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\\n-                        weights_initializer=slim.xavier_initializer_conv2d(uniform=True),\\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\\n-                        normalizer_fn=slim.batch_norm,\\n-                        normalizer_params=batch_norm_params):\\n-        with tf.variable_scope(\\\'squeezenet\\\', [images], reuse=reuse):\\n-            with slim.arg_scope([slim.batch_norm, slim.dropout],\\n-                                is_training=phase_train):\\n-                net = slim.conv2d(images, 96, [7, 7], stride=2, scope=\\\'conv1\\\')\\n-                net = slim.max_pool2d(net, [3, 3], stride=2, scope=\\\'maxpool1\\\')\\n-                net = fire_module(net, 16, 64, scope=\\\'fire2\\\')\\n-                net = fire_module(net, 16, 64, scope=\\\'fire3\\\')\\n-                net = fire_module(net, 32, 128, scope=\\\'fire4\\\')\\n-                net = slim.max_pool2d(net, [2, 2], stride=2, scope=\\\'maxpool4\\\')\\n-                net = fire_module(net, 32, 128, scope=\\\'fire5\\\')\\n-                net = fire_module(net, 48, 192, scope=\\\'fire6\\\')\\n-                net = fire_module(net, 48, 192, scope=\\\'fire7\\\')\\n-                net = fire_module(net, 64, 256, scope=\\\'fire8\\\')\\n-                net = slim.max_pool2d(net, [3, 3], stride=2, scope=\\\'maxpool8\\\')\\n-                net = fire_module(net, 64, 256, scope=\\\'fire9\\\')\\n-                net = slim.dropout(net, keep_probability)\\n-                net = slim.conv2d(net, 1000, [1, 1], activation_fn=None, normalizer_fn=None, scope=\\\'conv10\\\')\\n-                net = slim.avg_pool2d(net, net.get_shape()[1:3], scope=\\\'avgpool10\\\')\\n-                net = tf.squeeze(net, [1, 2], name=\\\'logits\\\')\\n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \\n-                        scope=\\\'Bottleneck\\\', reuse=False)\\n-    return net, None\'\n\\ No newline at end of file\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_54_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_54_Pro.jpg\ndeleted file mode 100644\nindex 132948b..0000000\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_54_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_55_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_55_Pro.jpg\ndeleted file mode 100644\nindex d589b1a..0000000\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_55_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro (2).jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro (2).jpg\ndeleted file mode 100644\nindex d47e20d..0000000\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro.jpg\ndeleted file mode 100644\nindex fbe6de7..0000000\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_56_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro (2).jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro (2).jpg\ndeleted file mode 100644\nindex 3e5846d..0000000\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro.jpg\ndeleted file mode 100644\nindex b23bcfa..0000000\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_57_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro (2).jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro (2).jpg\ndeleted file mode 100644\nindex bb832e9..0000000\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro.jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro.jpg\ndeleted file mode 100644\nindex a0b4312..0000000\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_58_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_59_Pro (2).jpg b/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_59_Pro (2).jpg\ndeleted file mode 100644\nindex 928f751..0000000\nBinary files a/Dataset/FaceData/raw/Quang/WIN_20250411_21_48_59_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/raw/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg b/Dataset/FaceData/raw/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg\ndeleted file mode 100644\nindex 6537490..0000000\nBinary files a/Dataset/FaceData/raw/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg and /dev/null differ\ndiff --git a/Dataset/FaceData/raw/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg b/Dataset/FaceData/raw/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg\ndeleted file mode 100644\nindex 8f6b0f9..0000000\nBinary files a/Dataset/FaceData/raw/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg and /dev/null differ\ndiff --git a/Dataset/FaceData/raw/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg b/Dataset/FaceData/raw/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg\ndeleted file mode 100644\nindex f74031f..0000000\nBinary files a/Dataset/FaceData/raw/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.jpeg b/Dataset/FaceData/test/data/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.jpeg\ndeleted file mode 100644\nindex b5ebe28..0000000\nBinary files a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.jpeg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.jpeg b/Dataset/FaceData/test/data/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.jpeg\ndeleted file mode 100644\nindex af9bfea..0000000\nBinary files a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.jpeg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.jpeg b/Dataset/FaceData/test/data/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.jpeg\ndeleted file mode 100644\nindex 28c29dc..0000000\nBinary files a/Dataset/FaceData/test/data/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.jpeg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.jpeg b/Dataset/FaceData/test/data/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.jpeg\ndeleted file mode 100644\nindex 979d22b..0000000\nBinary files a/Dataset/FaceData/test/data/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.jpeg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.jpeg b/Dataset/FaceData/test/data/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.jpeg\ndeleted file mode 100644\nindex 95a477c..0000000\nBinary files a/Dataset/FaceData/test/data/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.jpeg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.jpeg b/Dataset/FaceData/test/data/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.jpeg\ndeleted file mode 100644\nindex 208ec5c..0000000\nBinary files a/Dataset/FaceData/test/data/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.jpeg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/ChuQuangHuy/IMG_0349.HEIC b/Dataset/FaceData/test/data/ChuQuangHuy/IMG_0349.HEIC\ndeleted file mode 100644\nindex 345acbc..0000000\nBinary files a/Dataset/FaceData/test/data/ChuQuangHuy/IMG_0349.HEIC and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.jpg b/Dataset/FaceData/test/data/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.jpg\ndeleted file mode 100644\nindex b038c0c..0000000\nBinary files a/Dataset/FaceData/test/data/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.jpg b/Dataset/FaceData/test/data/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.jpg\ndeleted file mode 100644\nindex e3de1ee..0000000\nBinary files a/Dataset/FaceData/test/data/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro (3).jpg\ndeleted file mode 100644\nindex 967a588..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro (3).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro.jpg\ndeleted file mode 100644\nindex c4e9ec9..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_31_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro (2).jpg\ndeleted file mode 100644\nindex ab17f1f..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro.jpg\ndeleted file mode 100644\nindex e9e3371..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_32_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (2).jpg\ndeleted file mode 100644\nindex dcd5b17..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (3).jpg\ndeleted file mode 100644\nindex bdf69fe..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro (3).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro.jpg\ndeleted file mode 100644\nindex e5e0d42..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_33_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (2).jpg\ndeleted file mode 100644\nindex cf7525e..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (3).jpg\ndeleted file mode 100644\nindex f22e4b7..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro (3).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro.jpg\ndeleted file mode 100644\nindex 56ac445..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_34_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro (2).jpg\ndeleted file mode 100644\nindex a72c6b8..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro.jpg\ndeleted file mode 100644\nindex e4e969a..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_35_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_36_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_36_Pro.jpg\ndeleted file mode 100644\nindex 94d80b4..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_07_36_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro (2).jpg\ndeleted file mode 100644\nindex ef3d4f9..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro.jpg\ndeleted file mode 100644\nindex d7333e8..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_12_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (2).jpg\ndeleted file mode 100644\nindex f8bec2a..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (3).jpg\ndeleted file mode 100644\nindex 42fdad1..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro (3).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro.jpg\ndeleted file mode 100644\nindex 9851605..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_13_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (2).jpg\ndeleted file mode 100644\nindex 63fd876..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (3).jpg\ndeleted file mode 100644\nindex 90c377d..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro (3).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro.jpg\ndeleted file mode 100644\nindex f669175..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_14_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (2).jpg\ndeleted file mode 100644\nindex 5ba014d..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (3).jpg\ndeleted file mode 100644\nindex 17c4b93..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro (3).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro.jpg\ndeleted file mode 100644\nindex 7c91590..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_15_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro (2).jpg\ndeleted file mode 100644\nindex 16d41be..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro.jpg\ndeleted file mode 100644\nindex 4271bdd..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_16_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (2).jpg\ndeleted file mode 100644\nindex 0646f0b..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (3).jpg\ndeleted file mode 100644\nindex eeb4397..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro (3).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro.jpg\ndeleted file mode 100644\nindex dba3891..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_17_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (2).jpg\ndeleted file mode 100644\nindex b2c8e83..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (3).jpg\ndeleted file mode 100644\nindex a6e8ebf..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro (3).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro.jpg\ndeleted file mode 100644\nindex d163084..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_18_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro (2).jpg\ndeleted file mode 100644\nindex 38971bf..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro.jpg\ndeleted file mode 100644\nindex 5d9bd5c..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_19_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (2).jpg\ndeleted file mode 100644\nindex 9903ab8..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (3).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (3).jpg\ndeleted file mode 100644\nindex 5f8c977..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro (3).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro.jpg\ndeleted file mode 100644\nindex 6db75f8..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_20_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro (2).jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro (2).jpg\ndeleted file mode 100644\nindex 2daaf72..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro.jpg\ndeleted file mode 100644\nindex e84c735..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_21_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_22_Pro.jpg b/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_22_Pro.jpg\ndeleted file mode 100644\nindex 7888261..0000000\nBinary files a/Dataset/FaceData/test/data/LVQuang/WIN_20250331_09_26_22_Pro.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_0327.JPG b/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_0327.JPG\ndeleted file mode 100644\nindex 9f03d5d..0000000\nBinary files a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_0327.JPG and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092236.jpg b/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092236.jpg\ndeleted file mode 100644\nindex e587b19..0000000\nBinary files a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092236.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092237.jpg b/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092237.jpg\ndeleted file mode 100644\nindex cf24b36..0000000\nBinary files a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092237.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092241.jpg b/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092241.jpg\ndeleted file mode 100644\nindex a505a8a..0000000\nBinary files a/Dataset/FaceData/test/data/NghiemAnhHieu/IMG_20250327_092241.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/NgocKem/IMG_1742972407559.jpg b/Dataset/FaceData/test/data/NgocKem/IMG_1742972407559.jpg\ndeleted file mode 100644\nindex 0483ff8..0000000\nBinary files a/Dataset/FaceData/test/data/NgocKem/IMG_1742972407559.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/NgocKem/IMG_1742972428266.jpg b/Dataset/FaceData/test/data/NgocKem/IMG_1742972428266.jpg\ndeleted file mode 100644\nindex 96d77fc..0000000\nBinary files a/Dataset/FaceData/test/data/NgocKem/IMG_1742972428266.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/NgocKem/IMG_1742972431298.jpg b/Dataset/FaceData/test/data/NgocKem/IMG_1742972431298.jpg\ndeleted file mode 100644\nindex bd4c93c..0000000\nBinary files a/Dataset/FaceData/test/data/NgocKem/IMG_1742972431298.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.jpeg b/Dataset/FaceData/test/data/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.jpeg\ndeleted file mode 100644\nindex 6aba153..0000000\nBinary files a/Dataset/FaceData/test/data/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.jpeg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/Phuong/Messenger_creation_7343AEA7-71DD-40DB-B8C3-9D6F0CED60F9.jpeg b/Dataset/FaceData/test/data/Phuong/Messenger_creation_7343AEA7-71DD-40DB-B8C3-9D6F0CED60F9.jpeg\ndeleted file mode 100644\nindex d41ca36..0000000\nBinary files a/Dataset/FaceData/test/data/Phuong/Messenger_creation_7343AEA7-71DD-40DB-B8C3-9D6F0CED60F9.jpeg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.jpeg b/Dataset/FaceData/test/data/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.jpeg\ndeleted file mode 100644\nindex 161aa4b..0000000\nBinary files a/Dataset/FaceData/test/data/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.jpeg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173023.jpg b/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173023.jpg\ndeleted file mode 100644\nindex 4982c4f..0000000\nBinary files a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173023.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173025.jpg b/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173025.jpg\ndeleted file mode 100644\nindex 85c8a30..0000000\nBinary files a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173025.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173026.jpg b/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173026.jpg\ndeleted file mode 100644\nindex a18bfd6..0000000\nBinary files a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173026.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173028.jpg b/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173028.jpg\ndeleted file mode 100644\nindex 5b6d155..0000000\nBinary files a/Dataset/FaceData/test/data/ThaoNguyen/IMG_20250325_173028.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.jpg b/Dataset/FaceData/test/data/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.jpg\ndeleted file mode 100644\nindex c688251..0000000\nBinary files a/Dataset/FaceData/test/data/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/TranDangHieu/download (1).jpg b/Dataset/FaceData/test/data/TranDangHieu/download (1).jpg\ndeleted file mode 100644\nindex e0c8116..0000000\nBinary files a/Dataset/FaceData/test/data/TranDangHieu/download (1).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/TranDangHieu/download (2).jpg b/Dataset/FaceData/test/data/TranDangHieu/download (2).jpg\ndeleted file mode 100644\nindex ef5f43d..0000000\nBinary files a/Dataset/FaceData/test/data/TranDangHieu/download (2).jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/TranDangHieu/download.jpg b/Dataset/FaceData/test/data/TranDangHieu/download.jpg\ndeleted file mode 100644\nindex 3d6e0fb..0000000\nBinary files a/Dataset/FaceData/test/data/TranDangHieu/download.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134208.jpg b/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134208.jpg\ndeleted file mode 100644\nindex 104e453..0000000\nBinary files a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134208.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134210.jpg b/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134210.jpg\ndeleted file mode 100644\nindex c3ee227..0000000\nBinary files a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134210.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134211.jpg b/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134211.jpg\ndeleted file mode 100644\nindex 77cc3f0..0000000\nBinary files a/Dataset/FaceData/test/data/TrangNhung/IMG_20250326_134211.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png b/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png\ndeleted file mode 100644\nindex 68c44d6..0000000\nBinary files a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png b/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png\ndeleted file mode 100644\nindex f2dc8a1..0000000\nBinary files a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png b/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png\ndeleted file mode 100644\nindex 9b27a7d..0000000\nBinary files a/Dataset/FaceData/test/data/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135142.jpg b/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135142.jpg\ndeleted file mode 100644\nindex 5b6bf55..0000000\nBinary files a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135142.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135145.jpg b/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135145.jpg\ndeleted file mode 100644\nindex 46005cc..0000000\nBinary files a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135145.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135147.jpg b/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135147.jpg\ndeleted file mode 100644\nindex 0c45781..0000000\nBinary files a/Dataset/FaceData/test/data/TuSenna/IMG_20250326_135147.jpg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg b/Dataset/FaceData/test/data/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg\ndeleted file mode 100644\nindex 6537490..0000000\nBinary files a/Dataset/FaceData/test/data/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.jpeg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg b/Dataset/FaceData/test/data/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg\ndeleted file mode 100644\nindex 8f6b0f9..0000000\nBinary files a/Dataset/FaceData/test/data/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.jpeg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg b/Dataset/FaceData/test/data/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg\ndeleted file mode 100644\nindex f74031f..0000000\nBinary files a/Dataset/FaceData/test/data/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.jpeg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.jpeg b/Dataset/FaceData/test/data/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.jpeg\ndeleted file mode 100644\nindex 7b62bc3..0000000\nBinary files a/Dataset/FaceData/test/data/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.jpeg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.jpeg b/Dataset/FaceData/test/data/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.jpeg\ndeleted file mode 100644\nindex 1be1f19..0000000\nBinary files a/Dataset/FaceData/test/data/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.jpeg and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.png b/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.png\ndeleted file mode 100644\nindex 87978ce..0000000\nBinary files a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.png b/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.png\ndeleted file mode 100644\nindex 5269259..0000000\nBinary files a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.png b/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.png\ndeleted file mode 100644\nindex cd8c24c..0000000\nBinary files a/Dataset/FaceData/test/data/txl/AnhTu/Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.png b/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.png\ndeleted file mode 100644\nindex 0650f16..0000000\nBinary files a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.png b/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.png\ndeleted file mode 100644\nindex 714bc33..0000000\nBinary files a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.png b/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.png\ndeleted file mode 100644\nindex 9b3ac40..0000000\nBinary files a/Dataset/FaceData/test/data/txl/BotIu/Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/485290655_1069958524968534_6128511440722701069_n.png b/Dataset/FaceData/test/data/txl/BuiCongSon/485290655_1069958524968534_6128511440722701069_n.png\ndeleted file mode 100644\nindex cd86dc1..0000000\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/485290655_1069958524968534_6128511440722701069_n.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/485424995_1386922852752466_129752981138586263_n.png b/Dataset/FaceData/test/data/txl/BuiCongSon/485424995_1386922852752466_129752981138586263_n.png\ndeleted file mode 100644\nindex 90013f6..0000000\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/485424995_1386922852752466_129752981138586263_n.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/485464001_1219012669643907_2610181536763391909_n.png b/Dataset/FaceData/test/data/txl/BuiCongSon/485464001_1219012669643907_2610181536763391909_n.png\ndeleted file mode 100644\nindex 31c9d0c..0000000\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/485464001_1219012669643907_2610181536763391909_n.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/485512577_2113779945729669_160018376255303225_n.png b/Dataset/FaceData/test/data/txl/BuiCongSon/485512577_2113779945729669_160018376255303225_n.png\ndeleted file mode 100644\nindex efbe736..0000000\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/485512577_2113779945729669_160018376255303225_n.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/BuiCongSon/z6377532752047_4b7b3c7d460239f92fa3cec037738eca.png b/Dataset/FaceData/test/data/txl/BuiCongSon/z6377532752047_4b7b3c7d460239f92fa3cec037738eca.png\ndeleted file mode 100644\nindex 09d25b3..0000000\nBinary files a/Dataset/FaceData/test/data/txl/BuiCongSon/z6377532752047_4b7b3c7d460239f92fa3cec037738eca.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151626.png b/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151626.png\ndeleted file mode 100644\nindex 66a927d..0000000\nBinary files a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151626.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151720.png b/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151720.png\ndeleted file mode 100644\nindex 5554b2f..0000000\nBinary files a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151720.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151731.png b/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151731.png\ndeleted file mode 100644\nindex b92b066..0000000\nBinary files a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151731.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151738.png b/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151738.png\ndeleted file mode 100644\nindex 852ab42..0000000\nBinary files a/Dataset/FaceData/test/data/txl/Emma/IMG_20250326_151738.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.png b/Dataset/FaceData/test/data/txl/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.png\ndeleted file mode 100644\nindex f6ad820..0000000\nBinary files a/Dataset/FaceData/test/data/txl/Jack/2209C349-3B30-47AC-89C1-1152E1A42FFC.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.png b/Dataset/FaceData/test/data/txl/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.png\ndeleted file mode 100644\nindex 8021733..0000000\nBinary files a/Dataset/FaceData/test/data/txl/Jack/A3FED369-0E1F-4064-830E-FBFD69D77C85.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_1742972296094.png b/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_1742972296094.png\ndeleted file mode 100644\nindex e7677d6..0000000\nBinary files a/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_1742972296094.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_20250326_134739.png b/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_20250326_134739.png\ndeleted file mode 100644\nindex d8efdad..0000000\nBinary files a/Dataset/FaceData/test/data/txl/LeAnhTrang/IMG_20250326_134739.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_0327.png b/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_0327.png\ndeleted file mode 100644\nindex 1037e8e..0000000\nBinary files a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_0327.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092236.png b/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092236.png\ndeleted file mode 100644\nindex 9be2f92..0000000\nBinary files a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092236.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092237.png b/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092237.png\ndeleted file mode 100644\nindex 17b5d34..0000000\nBinary files a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092237.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092241.png b/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092241.png\ndeleted file mode 100644\nindex 97fcfdb..0000000\nBinary files a/Dataset/FaceData/test/data/txl/NghiemAnhHieu/IMG_20250327_092241.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972407559.png b/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972407559.png\ndeleted file mode 100644\nindex 66c7877..0000000\nBinary files a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972407559.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972428266.png b/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972428266.png\ndeleted file mode 100644\nindex 47db00e..0000000\nBinary files a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972428266.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972431298.png b/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972431298.png\ndeleted file mode 100644\nindex 1bcc4bf..0000000\nBinary files a/Dataset/FaceData/test/data/txl/NgocKem/IMG_1742972431298.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.png b/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.png\ndeleted file mode 100644\nindex 6d2cfbd..0000000\nBinary files a/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.png b/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.png\ndeleted file mode 100644\nindex d1d90a5..0000000\nBinary files a/Dataset/FaceData/test/data/txl/Phuong/Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/Thao/851D4C65-07BE-4220-8488-84C40C1A79D6.png b/Dataset/FaceData/test/data/txl/Thao/851D4C65-07BE-4220-8488-84C40C1A79D6.png\ndeleted file mode 100644\nindex d2170a7..0000000\nBinary files a/Dataset/FaceData/test/data/txl/Thao/851D4C65-07BE-4220-8488-84C40C1A79D6.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/Thao/98E5A551-3DC2-4A85-9497-AFA4A845325A.png b/Dataset/FaceData/test/data/txl/Thao/98E5A551-3DC2-4A85-9497-AFA4A845325A.png\ndeleted file mode 100644\nindex 055d27a..0000000\nBinary files a/Dataset/FaceData/test/data/txl/Thao/98E5A551-3DC2-4A85-9497-AFA4A845325A.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173023.png b/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173023.png\ndeleted file mode 100644\nindex f95fdd9..0000000\nBinary files a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173023.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173025.png b/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173025.png\ndeleted file mode 100644\nindex f2c1813..0000000\nBinary files a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173025.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173026.png b/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173026.png\ndeleted file mode 100644\nindex 2541428..0000000\nBinary files a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173026.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173028.png b/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173028.png\ndeleted file mode 100644\nindex 1eae995..0000000\nBinary files a/Dataset/FaceData/test/data/txl/ThaoNguyen/IMG_20250325_173028.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.png b/Dataset/FaceData/test/data/txl/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.png\ndeleted file mode 100644\nindex 0678197..0000000\nBinary files a/Dataset/FaceData/test/data/txl/ThienAn/att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/TranDangHieu/download (1).png b/Dataset/FaceData/test/data/txl/TranDangHieu/download (1).png\ndeleted file mode 100644\nindex d7a2377..0000000\nBinary files a/Dataset/FaceData/test/data/txl/TranDangHieu/download (1).png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/TranDangHieu/download (2).png b/Dataset/FaceData/test/data/txl/TranDangHieu/download (2).png\ndeleted file mode 100644\nindex da1b935..0000000\nBinary files a/Dataset/FaceData/test/data/txl/TranDangHieu/download (2).png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/TranDangHieu/download.png b/Dataset/FaceData/test/data/txl/TranDangHieu/download.png\ndeleted file mode 100644\nindex 51eee7f..0000000\nBinary files a/Dataset/FaceData/test/data/txl/TranDangHieu/download.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134208.png b/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134208.png\ndeleted file mode 100644\nindex a3ffc1f..0000000\nBinary files a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134208.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134210.png b/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134210.png\ndeleted file mode 100644\nindex 49694a3..0000000\nBinary files a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134210.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134211.png b/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134211.png\ndeleted file mode 100644\nindex 7af9479..0000000\nBinary files a/Dataset/FaceData/test/data/txl/TrangNhung/IMG_20250326_134211.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG1.png b/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG1.png\ndeleted file mode 100644\nindex 4e78809..0000000\nBinary files a/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG1.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG2.png b/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG2.png\ndeleted file mode 100644\nindex 21b37f7..0000000\nBinary files a/Dataset/FaceData/test/data/txl/TrinhHuuTuan/IMG2.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png b/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png\ndeleted file mode 100644\nindex 971746e..0000000\nBinary files a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152730.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png b/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png\ndeleted file mode 100644\nindex 275fcb7..0000000\nBinary files a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152756.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png b/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png\ndeleted file mode 100644\nindex 29af910..0000000\nBinary files a/Dataset/FaceData/test/data/txl/TrinhTranPhuongTuan/Screenshot 2025-03-26 152811.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135142.png b/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135142.png\ndeleted file mode 100644\nindex 0149809..0000000\nBinary files a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135142.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135145.png b/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135145.png\ndeleted file mode 100644\nindex 2a6a9b4..0000000\nBinary files a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135145.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135147.png b/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135147.png\ndeleted file mode 100644\nindex 83a6450..0000000\nBinary files a/Dataset/FaceData/test/data/txl/TuSenna/IMG_20250326_135147.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png b/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png\ndeleted file mode 100644\nindex 8257ab5..0000000\nBinary files a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png b/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png\ndeleted file mode 100644\nindex 45870ff..0000000\nBinary files a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png b/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png\ndeleted file mode 100644\nindex f4593ba..0000000\nBinary files a/Dataset/FaceData/test/data/txl/Viruss/Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.png b/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.png\ndeleted file mode 100644\nindex 942d65d..0000000\nBinary files a/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.png b/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.png\ndeleted file mode 100644\nindex 6872f3e..0000000\nBinary files a/Dataset/FaceData/test/data/txl/Xemesis/Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.png and /dev/null differ\ndiff --git a/Dataset/FaceData/test/data/txl/bounding_boxes_47191.txt b/Dataset/FaceData/test/data/txl/bounding_boxes_47191.txt\ndeleted file mode 100644\nindex d314c07..0000000\n--- a/Dataset/FaceData/test/data/txl/bounding_boxes_47191.txt\n+++ /dev/null\n@@ -1,55 +0,0 @@\n-Dataset/FaceData/processed\\TrangNhung\\IMG_20250326_134210.png 895 349 1544 1272\n-Dataset/FaceData/processed\\TrangNhung\\IMG_20250326_134208.png 535 336 1175 1157\n-Dataset/FaceData/processed\\TrangNhung\\IMG_20250326_134211.png 586 360 1301 1293\n-Dataset/FaceData/processed\\Xemesis\\Messenger_creation_40E43D0C-5583-4E7F-BBAF-F0BD248EE25D.png 348 648 886 1345\n-Dataset/FaceData/processed\\Xemesis\\Messenger_creation_0BB36BA4-463A-4FE8-A1D8-7FBEEAD0218A.png 0 293 772 1203\n-Dataset/FaceData/processed\\Emma\\IMG_20250326_151626.png 967 170 1656 1050\n-Dataset/FaceData/processed\\Emma\\IMG_20250326_151731.png 362 264 1458 1704\n-Dataset/FaceData/processed\\Emma\\IMG_20250326_151738.png 380 538 1544 1985\n-Dataset/FaceData/processed\\Emma\\IMG_20250326_151720.png 417 326 1549 1712\n-Dataset/FaceData/processed\\TrinhHuuTuan\\IMG1.png 199 545 810 1319\n-Dataset/FaceData/processed\\TrinhHuuTuan\\IMG2.png 298 659 907 1438\n-Dataset/FaceData/processed\\BotIu\\Messenger_creation_1EC7F595-3688-43DB-B2E4-1A65CF0802F0.png 589 739 1206 1280\n-Dataset/FaceData/processed\\BotIu\\Messenger_creation_9EA4596D-3B30-4172-9ADE-DFA0E6875DA9.png 338 362 954 1198\n-Dataset/FaceData/processed\\BotIu\\Messenger_creation_9B02EF81-19C8-49B4-9125-9439B97BE7A4.png 527 904 1096 1457\n-Dataset/FaceData/processed\\TrinhTranPhuongTuan\\Screenshot 2025-03-26 152756.png 73 208 429 642\n-Dataset/FaceData/processed\\TrinhTranPhuongTuan\\Screenshot 2025-03-26 152811.png 20 150 367 604\n-Dataset/FaceData/processed\\TrinhTranPhuongTuan\\Screenshot 2025-03-26 152730.png 73 210 394 633\n-Dataset/FaceData/processed\\Viruss\\Messenger_creation_1EFFC217-D09C-416C-86FD-59975646D0EE.png 154 108 1417 1799\n-Dataset/FaceData/processed\\Viruss\\Messenger_creation_B0605F72-CD05-4E02-8A7B-66FF0577F5AF.png 364 442 910 1148\n-Dataset/FaceData/processed\\Viruss\\Messenger_creation_7D77F713-A6DC-42A6-B432-478470A160F4.png 50 121 926 1264\n-Dataset/FaceData/processed\\NgocKem\\IMG_1742972407559.png 304 247 820 976\n-Dataset/FaceData/processed\\NgocKem\\IMG_1742972428266.png 300 233 870 981\n-Dataset/FaceData/processed\\NgocKem\\IMG_1742972431298.png 180 168 796 998\n-Dataset/FaceData/processed\\ThaoNguyen\\IMG_20250325_173023.png 389 530 1184 1500\n-Dataset/FaceData/processed\\ThaoNguyen\\IMG_20250325_173028.png 490 449 1140 1231\n-Dataset/FaceData/processed\\ThaoNguyen\\IMG_20250325_173025.png 513 544 1284 1399\n-Dataset/FaceData/processed\\ThaoNguyen\\IMG_20250325_173026.png 489 328 1176 1142\n-Dataset/FaceData/processed\\AnhTu\\Messenger_creation_B7948BE5-4E60-488E-AA05-A6EE773375FF.png 321 361 1163 1438\n-Dataset/FaceData/processed\\AnhTu\\Messenger_creation_3513C404-872B-4059-9818-DA01337AFC7D.png 217 635 1053 1802\n-Dataset/FaceData/processed\\AnhTu\\Messenger_creation_74F48522-20E2-499E-BF13-183A3613A977.png 474 420 1348 1596\n-Dataset/FaceData/processed\\Thao\\98E5A551-3DC2-4A85-9497-AFA4A845325A.png 287 854 841 1536\n-Dataset/FaceData/processed\\Thao\\851D4C65-07BE-4220-8488-84C40C1A79D6.png 163 460 841 1386\n-Dataset/FaceData/processed\\Phuong\\Messenger_creation_07B34AE7-45ED-4C9F-BD51-83035DA8B5A4.png 383 266 1204 1442\n-Dataset/FaceData/processed\\Phuong\\Messenger_creation_8461CDFC-3B34-4479-B438-1BD2FD3647CF.png 398 511 855 1063\n-Dataset/FaceData/processed\\Phuong\\Messenger_creation_7343AEA7-71DD-40DB-B8C3-9D6F0CED60F9.png\n-Dataset/FaceData/processed\\ThienAn\\att.egtzxRes4EVPEbV4hPcWEW7g0qWm7_7dB1ObmQ-jbw8.png 357 494 1083 1463\n-Dataset/FaceData/processed\\LeAnhTrang\\IMG_1742972296094.png 436 138 965 850\n-Dataset/FaceData/processed\\LeAnhTrang\\IMG_20250326_134739.png 417 628 1522 2056\n-Dataset/FaceData/processed\\Jack\\A3FED369-0E1F-4064-830E-FBFD69D77C85.png 111 709 831 1654\n-Dataset/FaceData/processed\\Jack\\2209C349-3B30-47AC-89C1-1152E1A42FFC.png 224 606 982 1624\n-Dataset/FaceData/processed\\NghiemAnhHieu\\IMG_20250327_092236.png 274 800 1031 1820\n-Dataset/FaceData/processed\\NghiemAnhHieu\\IMG_20250327_092241.png 486 1016 840 1487\n-Dataset/FaceData/processed\\NghiemAnhHieu\\IMG_0327.png 83 74 764 794\n-Dataset/FaceData/processed\\NghiemAnhHieu\\IMG_20250327_092237.png 566 1129 1142 1953\n-Dataset/FaceData/processed\\TranDangHieu\\download (2).png 152 762 696 1478\n-Dataset/FaceData/processed\\TranDangHieu\\download (1).png 262 672 767 1357\n-Dataset/FaceData/processed\\TranDangHieu\\download.png 202 705 750 1444\n-Dataset/FaceData/processed\\BuiCongSon\\485424995_1386922852752466_129752981138586263_n.png 320 554 1058 1544\n-Dataset/FaceData/processed\\BuiCongSon\\485512577_2113779945729669_160018376255303225_n.png 318 390 1184 1528\n-Dataset/FaceData/processed\\BuiCongSon\\485464001_1219012669643907_2610181536763391909_n.png 461 308 1375 1574\n-Dataset/FaceData/processed\\BuiCongSon\\z6377532752047_4b7b3c7d460239f92fa3cec037738eca.png 462 527 1017 1272\n-Dataset/FaceData/processed\\BuiCongSon\\485290655_1069958524968534_6128511440722701069_n.png 376 472 1198 1582\n-Dataset/FaceData/processed\\TuSenna\\IMG_20250326_135145.png 80 165 468 685\n-Dataset/FaceData/processed\\TuSenna\\IMG_20250326_135147.png 225 199 579 683\n-Dataset/FaceData/processed\\TuSenna\\IMG_20250326_135142.png 171 209 549 722\ndiff --git a/Dataset/FaceData/test/data/txl/revision_info.txt b/Dataset/FaceData/test/data/txl/revision_info.txt\ndeleted file mode 100644\nindex 33dea13..0000000\n--- a/Dataset/FaceData/test/data/txl/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25\n---------------------\n-tensorflow version: 2.19.0\n---------------------\n-git hash: b\'69ff1e149c0d84a123d6516ddd82970e65392608\'\n---------------------\n-b\'\'\n\\ No newline at end of file\ndiff --git a/cmd.txt b/cmd.txt\ndeleted file mode 100644\nindex a3d455b..0000000\n--- a/cmd.txt\n+++ /dev/null\n@@ -1,6 +0,0 @@\n-python src/align_dataset_mtcnn.py  Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32  --random_order --gpu_memory_fraction 0.25\n-\n-\n-python src/classifier.py TRAIN Dataset/FaceData/processed Models/20180402-114759.pb Models/facemodel.pkl --batch_size 1000\n-\n-python src/face_rec_cam.py \n\\ No newline at end of file\ndiff --git a/src/compare.py b/src/compare.py\ndeleted file mode 100644\nindex bc53cc4..0000000\n--- a/src/compare.py\n+++ /dev/null\n@@ -1,130 +0,0 @@\n-"""Performs face alignment and calculates L2 distance between the embeddings of images."""\n-\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from scipy import misc\n-import tensorflow as tf\n-import numpy as np\n-import sys\n-import os\n-import copy\n-import argparse\n-import facenet\n-import align.detect_face\n-\n-def main(args):\n-\n-    images = load_and_align_data(args.image_files, args.image_size, args.margin, args.gpu_memory_fraction)\n-    with tf.Graph().as_default():\n-\n-        with tf.Session() as sess:\n-      \n-            # Load the model\n-            facenet.load_model(args.model)\n-    \n-            # Get input and output tensors\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n-\n-            # Run forward pass to calculate embeddings\n-            feed_dict = { images_placeholder: images, phase_train_placeholder:False }\n-            emb = sess.run(embeddings, feed_dict=feed_dict)\n-            \n-            nrof_images = len(args.image_files)\n-\n-            print(\'Images:\')\n-            for i in range(nrof_images):\n-                print(\'%1d: %s\' % (i, args.image_files[i]))\n-            print(\'\')\n-            \n-            # Print distance matrix\n-            print(\'Distance matrix\')\n-            print(\'    \', end=\'\')\n-            for i in range(nrof_images):\n-                print(\'    %1d     \' % i, end=\'\')\n-            print(\'\')\n-            for i in range(nrof_images):\n-                print(\'%1d  \' % i, end=\'\')\n-                for j in range(nrof_images):\n-                    dist = np.sqrt(np.sum(np.square(np.subtract(emb[i,:], emb[j,:]))))\n-                    print(\'  %1.4f  \' % dist, end=\'\')\n-                print(\'\')\n-            \n-            \n-def load_and_align_data(image_paths, image_size, margin, gpu_memory_fraction):\n-\n-    minsize = 20 # minimum size of face\n-    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\'s threshold\n-    factor = 0.709 # scale factor\n-    \n-    print(\'Creating networks and loading parameters\')\n-    with tf.Graph().as_default():\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-        with sess.as_default():\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n-  \n-    tmp_image_paths=copy.copy(image_paths)\n-    img_list = []\n-    for image in tmp_image_paths:\n-        img = misc.imread(os.path.expanduser(image), mode=\'RGB\')\n-        img_size = np.asarray(img.shape)[0:2]\n-        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n-        if len(bounding_boxes) < 1:\n-          image_paths.remove(image)\n-          print("can\'t detect face, remove ", image)\n-          continue\n-        det = np.squeeze(bounding_boxes[0,0:4])\n-        bb = np.zeros(4, dtype=np.int32)\n-        bb[0] = np.maximum(det[0]-margin/2, 0)\n-        bb[1] = np.maximum(det[1]-margin/2, 0)\n-        bb[2] = np.minimum(det[2]+margin/2, img_size[1])\n-        bb[3] = np.minimum(det[3]+margin/2, img_size[0])\n-        cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n-        aligned = misc.imresize(cropped, (image_size, image_size), interp=\'bilinear\')\n-        prewhitened = facenet.prewhiten(aligned)\n-        img_list.append(prewhitened)\n-    images = np.stack(img_list)\n-    return images\n-\n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'model\', type=str, \n-        help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n-    parser.add_argument(\'image_files\', type=str, nargs=\'+\', help=\'Images to compare\')\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=160)\n-    parser.add_argument(\'--margin\', type=int,\n-        help=\'Margin for the crop around the bounding box (height, width) in pixels.\', default=44)\n-    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n-        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n-    return parser.parse_args(argv)\n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\ndeleted file mode 100644\nindex 455f67a..0000000\n--- a/src/face_rec_cam.py\n+++ /dev/null\n@@ -1,135 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-from imutils.video import VideoStream\n-\n-\n-import argparse\n-import facenet\n-import imutils\n-import os\n-import sys\n-import math\n-import pickle\n-import align.detect_face\n-import numpy as np\n-import cv2\n-import collections\n-from sklearn.svm import SVC\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\'--path\', help=\'Path of the video you want to test on.\', default=0)\n-    args = parser.parse_args()\n-\n-    MINSIZE = 20\n-    THRESHOLD = [0.6, 0.7, 0.7]\n-    FACTOR = 0.709\n-    IMAGE_SIZE = 182\n-    INPUT_IMAGE_SIZE = 160\n-    CLASSIFIER_PATH = \'Models/facemodel.pkl\'\n-    VIDEO_PATH = args.path\n-    FACENET_MODEL_PATH = \'Models/20180402-114759.pb\'\n-\n-    # Load The Custom Classifier\n-    with open(CLASSIFIER_PATH, \'rb\') as file:\n-        model, class_names = pickle.load(file)\n-    print("Custom Classifier, Successfully loaded")\n-\n-    with tf.Graph().as_default():\n-\n-        # Cai dat GPU neu co\n-        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-\n-        with sess.as_default():\n-\n-            # Load the model\n-            print(\'Loading feature extraction model\')\n-            facenet.load_model(FACENET_MODEL_PATH)\n-\n-            # Get input and output tensors\n-            graph = tf.compat.v1.get_default_graph()\n-            images_placeholder = graph.get_tensor_by_name("input:0")\n-            embeddings = graph.get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = graph.get_tensor_by_name("phase_train:0")\n-            embedding_size = embeddings.get_shape()[1]\n-\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\n-\n-            people_detected = set()\n-            person_detected = collections.Counter()\n-\n-            cap  = VideoStream(src=0).start()\n-\n-            while (True):\n-                frame = cap.read()\n-                frame = imutils.resize(frame, width=600)\n-                frame = cv2.flip(frame, 1)\n-\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\n-\n-                faces_found = bounding_boxes.shape[0]\n-                try:\n-                    if faces_found > 1:\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                    1, (255, 255, 255), thickness=1, lineType=2)\n-                    elif faces_found > 0:\n-                        det = bounding_boxes[:, 0:4]\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\n-                        for i in range(faces_found):\n-                            bb[i][0] = det[i][0]\n-                            bb[i][1] = det[i][1]\n-                            bb[i][2] = det[i][2]\n-                            bb[i][3] = det[i][3]\n-                            print(bb[i][3]-bb[i][1])\n-                            print(frame.shape[0])\n-                            print((bb[i][3]-bb[i][1])/frame.shape[0])\n-                            if (bb[i][3]-bb[i][1])/frame.shape[0]>0.25:\n-                                cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\n-                                                    interpolation=cv2.INTER_CUBIC)\n-                                scaled = facenet.prewhiten(scaled)\n-                                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\n-                                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\n-                                emb_array = sess.run(embeddings, feed_dict=feed_dict)\n-\n-                                predictions = model.predict_proba(emb_array)\n-                                best_class_indices = np.argmax(predictions, axis=1)\n-                                best_class_probabilities = predictions[\n-                                    np.arange(len(best_class_indices)), best_class_indices]\n-                                best_name = class_names[best_class_indices[0]]\n-                                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\n-\n-\n-\n-                                if best_class_probabilities > 0.8:\n-                                    cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\n-                                    text_x = bb[i][0]\n-                                    text_y = bb[i][3] + 20\n-\n-                                    name = class_names[best_class_indices[0]]\n-                                    cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\n-                                    cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\n-                                                cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\n-                                    person_detected[best_name] += 1\n-                                else:\n-                                    name = "Unknown"\n-\n-                except:\n-                    pass\n-\n-                cv2.imshow(\'Face Recognition\', frame)\n-                if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n-                    break\n-\n-            cap.release()\n-            cv2.destroyAllWindows()\n-\n-\n-main()\ndiff --git a/src/face_rec_flask.py b/src/face_rec_flask.py\ndeleted file mode 100644\nindex 3f7a27b..0000000\n--- a/src/face_rec_flask.py\n+++ /dev/null\n@@ -1,118 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from flask import Flask\n-from flask import render_template , request\n-from flask_cors import CORS, cross_origin\n-import tensorflow as tf\n-import argparse\n-import facenet\n-import os\n-import sys\n-import math\n-import pickle\n-import align.detect_face\n-import numpy as np\n-import cv2\n-import collections\n-from sklearn.svm import SVC\n-import base64\n-\n-MINSIZE = 20\n-THRESHOLD = [0.6, 0.7, 0.7]\n-FACTOR = 0.709\n-IMAGE_SIZE = 182\n-INPUT_IMAGE_SIZE = 160\n-CLASSIFIER_PATH = \'./Models/facemodel.pkl\'\n-FACENET_MODEL_PATH = \'./Models/20180402-114759.pb\'\n-\n-# Load The Custom Classifier\n-with open(CLASSIFIER_PATH, \'rb\') as file:\n-    model, class_names = pickle.load(file)\n-print("Custom Classifier, Successfully loaded")\n-\n-tf.Graph().as_default()\n-\n-# Cai dat GPU neu co\n-gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\n-sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-\n-\n-# Load the model\n-print(\'Loading feature extraction model\')\n-facenet.load_model(FACENET_MODEL_PATH)\n-\n-# Get input and output tensors\n-images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n-embedding_size = embeddings.get_shape()[1]\n-pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "align")\n-\n-\n-\n-app = Flask(__name__)\n-CORS(app)\n-\n-\n-\n-@app.route(\'/\')\n-@cross_origin()\n-def index():\n-    return "OK!";\n-\n-@app.route(\'/recog\', methods=[\'POST\'])\n-@cross_origin()\n-def upload_img_file():\n-    if request.method == \'POST\':\n-        # base 64\n-        name="Unknown"\n-        f = request.form.get(\'image\')\n-        w = int(request.form.get(\'w\'))\n-        h = int(request.form.get(\'h\'))\n-\n-        decoded_string = base64.b64decode(f)\n-        frame = np.fromstring(decoded_string, dtype=np.uint8)\n-        #frame = frame.reshape(w,h,3)\n-        frame = cv2.imdecode(frame, cv2.IMREAD_ANYCOLOR)  # cv2.IMREAD_COLOR in OpenCV 3.1\n-\n-        bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\n-\n-        faces_found = bounding_boxes.shape[0]\n-\n-        if faces_found > 0:\n-            det = bounding_boxes[:, 0:4]\n-            bb = np.zeros((faces_found, 4), dtype=np.int32)\n-            for i in range(faces_found):\n-                bb[i][0] = det[i][0]\n-                bb[i][1] = det[i][1]\n-                bb[i][2] = det[i][2]\n-                bb[i][3] = det[i][3]\n-                cropped = frame\n-                #cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\n-                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\n-                                    interpolation=cv2.INTER_CUBIC)\n-                scaled = facenet.prewhiten(scaled)\n-                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\n-                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\n-                emb_array = sess.run(embeddings, feed_dict=feed_dict)\n-                predictions = model.predict_proba(emb_array)\n-                best_class_indices = np.argmax(predictions, axis=1)\n-                best_class_probabilities = predictions[\n-                    np.arange(len(best_class_indices)), best_class_indices]\n-                best_name = class_names[best_class_indices[0]]\n-                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\n-\n-                if best_class_probabilities > 0.8:\n-                    name = class_names[best_class_indices[0]]\n-                else:\n-                    name = "Unknown"\n-\n-\n-        return name;\n-\n-\n-if __name__ == \'__main__\':\n-    app.run(debug=True, host=\'0.0.0.0\',port=\'8000\')\n-\ndiff --git a/src/models/__init__.py b/src/models/__init__.py\ndeleted file mode 100644\nindex efa6252..0000000\n--- a/src/models/__init__.py\n+++ /dev/null\n@@ -1,2 +0,0 @@\n-# flake8: noqa\n-\ndiff --git a/src/models/dummy.py b/src/models/dummy.py\ndeleted file mode 100644\nindex 7afe1ef..0000000\n--- a/src/models/dummy.py\n+++ /dev/null\n@@ -1,54 +0,0 @@\n-"""Dummy model used only for testing\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n-import numpy as np\n-  \n-def inference(images, keep_probability, phase_train=True,  # @UnusedVariable\n-              bottleneck_layer_size=128, bottleneck_layer_activation=None, weight_decay=0.0, reuse=None):  # @UnusedVariable\n-    batch_norm_params = {\n-        # Decay for the moving averages.\n-        \'decay\': 0.995,\n-        # epsilon to prevent 0s in variance.\n-        \'epsilon\': 0.001,\n-        # force in-place updates of mean and variance estimates\n-        \'updates_collections\': None,\n-        # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n-    }\n-    \n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\n-                        normalizer_fn=slim.batch_norm,\n-                        normalizer_params=batch_norm_params):\n-        size = np.prod(images.get_shape()[1:].as_list())\n-        net = slim.fully_connected(tf.reshape(images, (-1,size)), bottleneck_layer_size, activation_fn=None, \n-                scope=\'Bottleneck\', reuse=False)\n-        return net, None\ndiff --git a/src/models/inception_resnet_v1.py b/src/models/inception_resnet_v1.py\ndeleted file mode 100644\nindex 475e81b..0000000\n--- a/src/models/inception_resnet_v1.py\n+++ /dev/null\n@@ -1,246 +0,0 @@\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the "License");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-# http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an "AS IS" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\n-"""Contains the definition of the Inception Resnet V1 architecture.\n-As described in http://arxiv.org/abs/1602.07261.\n-  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n-    on Learning\n-  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n-"""\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n-\n-# Inception-Resnet-A\n-def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n-    """Builds the 35x35 resnet block."""\n-    with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n-        with tf.variable_scope(\'Branch_2\'):\n-            tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv2_1 = slim.conv2d(tower_conv2_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n-            tower_conv2_2 = slim.conv2d(tower_conv2_1, 32, 3, scope=\'Conv2d_0c_3x3\')\n-        mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n-        net += scale * up\n-        if activation_fn:\n-            net = activation_fn(net)\n-    return net\n-\n-# Inception-Resnet-B\n-def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n-    """Builds the 17x17 resnet block."""\n-    with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 128, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 128, [1, 7],\n-                                        scope=\'Conv2d_0b_1x7\')\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 128, [7, 1],\n-                                        scope=\'Conv2d_0c_7x1\')\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n-        net += scale * up\n-        if activation_fn:\n-            net = activation_fn(net)\n-    return net\n-\n-\n-# Inception-Resnet-C\n-def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n-    """Builds the 8x8 resnet block."""\n-    with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 192, [1, 3],\n-                                        scope=\'Conv2d_0b_1x3\')\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [3, 1],\n-                                        scope=\'Conv2d_0c_3x1\')\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n-        net += scale * up\n-        if activation_fn:\n-            net = activation_fn(net)\n-    return net\n-  \n-def reduction_a(net, k, l, m, n):\n-    with tf.variable_scope(\'Branch_0\'):\n-        tower_conv = slim.conv2d(net, n, 3, stride=2, padding=\'VALID\',\n-                                 scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_1\'):\n-        tower_conv1_0 = slim.conv2d(net, k, 1, scope=\'Conv2d_0a_1x1\')\n-        tower_conv1_1 = slim.conv2d(tower_conv1_0, l, 3,\n-                                    scope=\'Conv2d_0b_3x3\')\n-        tower_conv1_2 = slim.conv2d(tower_conv1_1, m, 3,\n-                                    stride=2, padding=\'VALID\',\n-                                    scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_2\'):\n-        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                     scope=\'MaxPool_1a_3x3\')\n-    net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n-    return net\n-\n-def reduction_b(net):\n-    with tf.variable_scope(\'Branch_0\'):\n-        tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n-                                   padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_1\'):\n-        tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-        tower_conv1_1 = slim.conv2d(tower_conv1, 256, 3, stride=2,\n-                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_2\'):\n-        tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-        tower_conv2_1 = slim.conv2d(tower_conv2, 256, 3,\n-                                    scope=\'Conv2d_0b_3x3\')\n-        tower_conv2_2 = slim.conv2d(tower_conv2_1, 256, 3, stride=2,\n-                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_3\'):\n-        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                     scope=\'MaxPool_1a_3x3\')\n-    net = tf.concat([tower_conv_1, tower_conv1_1,\n-                        tower_conv2_2, tower_pool], 3)\n-    return net\n-  \n-def inference(images, keep_probability, phase_train=True, \n-              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\n-    batch_norm_params = {\n-        # Decay for the moving averages.\n-        \'decay\': 0.995,\n-        # epsilon to prevent 0s in variance.\n-        \'epsilon\': 0.001,\n-        # force in-place updates of mean and variance estimates\n-        \'updates_collections\': None,\n-        # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n-    }\n-    \n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                        weights_initializer=slim.initializers.xavier_initializer(), \n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\n-                        normalizer_fn=slim.batch_norm,\n-                        normalizer_params=batch_norm_params):\n-        return inception_resnet_v1(images, is_training=phase_train,\n-              dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size, reuse=reuse)\n-\n-\n-def inception_resnet_v1(inputs, is_training=True,\n-                        dropout_keep_prob=0.8,\n-                        bottleneck_layer_size=128,\n-                        reuse=None, \n-                        scope=\'InceptionResnetV1\'):\n-    """Creates the Inception Resnet V1 model.\n-    Args:\n-      inputs: a 4-D tensor of size [batch_size, height, width, 3].\n-      num_classes: number of predicted classes.\n-      is_training: whether is training or not.\n-      dropout_keep_prob: float, the fraction to keep before final layer.\n-      reuse: whether or not the network and its variables should be reused. To be\n-        able to reuse \'scope\' must be given.\n-      scope: Optional variable_scope.\n-    Returns:\n-      logits: the logits outputs of the model.\n-      end_points: the set of end_points from the inception model.\n-    """\n-    end_points = {}\n-  \n-    with tf.variable_scope(scope, \'InceptionResnetV1\', [inputs], reuse=reuse):\n-        with slim.arg_scope([slim.batch_norm, slim.dropout],\n-                            is_training=is_training):\n-            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n-                                stride=1, padding=\'SAME\'):\n-      \n-                # 149 x 149 x 32\n-                net = slim.conv2d(inputs, 32, 3, stride=2, padding=\'VALID\',\n-                                  scope=\'Conv2d_1a_3x3\')\n-                end_points[\'Conv2d_1a_3x3\'] = net\n-                # 147 x 147 x 32\n-                net = slim.conv2d(net, 32, 3, padding=\'VALID\',\n-                                  scope=\'Conv2d_2a_3x3\')\n-                end_points[\'Conv2d_2a_3x3\'] = net\n-                # 147 x 147 x 64\n-                net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n-                end_points[\'Conv2d_2b_3x3\'] = net\n-                # 73 x 73 x 64\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                      scope=\'MaxPool_3a_3x3\')\n-                end_points[\'MaxPool_3a_3x3\'] = net\n-                # 73 x 73 x 80\n-                net = slim.conv2d(net, 80, 1, padding=\'VALID\',\n-                                  scope=\'Conv2d_3b_1x1\')\n-                end_points[\'Conv2d_3b_1x1\'] = net\n-                # 71 x 71 x 192\n-                net = slim.conv2d(net, 192, 3, padding=\'VALID\',\n-                                  scope=\'Conv2d_4a_3x3\')\n-                end_points[\'Conv2d_4a_3x3\'] = net\n-                # 35 x 35 x 256\n-                net = slim.conv2d(net, 256, 3, stride=2, padding=\'VALID\',\n-                                  scope=\'Conv2d_4b_3x3\')\n-                end_points[\'Conv2d_4b_3x3\'] = net\n-                \n-                # 5 x Inception-resnet-A\n-                net = slim.repeat(net, 5, block35, scale=0.17)\n-                end_points[\'Mixed_5a\'] = net\n-        \n-                # Reduction-A\n-                with tf.variable_scope(\'Mixed_6a\'):\n-                    net = reduction_a(net, 192, 192, 256, 384)\n-                end_points[\'Mixed_6a\'] = net\n-                \n-                # 10 x Inception-Resnet-B\n-                net = slim.repeat(net, 10, block17, scale=0.10)\n-                end_points[\'Mixed_6b\'] = net\n-                \n-                # Reduction-B\n-                with tf.variable_scope(\'Mixed_7a\'):\n-                    net = reduction_b(net)\n-                end_points[\'Mixed_7a\'] = net\n-                \n-                # 5 x Inception-Resnet-C\n-                net = slim.repeat(net, 5, block8, scale=0.20)\n-                end_points[\'Mixed_8a\'] = net\n-                \n-                net = block8(net, activation_fn=None)\n-                end_points[\'Mixed_8b\'] = net\n-                \n-                with tf.variable_scope(\'Logits\'):\n-                    end_points[\'PrePool\'] = net\n-                    #pylint: disable=no-member\n-                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n-                                          scope=\'AvgPool_1a_8x8\')\n-                    net = slim.flatten(net)\n-          \n-                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n-                                       scope=\'Dropout\')\n-          \n-                    end_points[\'PreLogitsFlatten\'] = net\n-                \n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \n-                        scope=\'Bottleneck\', reuse=False)\n-  \n-    return net, end_points\ndiff --git a/src/models/inception_resnet_v2.py b/src/models/inception_resnet_v2.py\ndeleted file mode 100644\nindex 0fb176f..0000000\n--- a/src/models/inception_resnet_v2.py\n+++ /dev/null\n@@ -1,255 +0,0 @@\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the "License");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-# http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an "AS IS" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\n-"""Contains the definition of the Inception Resnet V2 architecture.\n-As described in http://arxiv.org/abs/1602.07261.\n-  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n-    on Learning\n-  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n-"""\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n-\n-# Inception-Resnet-A\n-def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n-    """Builds the 35x35 resnet block."""\n-    with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n-        with tf.variable_scope(\'Branch_2\'):\n-            tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\'Conv2d_0b_3x3\')\n-            tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\'Conv2d_0c_3x3\')\n-        mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n-        net += scale * up\n-        if activation_fn:\n-            net = activation_fn(net)\n-    return net\n-\n-# Inception-Resnet-B\n-def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n-    """Builds the 17x17 resnet block."""\n-    with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n-                                        scope=\'Conv2d_0b_1x7\')\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n-                                        scope=\'Conv2d_0c_7x1\')\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n-        net += scale * up\n-        if activation_fn:\n-            net = activation_fn(net)\n-    return net\n-\n-\n-# Inception-Resnet-C\n-def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n-    """Builds the 8x8 resnet block."""\n-    with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n-                                        scope=\'Conv2d_0b_1x3\')\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n-                                        scope=\'Conv2d_0c_3x1\')\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n-        net += scale * up\n-        if activation_fn:\n-            net = activation_fn(net)\n-    return net\n-  \n-def inference(images, keep_probability, phase_train=True, \n-              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\n-    batch_norm_params = {\n-        # Decay for the moving averages.\n-        \'decay\': 0.995,\n-        # epsilon to prevent 0s in variance.\n-        \'epsilon\': 0.001,\n-        # force in-place updates of mean and variance estimates\n-        \'updates_collections\': None,\n-        # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n-}\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                        weights_initializer=slim.initializers.xavier_initializer(), \n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\n-                        normalizer_fn=slim.batch_norm,\n-                        normalizer_params=batch_norm_params):\n-        return inception_resnet_v2(images, is_training=phase_train,\n-              dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size, reuse=reuse)\n-\n-\n-def inception_resnet_v2(inputs, is_training=True,\n-                        dropout_keep_prob=0.8,\n-                        bottleneck_layer_size=128,\n-                        reuse=None,\n-                        scope=\'InceptionResnetV2\'):\n-    """Creates the Inception Resnet V2 model.\n-    Args:\n-      inputs: a 4-D tensor of size [batch_size, height, width, 3].\n-      num_classes: number of predicted classes.\n-      is_training: whether is training or not.\n-      dropout_keep_prob: float, the fraction to keep before final layer.\n-      reuse: whether or not the network and its variables should be reused. To be\n-        able to reuse \'scope\' must be given.\n-      scope: Optional variable_scope.\n-    Returns:\n-      logits: the logits outputs of the model.\n-      end_points: the set of end_points from the inception model.\n-    """\n-    end_points = {}\n-  \n-    with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs], reuse=reuse):\n-        with slim.arg_scope([slim.batch_norm, slim.dropout],\n-                            is_training=is_training):\n-            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n-                                stride=1, padding=\'SAME\'):\n-      \n-                # 149 x 149 x 32\n-                net = slim.conv2d(inputs, 32, 3, stride=2, padding=\'VALID\',\n-                                  scope=\'Conv2d_1a_3x3\')\n-                end_points[\'Conv2d_1a_3x3\'] = net\n-                # 147 x 147 x 32\n-                net = slim.conv2d(net, 32, 3, padding=\'VALID\',\n-                                  scope=\'Conv2d_2a_3x3\')\n-                end_points[\'Conv2d_2a_3x3\'] = net\n-                # 147 x 147 x 64\n-                net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n-                end_points[\'Conv2d_2b_3x3\'] = net\n-                # 73 x 73 x 64\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                      scope=\'MaxPool_3a_3x3\')\n-                end_points[\'MaxPool_3a_3x3\'] = net\n-                # 73 x 73 x 80\n-                net = slim.conv2d(net, 80, 1, padding=\'VALID\',\n-                                  scope=\'Conv2d_3b_1x1\')\n-                end_points[\'Conv2d_3b_1x1\'] = net\n-                # 71 x 71 x 192\n-                net = slim.conv2d(net, 192, 3, padding=\'VALID\',\n-                                  scope=\'Conv2d_4a_3x3\')\n-                end_points[\'Conv2d_4a_3x3\'] = net\n-                # 35 x 35 x 192\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                      scope=\'MaxPool_5a_3x3\')\n-                end_points[\'MaxPool_5a_3x3\'] = net\n-        \n-                # 35 x 35 x 320\n-                with tf.variable_scope(\'Mixed_5b\'):\n-                    with tf.variable_scope(\'Branch_0\'):\n-                        tower_conv = slim.conv2d(net, 96, 1, scope=\'Conv2d_1x1\')\n-                    with tf.variable_scope(\'Branch_1\'):\n-                        tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n-                                                    scope=\'Conv2d_0b_5x5\')\n-                    with tf.variable_scope(\'Branch_2\'):\n-                        tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n-                                                    scope=\'Conv2d_0b_3x3\')\n-                        tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n-                                                    scope=\'Conv2d_0c_3x3\')\n-                    with tf.variable_scope(\'Branch_3\'):\n-                        tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\'SAME\',\n-                                                     scope=\'AvgPool_0a_3x3\')\n-                        tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n-                                                   scope=\'Conv2d_0b_1x1\')\n-                    net = tf.concat([tower_conv, tower_conv1_1,\n-                                        tower_conv2_2, tower_pool_1], 3)\n-        \n-                end_points[\'Mixed_5b\'] = net\n-                net = slim.repeat(net, 10, block35, scale=0.17)\n-        \n-                # 17 x 17 x 1024\n-                with tf.variable_scope(\'Mixed_6a\'):\n-                    with tf.variable_scope(\'Branch_0\'):\n-                        tower_conv = slim.conv2d(net, 384, 3, stride=2, padding=\'VALID\',\n-                                                 scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_1\'):\n-                        tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n-                                                    scope=\'Conv2d_0b_3x3\')\n-                        tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n-                                                    stride=2, padding=\'VALID\',\n-                                                    scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_2\'):\n-                        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                                     scope=\'MaxPool_1a_3x3\')\n-                    net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n-        \n-                end_points[\'Mixed_6a\'] = net\n-                net = slim.repeat(net, 20, block17, scale=0.10)\n-        \n-                with tf.variable_scope(\'Mixed_7a\'):\n-                    with tf.variable_scope(\'Branch_0\'):\n-                        tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n-                                                   padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_1\'):\n-                        tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n-                                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_2\'):\n-                        tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n-                                                    scope=\'Conv2d_0b_3x3\')\n-                        tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n-                                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_3\'):\n-                        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                                     scope=\'MaxPool_1a_3x3\')\n-                    net = tf.concat([tower_conv_1, tower_conv1_1,\n-                                        tower_conv2_2, tower_pool], 3)\n-        \n-                end_points[\'Mixed_7a\'] = net\n-        \n-                net = slim.repeat(net, 9, block8, scale=0.20)\n-                net = block8(net, activation_fn=None)\n-        \n-                net = slim.conv2d(net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n-                end_points[\'Conv2d_7b_1x1\'] = net\n-        \n-                with tf.variable_scope(\'Logits\'):\n-                    end_points[\'PrePool\'] = net\n-                    #pylint: disable=no-member\n-                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n-                                          scope=\'AvgPool_1a_8x8\')\n-                    net = slim.flatten(net)\n-          \n-                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n-                                       scope=\'Dropout\')\n-          \n-                    end_points[\'PreLogitsFlatten\'] = net\n-                \n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \n-                        scope=\'Bottleneck\', reuse=False)\n-  \n-    return net, end_points\ndiff --git a/src/models/squeezenet.py b/src/models/squeezenet.py\ndeleted file mode 100644\nindex ae117e1..0000000\n--- a/src/models/squeezenet.py\n+++ /dev/null\n@@ -1,67 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n-\n-def fire_module(inputs,\n-                squeeze_depth,\n-                expand_depth,\n-                reuse=None,\n-                scope=None,\n-                outputs_collections=None):\n-    with tf.variable_scope(scope, \'fire\', [inputs], reuse=reuse):\n-        with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n-                            outputs_collections=None):\n-            net = squeeze(inputs, squeeze_depth)\n-            outputs = expand(net, expand_depth)\n-            return outputs\n-\n-def squeeze(inputs, num_outputs):\n-    return slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope=\'squeeze\')\n-\n-def expand(inputs, num_outputs):\n-    with tf.variable_scope(\'expand\'):\n-        e1x1 = slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope=\'1x1\')\n-        e3x3 = slim.conv2d(inputs, num_outputs, [3, 3], scope=\'3x3\')\n-    return tf.concat([e1x1, e3x3], 3)\n-\n-def inference(images, keep_probability, phase_train=True, bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\n-    batch_norm_params = {\n-        # Decay for the moving averages.\n-        \'decay\': 0.995,\n-        # epsilon to prevent 0s in variance.\n-        \'epsilon\': 0.001,\n-        # force in-place updates of mean and variance estimates\n-        \'updates_collections\': None,\n-        # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n-    }\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                        weights_initializer=slim.xavier_initializer_conv2d(uniform=True),\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\n-                        normalizer_fn=slim.batch_norm,\n-                        normalizer_params=batch_norm_params):\n-        with tf.variable_scope(\'squeezenet\', [images], reuse=reuse):\n-            with slim.arg_scope([slim.batch_norm, slim.dropout],\n-                                is_training=phase_train):\n-                net = slim.conv2d(images, 96, [7, 7], stride=2, scope=\'conv1\')\n-                net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'maxpool1\')\n-                net = fire_module(net, 16, 64, scope=\'fire2\')\n-                net = fire_module(net, 16, 64, scope=\'fire3\')\n-                net = fire_module(net, 32, 128, scope=\'fire4\')\n-                net = slim.max_pool2d(net, [2, 2], stride=2, scope=\'maxpool4\')\n-                net = fire_module(net, 32, 128, scope=\'fire5\')\n-                net = fire_module(net, 48, 192, scope=\'fire6\')\n-                net = fire_module(net, 48, 192, scope=\'fire7\')\n-                net = fire_module(net, 64, 256, scope=\'fire8\')\n-                net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'maxpool8\')\n-                net = fire_module(net, 64, 256, scope=\'fire9\')\n-                net = slim.dropout(net, keep_probability)\n-                net = slim.conv2d(net, 1000, [1, 1], activation_fn=None, normalizer_fn=None, scope=\'conv10\')\n-                net = slim.avg_pool2d(net, net.get_shape()[1:3], scope=\'avgpool10\')\n-                net = tf.squeeze(net, [1, 2], name=\'logits\')\n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \n-                        scope=\'Bottleneck\', reuse=False)\n-    return net, None'